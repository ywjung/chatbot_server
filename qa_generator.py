import json
import os
import glob
import chromadb
from sentence_transformers import SentenceTransformer
from fastapi import FastAPI, HTTPException, Request, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.openapi.docs import get_swagger_ui_html, get_redoc_html
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union
import requests
import logging
import uuid
import time
import re
import uvicorn
import numpy as np
import PyPDF2
import pdfplumber
from io import BytesIO
import tempfile

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ê¸°ì¡´ Pydantic ëª¨ë¸ë“¤ (ìƒëµí•˜ê³  í•„ìš”í•œ ìƒˆ ëª¨ë¸ë“¤ë§Œ ì¶”ê°€)
class SearchRequest(BaseModel):
    query: str = Field(..., description="ê²€ìƒ‰í•  ì§ˆì˜", example="íœ´ê°€ ì‹ ì²­ ë°©ë²•")
    top_k: int = Field(20, description="ë°˜í™˜í•  ê²°ê³¼ ìˆ˜", example=20, ge=1, le=50)
    main_category_filter: Optional[str] = Field(None, description="ëŒ€ë¶„ë¥˜ í•„í„°", example="ì¸ì‚¬")

class ConversationMessage(BaseModel):
    """ëŒ€í™” ë©”ì‹œì§€ ëª¨ë¸"""
    role: str = Field(..., description="ë©”ì‹œì§€ ì—­í•  (user ë˜ëŠ” assistant)", example="user")
    content: str = Field(..., description="ë©”ì‹œì§€ ë‚´ìš©", example="íœ´ê°€ ì‹ ì²­ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?")
    context: Optional[List[Dict[str, Any]]] = Field(None, description="ì‘ë‹µ ì»¨í…ìŠ¤íŠ¸ (assistant ë©”ì‹œì§€ì¸ ê²½ìš°)")

class ChatRequest(BaseModel):
    query: str = Field(..., description="ì§ˆë¬¸ ë‚´ìš©", example="21ë…„ì°¨ íœ´ê°€ëŠ” ë©°ì¹ ì¸ê°€ìš”?", min_length=1)
    main_category_filter: Optional[str] = Field(None, description="ëŒ€ë¶„ë¥˜ í•„í„°", example="ì¸ì‚¬")
    conversation_history: List[ConversationMessage] = Field(
        default_factory=list, 
        description="ì´ì „ ëŒ€í™” ê¸°ë¡",
        example=[
            {
                "role": "user",
                "content": "íœ´ê°€ ì‹ ì²­ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
            },
            {
                "role": "assistant", 
                "content": "íœ´ê°€ ì‹ ì²­ì€ ì „ìê²°ì¬ ì‹œìŠ¤í…œì„ í†µí•´ ì§„í–‰ë©ë‹ˆë‹¤.",
                "context": []
            }
        ]
    )

class StreamChatRequest(BaseModel):
    """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì „ìš© ìš”ì²­ ëª¨ë¸ (ë” ê´€ëŒ€í•œ ê²€ì¦)"""
    query: str = Field(..., description="ì§ˆë¬¸ ë‚´ìš©", example="21ë…„ì°¨ íœ´ê°€ëŠ” ë©°ì¹ ì¸ê°€ìš”?", min_length=1)
    main_category_filter: Optional[str] = Field(None, description="ëŒ€ë¶„ë¥˜ í•„í„°", example="ì¸ì‚¬")
    conversation_history: Optional[List[Dict[str, Any]]] = Field(
        default_factory=list,
        description="ì´ì „ ëŒ€í™” ê¸°ë¡ (ìœ ì—°í•œ í˜•ì‹)",
        example=[
            {"role": "user", "content": "íœ´ê°€ ì‹ ì²­ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"},
            {"role": "assistant", "content": "íœ´ê°€ ì‹ ì²­ì€ ì „ìê²°ì¬ ì‹œìŠ¤í…œì„ í†µí•´ ì§„í–‰ë©ë‹ˆë‹¤."}
        ]
    )

class SimpleTestRequest(BaseModel):
    """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­ ëª¨ë¸"""
    query: str = Field(..., description="ì§ˆë¬¸", example="íœ´ê°€ ì‹ ì²­ ë°©ë²•", min_length=1)
    category: Optional[str] = Field(None, description="ì¹´í…Œê³ ë¦¬ í•„í„°", example="ì¸ì‚¬")

class SearchResult(BaseModel):
    main_category: str
    sub_category: str
    question: str
    answer: str
    source_file: str
    chunk_type: str
    score: float
    rank: int

class HealthResponse(BaseModel):
    status: str
    rag_ready: bool
    regulations_count: int
    main_categories_count: int
    chunk_statistics: Optional[Dict[str, int]] = None
    improvements: str
    enhanced_features: Optional[List[str]] = None

class SearchResponse(BaseModel):
    query: str
    results: List[SearchResult]
    count: int
    main_category_filter: Optional[str]
    search_type: str
    min_relevance: float
    enhanced_features: List[str]

class ChatResponse(BaseModel):
    query: str
    response: str
    context: List[SearchResult]
    context_count: int
    context_quality: Dict[str, int]
    search_type: str
    response_type: str
    enhanced_features: List[str]

# ìƒˆë¡œìš´ PDF ê´€ë ¨ ëª¨ë¸ë“¤
class PDFUploadResponse(BaseModel):
    """PDF ì—…ë¡œë“œ ì‘ë‹µ ëª¨ë¸"""
    status: str
    message: str
    filename: str
    file_size: int
    pages_count: int
    extraction_method: str
    processing_time: float
    json_data: List[Dict[str, Any]]
    statistics: Dict[str, int]

class QAGenerationRequest(BaseModel):
    """Q&A ìƒì„± ìš”ì²­ ëª¨ë¸"""
    text: str = Field(..., description="Q&Aë¥¼ ìƒì„±í•  í…ìŠ¤íŠ¸", min_length=100)
    category_name: str = Field(..., description="ì¹´í…Œê³ ë¦¬ ì´ë¦„", example="ë²•ì¸ì¹´ë“œ ì‚¬ìš© ì§€ì¹¨")
    qa_count: int = Field(10, description="ìƒì„±í•  Q&A ìˆ˜", ge=5, le=50)
    company_name: str = Field("íšŒì‚¬", description="íšŒì‚¬ëª…", example="ë”ì¼€ì´êµì§ì›ë‚˜ë¼(ì£¼)")

class QAGenerationResponse(BaseModel):
    """Q&A ìƒì„± ì‘ë‹µ ëª¨ë¸"""
    status: str
    category: str
    generated_qa_count: int
    processing_time: float
    json_data: List[Dict[str, Any]]

class PDFTextExtractor:
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    @staticmethod
    def extract_text_pypdf2(pdf_content: bytes) -> tuple[str, int]:
        """PyPDF2ë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            pdf_file = BytesIO(pdf_content)
            reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            pages_count = len(reader.pages)
            
            for page in reader.pages:
                try:
                    text += page.extract_text() + "\n"
                except Exception as e:
                    logger.warning(f"PyPDF2 í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            return text.strip(), pages_count
        except Exception as e:
            logger.error(f"PyPDF2 í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "", 0
    
    @staticmethod
    def extract_text_pdfplumber(pdf_content: bytes) -> tuple[str, int]:
        """pdfplumberë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                temp_file.write(pdf_content)
                temp_file_path = temp_file.name
            
            try:
                with pdfplumber.open(temp_file_path) as pdf:
                    text = ""
                    pages_count = len(pdf.pages)
                    
                    for page in pdf.pages:
                        try:
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                        except Exception as e:
                            logger.warning(f"pdfplumber í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                            continue
                    
                    return text.strip(), pages_count
            finally:
                os.unlink(temp_file_path)
                
        except Exception as e:
            logger.error(f"pdfplumber í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "", 0
    
    @staticmethod
    def extract_text_hybrid(pdf_content: bytes) -> tuple[str, int, str]:
        """í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë‘ ë°©ë²• ëª¨ë‘ ì‹œë„)"""
        # ë¨¼ì € pdfplumber ì‹œë„
        text_plumber, pages_plumber = PDFTextExtractor.extract_text_pdfplumber(pdf_content)
        
        # PyPDF2ë„ ì‹œë„
        text_pypdf2, pages_pypdf2 = PDFTextExtractor.extract_text_pypdf2(pdf_content)
        
        # ë” ë§ì€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œ ë°©ë²• ì„ íƒ
        if len(text_plumber) > len(text_pypdf2):
            return text_plumber, pages_plumber, "pdfplumber"
        elif len(text_pypdf2) > 0:
            return text_pypdf2, pages_pypdf2, "PyPDF2"
        else:
            return text_plumber, pages_plumber, "pdfplumber (fallback)"

class QAGenerator:
    """Q&A ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, api_url: str = "http://localhost:1234/v1/chat/completions"):
        self.api_url = api_url
        logger.info(f"Q&A ìƒì„±ê¸° ì´ˆê¸°í™”: API URL = {self.api_url}")
    
    def _create_qa_generation_prompt(self, text: str, category_name: str, qa_count: int, company_name: str) -> str:
        """Q&A ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""ë‹¹ì‹ ì€ íšŒì‚¬ ë‚´ê·œ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ RAG ì‹œìŠ¤í…œìš© Q&Aë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸ“‹ **ì„ë¬´**: ì•„ë˜ ì œê³µëœ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì‹¤ì œ ì§ì›ë“¤ì´ ìì£¼ ë¬¼ì–´ë³¼ ë§Œí•œ í˜„ì‹¤ì ì´ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ê³¼ ì •í™•í•œ ë‹µë³€ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ğŸ¯ **ìƒì„± ê¸°ì¤€**:
â€¢ **ì‹¤ìš©ì„±**: ì§ì›ë“¤ì´ ì‹¤ì œë¡œ ê¶ê¸ˆí•´í•  ë§Œí•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸
â€¢ **ë‹¤ì–‘ì„±**: ê¸°ë³¸ ê°œë…ë¶€í„° ì„¸ë¶€ ì ˆì°¨ê¹Œì§€ ë‹¤ì–‘í•œ ìˆ˜ì¤€ì˜ ì§ˆë¬¸
â€¢ **ì™„ì „ì„±**: ê° ë‹µë³€ì€ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì™„ì „í•˜ê³  ëª…í™•í•˜ê²Œ
â€¢ **ì°¸ì¡° í‘œì‹œ**: ê°€ëŠ¥í•œ ê²½ìš° ì¡°í•­ ë²ˆí˜¸ë‚˜ ì„¹ì…˜ í‘œì‹œ
â€¢ **ìƒí™©ë³„ ì§ˆë¬¸**: "~ì¸ ê²½ìš°", "~í•  ë•Œ" ë“± êµ¬ì²´ì  ìƒí™© í¬í•¨

ğŸ” **ì§ˆë¬¸ ìœ í˜• ì˜ˆì‹œ**:
â€¢ ê¸°ë³¸ ê°œë…: "~ì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?", "~ë€ ë¬´ì—‡ì¸ê°€ìš”?"
â€¢ ì ˆì°¨/ë°©ë²•: "~ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "~ì˜ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
â€¢ ì¡°ê±´/ê¸°ì¤€: "~ì˜ ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?", "ì–¸ì œ ~í•´ì•¼ í•˜ë‚˜ìš”?"
â€¢ ì±…ì„/ê¶Œí•œ: "ëˆ„ê°€ ~ë¥¼ ë‹´ë‹¹í•˜ë‚˜ìš”?", "~ì˜ ê¶Œí•œì€ ëˆ„êµ¬ì—ê²Œ ìˆë‚˜ìš”?"
â€¢ ì˜ˆì™¸/íŠ¹ìˆ˜ìƒí™©: "~ì¸ ê²½ìš° ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "ì˜ˆì™¸ì ìœ¼ë¡œ ~í•  ìˆ˜ ìˆë‚˜ìš”?"
â€¢ ì œì¬/ê²°ê³¼: "~í•˜ì§€ ì•Šìœ¼ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", "ìœ„ë°˜ì‹œ ì²˜ë²Œì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"

ğŸ“– **ë¶„ì„í•  ë¬¸ì„œ**:
ì¹´í…Œê³ ë¦¬: {category_name}
íšŒì‚¬ëª…: {company_name}

ë¬¸ì„œ ë‚´ìš©:
{text}

ğŸ“ **ì¶œë ¥ í˜•ì‹** (ì •í™•íˆ ì´ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ):
[
  {{
    "category": "{category_name}",
    "faqs": [
      {{
        "question": "êµ¬ì²´ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸",
        "answer": "ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì™„ì „í•˜ê³  ì •í™•í•œ ë‹µë³€ (ê°€ëŠ¥í•œ ê²½ìš° ì¡°í•­ ë²ˆí˜¸ í¬í•¨)"
      }},
      ... (ì´ {qa_count}ê°œ ì´ìƒ)
    ]
  }}
]

ğŸš¨ **ì¤‘ìš” ì§€ì¹¨**:
â€¢ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”
â€¢ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
â€¢ ê° ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ í•˜ì„¸ìš”
â€¢ ì§ˆë¬¸ì€ ìì—°ìŠ¤ëŸ½ê³  êµ¬ì²´ì ìœ¼ë¡œ ë§Œë“œì„¸ìš”
â€¢ ë‹µë³€ì€ ëª…í™•í•˜ê³  ì™„ì „í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
â€¢ ìµœì†Œ {qa_count}ê°œ ì´ìƒì˜ Q&Aë¥¼ ìƒì„±í•˜ì„¸ìš”"""

    def generate_qa_from_text(self, text: str, category_name: str, qa_count: int = 10, company_name: str = "íšŒì‚¬") -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ë¡œë¶€í„° Q&A ìƒì„±"""
        logger.info(f"Q&A ìƒì„± ì‹œì‘: {category_name}, ëª©í‘œ ìˆ˜ëŸ‰: {qa_count}ê°œ")
        
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_qa_generation_prompt(text, category_name, qa_count, company_name)
            
            # LLM API í˜¸ì¶œ
            response = requests.post(
                self.api_url,
                json={
                    "model": "qwen3-30b-a3b-mlx",
                    "messages": [
                        {"role": "system", "content": "ë‹¹ì‹ ì€ íšŒì‚¬ ë‚´ê·œ ë¬¸ì„œ ë¶„ì„ ë° Q&A ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.3,  # ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜•
                    "max_tokens": 4000,  # ì¶©ë¶„í•œ í† í° ìˆ˜
                    "top_p": 0.9,
                    "frequency_penalty": 0.2,
                    "presence_penalty": 0.1
                },
                timeout=180  # 3ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if response.status_code == 200:
                response_text = response.json()['choices'][0]['message']['content']
                
                # JSON íŒŒì‹± ì‹œë„
                try:
                    # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    json_start = response_text.find('[')
                    json_end = response_text.rfind(']') + 1
                    
                    if json_start != -1 and json_end > json_start:
                        json_text = response_text[json_start:json_end]
                        qa_data = json.loads(json_text)
                        
                        # ë°ì´í„° ê²€ì¦
                        if isinstance(qa_data, list) and len(qa_data) > 0:
                            category_data = qa_data[0]
                            if 'faqs' in category_data and len(category_data['faqs']) > 0:
                                logger.info(f"âœ… Q&A ìƒì„± ì™„ë£Œ: {len(category_data['faqs'])}ê°œ")
                                return {
                                    'status': 'success',
                                    'data': qa_data,
                                    'generated_count': len(category_data['faqs'])
                                }
                    
                    # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ì¬ì‹œë„
                    logger.warning("JSON íŒŒì‹± ì‹¤íŒ¨, ì‘ë‹µ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œë„...")
                    return self._parse_fallback_response(response_text, category_name)
                    
                except json.JSONDecodeError as e:
                    logger.error(f"JSON ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                    return self._parse_fallback_response(response_text, category_name)
            else:
                logger.error(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                return {'status': 'error', 'message': f'LLM API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}'}
                
        except requests.exceptions.Timeout:
            logger.error("LLM API íƒ€ì„ì•„ì›ƒ")
            return {'status': 'error', 'message': 'LLM API ì‘ë‹µ íƒ€ì„ì•„ì›ƒ'}
        except Exception as e:
            logger.error(f"Q&A ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return {'status': 'error', 'message': f'Q&A ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}'}
    
    def _parse_fallback_response(self, response_text: str, category_name: str) -> Dict[str, Any]:
        """ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ì‹œ ëŒ€ì•ˆ íŒŒì‹±"""
        try:
            # ê°„ë‹¨í•œ Q&A íŒ¨í„´ìœ¼ë¡œ íŒŒì‹± ì‹œë„
            qa_pairs = []
            lines = response_text.split('\n')
            current_question = None
            current_answer = ""
            
            for line in lines:
                line = line.strip()
                if line.startswith('"question":') or line.startswith('question:'):
                    if current_question and current_answer:
                        qa_pairs.append({
                            'question': current_question,
                            'answer': current_answer.strip()
                        })
                    # ì§ˆë¬¸ ì¶”ì¶œ
                    current_question = line.split(':', 1)[1].strip().strip('"').strip(',')
                    current_answer = ""
                elif line.startswith('"answer":') or line.startswith('answer:'):
                    # ë‹µë³€ ì¶”ì¶œ
                    current_answer = line.split(':', 1)[1].strip().strip('"').strip(',')
                elif current_answer and line and not line.startswith('{') and not line.startswith('}'):
                    current_answer += " " + line
            
            # ë§ˆì§€ë§‰ Q&A ì¶”ê°€
            if current_question and current_answer:
                qa_pairs.append({
                    'question': current_question,
                    'answer': current_answer.strip()
                })
            
            if qa_pairs:
                fallback_data = [{
                    'category': category_name,
                    'faqs': qa_pairs
                }]
                logger.info(f"âœ… ëŒ€ì•ˆ íŒŒì‹± ì„±ê³µ: {len(qa_pairs)}ê°œ Q&A")
                return {
                    'status': 'success',
                    'data': fallback_data,
                    'generated_count': len(qa_pairs),
                    'note': 'ëŒ€ì•ˆ íŒŒì‹± ë°©ë²• ì‚¬ìš©ë¨'
                }
            
            return {'status': 'error', 'message': 'ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨'}
        except Exception as e:
            logger.error(f"ëŒ€ì•ˆ íŒŒì‹±ë„ ì‹¤íŒ¨: {e}")
            return {'status': 'error', 'message': f'ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}'}

# ê¸°ì¡´ RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (ìƒëµ)
class CompanyRegulationsRAGSystem:
    def __init__(self, model_name: str = "nlpai-lab-KURE-v1", persist_directory: str = "./chroma_db"):
        """
        ê°œì„ ëœ ChromaDB ê¸°ë°˜ íšŒì‚¬ ì „ì²´ ë‚´ê·œ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (í–¥ìƒëœ ê²€ìƒ‰ ë° ì •ë³´ëŸ‰)

        Args:
            model_name: ./models ë””ë ‰í† ë¦¬ì— ìˆëŠ” ëª¨ë¸ í´ë” ì´ë¦„
            persist_directory: ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬
        """
        model_path = os.path.join("./models", model_name)

        # ./models ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(model_path):
            logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
            logger.info("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:")
            models_dir = "./models"
            if os.path.exists(models_dir):
                for item in os.listdir(models_dir):
                    if os.path.isdir(os.path.join(models_dir, item)):
                        logger.info(f"   - {item}")
            else:
                logger.error(f"âŒ ./models ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            raise FileNotFoundError(f"ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        
        logger.info(f"ğŸ”„ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")

        try:
            # ì €ì¥ëœ ëª¨ë¸ ì§ì ‘ ë¡œë“œ
            self.model = SentenceTransformer(model_path)
            logger.info(f"âœ… ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
            raise
        
        self.persist_directory = persist_directory

        # ChromaDB ì´ˆê¸°í™” (ê°œì„ ëœ ì˜¤ë¥˜ ì²˜ë¦¬)
        self.chroma_client = chromadb.PersistentClient(path=self.persist_directory)
        try:
            self.collection = self.chroma_client.get_collection(name="company_regulations")
            logger.info("ğŸ“ ê¸°ì¡´ company_regulations ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ")
        except Exception as get_error:
            logger.info(f"ğŸ“ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì—†ìŒ ë˜ëŠ” ì˜¤ë¥˜ ({get_error}), ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì¤‘...")
            try:
                self.collection = self.chroma_client.create_collection(
                    name="company_regulations",
                    metadata={"description": "í–¥ìƒëœ íšŒì‚¬ ì „ì²´ ë‚´ê·œ ë²¡í„° ê²€ìƒ‰ ì»¬ë ‰ì…˜"}
                )
                logger.info("ğŸ“ ìƒˆë¡œìš´ company_regulations ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
            except Exception as create_error:
                logger.error(f"âŒ ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {create_error}")
                # ëŒ€ì•ˆìœ¼ë¡œ ì„ì‹œ ì´ë¦„ ì‚¬ìš©
                import time
                temp_name = f"company_regulations_temp_{int(time.time())}"
                logger.info(f"ğŸ”„ ì„ì‹œ ì»¬ë ‰ì…˜ '{temp_name}' ìƒì„± ì‹œë„...")
                self.collection = self.chroma_client.create_collection(
                    name=temp_name,
                    metadata={"description": "ì„ì‹œ íšŒì‚¬ ë‚´ê·œ ë²¡í„° ê²€ìƒ‰ ì»¬ë ‰ì…˜"}
                )
                logger.info(f"ğŸ“ ì„ì‹œ ì»¬ë ‰ì…˜ '{temp_name}' ìƒì„± ì™„ë£Œ")

        logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        logger.info(f"ğŸ’¾ ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬: {self.persist_directory}")
    
    def load_company_regulations_data(self, data_directory: str = "./data_json"):
        """íšŒì‚¬ ì „ì²´ ë‚´ê·œ ë°ì´í„° ë¡œë“œ - í–¥ìƒëœ ì²­í‚¹ ì „ëµ"""
        logger.info(f"íšŒì‚¬ ì „ì²´ ë‚´ê·œ ë°ì´í„° ë¡œë“œ ì‹œì‘: {data_directory}")
        try:
            if not os.path.exists(data_directory):
                logger.error(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_directory}")
                return False
            
            json_files = glob.glob(os.path.join(data_directory, "*.json"))
            
            if not json_files:
                logger.warning(f"âš ï¸ {data_directory} ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            logger.info(f"ğŸ” ë°œê²¬ëœ JSON íŒŒì¼: {len(json_files)}ê°œ")
            
            self.regulations_data = []
            self.main_categories = {}
            
            for json_file in json_files:
                file_name = os.path.basename(json_file)
                main_category = os.path.splitext(file_name)[0]
                
                logger.info(f"ğŸ“‚ ë¡œë”© ì¤‘: {file_name} â†’ ëŒ€êµ¬ë¶„: {main_category}")
                
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    category_count = 0
                    faq_count = 0
                    
                    for category_section in data:
                        sub_category = category_section['category']
                        category_count += 1
                        
                        for faq in category_section['faqs']:
                            question = faq['question']
                            answer = faq['answer']
                            
                            # ê¸°ë³¸ Q&A ë‹¨ìœ„ ì €ì¥
                            base_id = str(uuid.uuid4())
                            regulation_item = {
                                'id': base_id,
                                'main_category': main_category,
                                'sub_category': sub_category,
                                'question': question,
                                'answer': answer,
                                'text': f"{question} {answer}",
                                'source_file': file_name,
                                'chunk_type': 'qa_full',
                                'chunk_id': 0
                            }
                            self.regulations_data.append(regulation_item)
                            
                            # í–¥ìƒëœ ì²­í‚¹: ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë¶„ë¦¬í•˜ì—¬ ì¶”ê°€ ì €ì¥
                            # 1) ì§ˆë¬¸ ì¤‘ì‹¬ ì²­í¬
                            question_item = {
                                'id': f"{base_id}_q",
                                'main_category': main_category,
                                'sub_category': sub_category,
                                'question': question,
                                'answer': answer,
                                'text': f"ì§ˆë¬¸: {question}",
                                'source_file': file_name,
                                'chunk_type': 'question_focused',
                                'chunk_id': 1
                            }
                            self.regulations_data.append(question_item)
                            
                            # 2) ë‹µë³€ ì¤‘ì‹¬ ì²­í¬
                            answer_item = {
                                'id': f"{base_id}_a",
                                'main_category': main_category,
                                'sub_category': sub_category,
                                'question': question,
                                'answer': answer,
                                'text': f"ë‹µë³€: {answer} (ê´€ë ¨ ì§ˆë¬¸: {question})",
                                'source_file': file_name,
                                'chunk_type': 'answer_focused',
                                'chunk_id': 2
                            }
                            self.regulations_data.append(answer_item)
                            
                            # 3) ê¸´ ë‹µë³€ì¸ ê²½ìš° ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì¶”ê°€ ì €ì¥
                            if len(answer) > 200:  # ê¸´ ë‹µë³€ì¸ ê²½ìš°
                                sentences = re.split(r'[.!?]\s+', answer)
                                for i, sentence in enumerate(sentences):
                                    if len(sentence.strip()) > 20:  # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë§Œ
                                        sentence_item = {
                                            'id': f"{base_id}_s{i}",
                                            'main_category': main_category,
                                            'sub_category': sub_category,
                                            'question': question,
                                            'answer': answer,
                                            'text': f"{sentence.strip()} (ì¶œì²˜: {question})",
                                            'source_file': file_name,
                                            'chunk_type': 'sentence_level',
                                            'chunk_id': 10 + i
                                        }
                                        self.regulations_data.append(sentence_item)
                            
                            faq_count += 1
                    
                    self.main_categories[main_category] = {
                        'file_name': file_name,
                        'sub_categories': category_count,
                        'total_faqs': faq_count
                    }
                    
                    logger.info(f"  â†’ ì†Œêµ¬ë¶„: {category_count}ê°œ, ê·œì •: {faq_count}ê°œ ë¡œë“œë¨")
                    
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨ {json_file}: {e}", exc_info=True)
                    continue
                except Exception as e:
                    logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {json_file}: {e}", exc_info=True)
                    continue
            
            total_chunks = len(self.regulations_data)
            total_files = len(self.main_categories)
            
            logger.info(f"âœ… íšŒì‚¬ ì „ì²´ ë‚´ê·œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
            logger.info(f"  - ëŒ€êµ¬ë¶„(íŒŒì¼): {total_files}ê°œ")
            logger.info(f"  - ì´ ì²­í¬: {total_chunks}ê°œ (í–¥ìƒëœ ì²­í‚¹ ì ìš©)")
            
            # ì²­í‚¹ íƒ€ì…ë³„ í†µê³„
            chunk_stats = {}
            for item in self.regulations_data:
                chunk_type = item.get('chunk_type', 'unknown')
                chunk_stats[chunk_type] = chunk_stats.get(chunk_type, 0) + 1
            
            for chunk_type, count in chunk_stats.items():
                logger.info(f"  - {chunk_type}: {count}ê°œ")
            
            return total_chunks > 0
            
        except Exception as e:
            logger.error(f"âŒ íšŒì‚¬ ë‚´ê·œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
            return False

    def search_with_enhanced_retrieval(self, query: str, top_k: int = 20, main_category_filter: str = None, min_relevance_score: float = 0.3) -> List[Dict[str, Any]]:
        """í–¥ìƒëœ ê²€ìƒ‰ - ë‹¤ì¤‘ ì¿¼ë¦¬, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, í–¥ìƒëœ ì¬ë­í‚¹ (í™•ì¥ëœ ì •ë³´ëŸ‰)"""
        logger.info(f"í–¥ìƒëœ RAG ê²€ìƒ‰: '{query[:50]}...', í•„í„°='{main_category_filter}', top_k={top_k}")
        # ì‹¤ì œ êµ¬í˜„ì€ ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ
        return []
    
    def get_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            count = self.collection.count()
            main_cats = self.main_categories if hasattr(self, 'main_categories') else {}
            
            # ì²­í‚¹ í†µê³„
            chunk_stats = {}
            if hasattr(self, 'regulations_data'):
                for item in self.regulations_data:
                    chunk_type = item.get('chunk_type', 'unknown')
                    chunk_stats[chunk_type] = chunk_stats.get(chunk_type, 0) + 1
            
            return {
                'total_documents': count,
                'collection_name': self.collection.name,
                'persist_directory': self.persist_directory,
                'is_ready': count > 0,
                'main_categories': main_cats,
                'total_main_categories': len(main_cats),
                'chunk_statistics': chunk_stats,
                'enhanced_features': [
                    'ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰',
                    'í–¥ìƒëœ ì²­í‚¹ ì „ëµ',
                    'í•˜ì´ë¸Œë¦¬ë“œ ì¬ë­í‚¹',
                    'ëŒ€í­ í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° (25ê°œ)',
                    'ë‹¤ì–‘ì„± ë³´ì¥ ì•Œê³ ë¦¬ì¦˜',
                    '2.5ë°° í™•ì¥ëœ ê²€ìƒ‰ ë²”ìœ„ (20ê°œ)'
                ]
            }
        except Exception as e:
            logger.error(f"âŒ í†µê³„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
            return {'total_documents': 0, 'is_ready': False, 'main_categories': {}, 'total_main_categories': 0}

# FastAPI ì•± ì„¤ì •
app = FastAPI(
    title="PDF to JSON RAG Converter + ëŒ€í­ í™•ì¥ëœ íšŒì‚¬ ë‚´ê·œ RAG ì‹œìŠ¤í…œ",
    description="""
    **FastAPI ê¸°ë°˜ PDF to JSON RAG Converter + ëŒ€í­ í™•ì¥ëœ ì •ë³´ëŸ‰ íšŒì‚¬ ë‚´ê·œ RAG ì‹œìŠ¤í…œ**
    
    ## ìƒˆë¡œìš´ PDF ì²˜ë¦¬ ê¸°ëŠ¥
    - ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - ğŸ¤– LLM ê¸°ë°˜ ìë™ Q&A ìƒì„± (qwen3-30b-a3b-mlx)
    - ğŸ“ RAGìš© JSON í˜•ì‹ ìë™ ë³€í™˜
    - ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyPDF2 + pdfplumber)
    
    ## ê¸°ì¡´ RAG ì‹œìŠ¤í…œ íŠ¹ì§•
    - ğŸ” ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ (ì›ë³¸ + ë³€í˜• ì¿¼ë¦¬ë¡œ ë‹¤ê°ë„ ê²€ìƒ‰)
    - ğŸ“ í–¥ìƒëœ ì²­í‚¹ ì „ëµ (QA + ì§ˆë¬¸ + ë‹µë³€ + ë¬¸ì¥ ë‹¨ìœ„)
    - ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì¬ë­í‚¹ (ë²¡í„° + í‚¤ì›Œë“œ + ë‹¤ì¤‘ì¿¼ë¦¬ ë§¤ì¹­)
    - ğŸ“Š ëŒ€í­ í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° (ìµœëŒ€ 25ê°œ â†’ í›¨ì”¬ ë” í’ë¶€í•œ ì •ë³´)
    - ğŸŒŸ ë‹¤ì–‘ì„± ë³´ì¥ ì•Œê³ ë¦¬ì¦˜ (ì¤‘ë³µ ì œê±° + í’ˆì§ˆ ìœ ì§€)
    - ğŸ“‰ ë‚®ì€ ê´€ë ¨ì„± ì„ê³„ê°’ (0.25, ë¶€ë¶„ ê´€ë ¨ ì •ë³´ë„ í¬í•¨)
    - ğŸ¯ ì •í™•ì„± ìµœìš°ì„  (Temperature: 0.1)
    - ğŸ“ ë” ê¸´ ì‘ë‹µ í—ˆìš© (2500 í† í°)
    
    ## PDF ì²˜ë¦¬ ì„±ëŠ¥
    - **Q&A ìƒì„± ìˆ˜**: 5-50ê°œ (ì‚¬ìš©ì ì§€ì • ê°€ëŠ¥)
    - **ì²˜ë¦¬ ì‹œê°„**: í‰ê·  30-60ì´ˆ (ë¬¸ì„œ í¬ê¸°ì— ë”°ë¼)
    - **ì§€ì› í˜•ì‹**: PDF (í…ìŠ¤íŠ¸ ê¸°ë°˜)
    - **ìµœëŒ€ íŒŒì¼ í¬ê¸°**: 10MB
    """,
    version="4.0.0",
    contact={
        "name": "RAG ì‹œìŠ¤í…œ ê°œë°œíŒ€",
        "email": "dev@company.com"
    }
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ê°ì²´ë“¤
rag_system: CompanyRegulationsRAGSystem = None
qa_generator: QAGenerator = None

def initialize_system():
    """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global rag_system, qa_generator
    
    if rag_system is not None and qa_generator is not None:
        logger.info("ì‹œìŠ¤í…œì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    logger.info("í–¥ìƒëœ RAG ì‹œìŠ¤í…œ + PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
    
    try:
        # ./models ë””ë ‰í† ë¦¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
        models_dir = "./models"
        available_models = []
        
        if os.path.exists(models_dir):
            for item in os.listdir(models_dir):
                model_path = os.path.join(models_dir, item)
                if os.path.isdir(model_path):
                    # sentence-transformers ëª¨ë¸ì¸ì§€ í™•ì¸
                    config_file = os.path.join(model_path, "config.json")
                    if os.path.exists(config_file):
                        available_models.append(item)
            
            logger.info(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œì»¬ ëª¨ë¸: {available_models}")
        else:
            logger.error(f"âŒ ./models ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            logger.info("ğŸ’¡ ./models ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•˜ê³  sentence-transformers ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
            return
        
        if not available_models:
            logger.error("âŒ ./models ë””ë ‰í† ë¦¬ì— ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            logger.info("ğŸ’¡ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
            logger.info("   - nlpai-lab/KURE-v1 (í•œêµ­ì–´ íŠ¹í™”)")
            logger.info("   - sentence-transformers/all-MiniLM-L6-v2 (ë‹¤êµ­ì–´)")
            logger.info("   - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            return
        
        # ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì‚¬ìš© (ë˜ëŠ” íŠ¹ì • ëª¨ë¸ ì§€ì •)
        model_to_use = available_models[0]
        
        # í•œêµ­ì–´ ëª¨ë¸ ìš°ì„  ì„ íƒ
        for model in available_models:
            if "kure" in model.lower() or "korean" in model.lower():
                model_to_use = model
                break
        
        logger.info(f"ğŸ¯ ì„ íƒëœ ëª¨ë¸: {model_to_use}")
        
        rag_system = CompanyRegulationsRAGSystem(model_name=model_to_use)
        qa_generator = QAGenerator()
        
        data_directory = "./data_json"
        
        if os.path.exists(data_directory):
            logger.info("íšŒì‚¬ ë‚´ê·œ ë°ì´í„° ë¡œë“œ ë° í–¥ìƒëœ ì¸ë±ìŠ¤ êµ¬ì¶•...")
            if rag_system.load_company_regulations_data(data_directory):
                stats = rag_system.get_stats()
                logger.info(f"âœ… í–¥ìƒëœ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {stats['total_documents']}ê°œ ì²­í¬")
                logger.info(f"ğŸ“Š ì²­í‚¹ í†µê³„: {stats.get('chunk_statistics', {})}")
            else:
                logger.error("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
        else:
            logger.warning(f"âš ï¸ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {data_directory}. ë¹ˆ ì¸ë±ìŠ¤ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            logger.info("ğŸ’¡ PDF ì—…ë¡œë“œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.critical(f"ğŸ”¥ í–¥ìƒëœ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        rag_system = None
        qa_generator = None

# ìƒˆë¡œìš´ PDF ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/models/check", summary="ë¡œì»¬ ëª¨ë¸ ìƒíƒœ í™•ì¸", description="./models ë””ë ‰í† ë¦¬ì˜ ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
async def check_local_models():
    """ë¡œì»¬ ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    try:
        models_dir = "./models"
        model_status = {
            "models_directory_exists": os.path.exists(models_dir),
            "available_models": [],
            "recommended_models": [
                "nlpai-lab/KURE-v1",
                "sentence-transformers/all-MiniLM-L6-v2",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            ],
            "download_instructions": {
                "method1_python": [
                    "from sentence_transformers import SentenceTransformer",
                    "model = SentenceTransformer('nlpai-lab/KURE-v1')",
                    "model.save('./models/nlpai-lab-KURE-v1')"
                ],
                "method2_huggingface": [
                    "git lfs install",
                    "git clone https://huggingface.co/nlpai-lab/KURE-v1 ./models/nlpai-lab-KURE-v1"
                ]
            }
        }
        
        if os.path.exists(models_dir):
            for item in os.listdir(models_dir):
                model_path = os.path.join(models_dir, item)
                if os.path.isdir(model_path):
                    # ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬
                    config_file = os.path.join(model_path, "config.json")
                    has_config = os.path.exists(config_file)
                    
                    # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    model_files = []
                    for file_pattern in ["*.bin", "*.safetensors", "*.pt"]:
                        model_files.extend(glob.glob(os.path.join(model_path, file_pattern)))
                    
                    model_info = {
                        "name": item,
                        "path": model_path,
                        "has_config": has_config,
                        "has_model_files": len(model_files) > 0,
                        "model_files": [os.path.basename(f) for f in model_files],
                        "is_valid": has_config and len(model_files) > 0,
                        "size_mb": sum(os.path.getsize(os.path.join(model_path, f)) for f in os.listdir(model_path) if os.path.isfile(os.path.join(model_path, f))) / (1024*1024) if os.path.exists(model_path) else 0
                    }
                    
                    model_status["available_models"].append(model_info)
        
        # ìœ íš¨í•œ ëª¨ë¸ ê°œìˆ˜
        valid_models = [m for m in model_status["available_models"] if m["is_valid"]]
        model_status["valid_models_count"] = len(valid_models)
        model_status["status"] = "ready" if len(valid_models) > 0 else "needs_setup"
        
        return model_status
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")

@app.post("/models/download", summary="ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ", description="ì¶”ì²œ sentence-transformers ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
async def download_recommended_model(
    model_name: str = "nlpai-lab/KURE-v1",
    force_redownload: bool = False
):
    """ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    try:
        # ëª¨ë¸ëª… ê²€ì¦
        allowed_models = [
            "nlpai-lab/KURE-v1",
            "sentence-transformers/all-MiniLM-L6-v2", 
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        ]
        
        if model_name not in allowed_models:
            raise HTTPException(
                status_code=400, 
                detail=f"í—ˆìš©ëœ ëª¨ë¸ì´ ì•„ë‹™ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {allowed_models}"
            )
        
        # ë¡œì»¬ ì €ì¥ ê²½ë¡œ
        safe_model_name = model_name.replace("/", "-")
        model_path = os.path.join("./models", safe_model_name)
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°
        if os.path.exists(model_path) and not force_redownload:
            return {
                "status": "already_exists",
                "message": f"ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {model_path}",
                "model_name": model_name,
                "local_path": model_path,
                "size_mb": sum(os.path.getsize(os.path.join(model_path, f)) for f in os.listdir(model_path) if os.path.isfile(os.path.join(model_path, f))) / (1024*1024)
            }
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("./models", exist_ok=True)
        
        logger.info(f"ğŸ“¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_name} â†’ {model_path}")
        
        # SentenceTransformerë¡œ ë‹¤ìš´ë¡œë“œ
        start_time = time.time()
        
        try:
            model = SentenceTransformer(model_name)
            model.save(model_path)
            
            download_time = time.time() - start_time
            
            # í¬ê¸° ê³„ì‚°
            total_size = sum(os.path.getsize(os.path.join(model_path, f)) for f in os.listdir(model_path) if os.path.isfile(os.path.join(model_path, f)))
            size_mb = total_size / (1024*1024)
            
            logger.info(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_name} ({size_mb:.1f}MB, {download_time:.1f}ì´ˆ)")
            
            return {
                "status": "success",
                "message": f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_name}",
                "model_name": model_name,
                "local_path": model_path,
                "download_time_seconds": round(download_time, 2),
                "size_mb": round(size_mb, 1),
                "files_downloaded": os.listdir(model_path)
            }
            
        except Exception as download_error:
            logger.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {download_error}")
            
            # ì‹¤íŒ¨í•œ ê²½ìš° ë¶€ë¶„ì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì •ë¦¬
            if os.path.exists(model_path):
                import shutil
                shutil.rmtree(model_path)
            
            raise HTTPException(
                status_code=500, 
                detail=f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(download_error)}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/pdf/upload", response_model=PDFUploadResponse, summary="PDF ì—…ë¡œë“œ ë° RAGìš© JSON ìƒì„±", description="PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  LLMìœ¼ë¡œ Q&Aë¥¼ ìƒì„±í•˜ì—¬ RAGìš© JSONì„ ë§Œë“­ë‹ˆë‹¤.")
async def upload_pdf_and_generate_qa(
    file: UploadFile = File(..., description="ì—…ë¡œë“œí•  PDF íŒŒì¼"),
    category_name: str = Form(..., description="ì¹´í…Œê³ ë¦¬ ì´ë¦„", example="ë²•ì¸ì¹´ë“œ ì‚¬ìš© ì§€ì¹¨"),
    qa_count: int = Form(15, description="ìƒì„±í•  Q&A ìˆ˜", ge=5, le=50),
    company_name: str = Form("íšŒì‚¬", description="íšŒì‚¬ëª…", example="ë”ì¼€ì´êµì§ì›ë‚˜ë¼(ì£¼)")
):
    """PDF ì—…ë¡œë“œ ë° RAGìš© JSON ìë™ ìƒì„±"""
    start_time = time.time()
    
    try:
        if not qa_generator:
            raise HTTPException(status_code=503, detail="Q&A ìƒì„± ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ê²€ì¦
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        # íŒŒì¼ í¬ê¸° ê²€ì¦ (10MB ì œí•œ)
        file_content = await file.read()
        file_size = len(file_content)
        
        if file_size > 10 * 1024 * 1024:  # 10MB
            raise HTTPException(status_code=400, detail="íŒŒì¼ í¬ê¸°ëŠ” 10MB ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        logger.info(f"ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ: {file.filename} ({file_size:,} bytes)")
        
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text, pages_count, extraction_method = PDFTextExtractor.extract_text_hybrid(file_content)
        
        if not extracted_text or len(extracted_text.strip()) < 100:
            raise HTTPException(
                status_code=400, 
                detail="PDFì—ì„œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº”ëœ ì´ë¯¸ì§€ë‚˜ ë³´í˜¸ëœ PDFì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
        
        logger.info(f"ğŸ“– í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text):,}ì, {pages_count}í˜ì´ì§€ ({extraction_method})")
        
        # Q&A ìƒì„±
        qa_result = qa_generator.generate_qa_from_text(
            text=extracted_text,
            category_name=category_name,
            qa_count=qa_count,
            company_name=company_name
        )
        
        if qa_result['status'] != 'success':
            raise HTTPException(status_code=500, detail=f"Q&A ìƒì„± ì‹¤íŒ¨: {qa_result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        processing_time = time.time() - start_time
        
        # í†µê³„ ì •ë³´
        total_qa = qa_result['generated_count']
        json_data = qa_result['data']
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        statistics = {
            'total_categories': len(json_data),
            'total_qa_pairs': total_qa,
            'average_qa_per_category': total_qa // len(json_data) if json_data else 0,
            'text_length': len(extracted_text),
            'pages_processed': pages_count
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
        output_filename = f"{category_name.replace(' ', '_')}_{int(time.time())}.json"
        output_path = os.path.join("./data_json", output_filename)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("./data_json", exist_ok=True)
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ JSON íŒŒì¼ ì €ì¥: {output_path}")
        except Exception as save_error:
            logger.warning(f"âš ï¸ JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_error}")
        
        return PDFUploadResponse(
            status="success",
            message=f"PDF ì²˜ë¦¬ ë° Q&A ìƒì„± ì™„ë£Œ ({total_qa}ê°œ ìƒì„±)",
            filename=file.filename,
            file_size=file_size,
            pages_count=pages_count,
            extraction_method=extraction_method,
            processing_time=round(processing_time, 2),
            json_data=json_data,
            statistics=statistics
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/pdf/text_to_qa", response_model=QAGenerationResponse, summary="í…ìŠ¤íŠ¸ë¡œë¶€í„° Q&A ìƒì„±", description="ì œê³µëœ í…ìŠ¤íŠ¸ë¡œë¶€í„° LLMì„ ì´ìš©í•´ Q&Aë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
async def generate_qa_from_text(request: QAGenerationRequest):
    """í…ìŠ¤íŠ¸ë¡œë¶€í„° Q&A ìƒì„±"""
    start_time = time.time()
    
    try:
        if not qa_generator:
            raise HTTPException(status_code=503, detail="Q&A ìƒì„± ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        logger.info(f"ğŸ¤– í…ìŠ¤íŠ¸ ê¸°ë°˜ Q&A ìƒì„± ì‹œì‘: {request.category_name}, ëª©í‘œ: {request.qa_count}ê°œ")
        
        # Q&A ìƒì„±
        qa_result = qa_generator.generate_qa_from_text(
            text=request.text,
            category_name=request.category_name,
            qa_count=request.qa_count,
            company_name=request.company_name
        )
        
        if qa_result['status'] != 'success':
            raise HTTPException(status_code=500, detail=f"Q&A ìƒì„± ì‹¤íŒ¨: {qa_result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        processing_time = time.time() - start_time
        generated_count = qa_result['generated_count']
        json_data = qa_result['data']
        
        logger.info(f"âœ… Q&A ìƒì„± ì™„ë£Œ: {generated_count}ê°œ ({processing_time:.2f}ì´ˆ)")
        
        return QAGenerationResponse(
            status="success",
            category=request.category_name,
            generated_qa_count=generated_count,
            processing_time=round(processing_time, 2),
            json_data=json_data
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Q&A ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Q&A ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/pdf/test", summary="PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸", description="PDF ì²˜ë¦¬ ì‹œìŠ¤í…œì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
async def test_pdf_system():
    """PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    try:
        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        system_status = {
            "qa_generator_ready": qa_generator is not None,
            "rag_system_ready": rag_system is not None,
            "supported_formats": ["PDF"],
            "max_file_size": "10MB",
            "text_extractors": ["PyPDF2", "pdfplumber"],
            "llm_model": "qwen3-30b-a3b-mlx",
            "llm_api_url": qa_generator.api_url if qa_generator else None
        }
        
        # LLM API ì—°ê²° í…ŒìŠ¤íŠ¸
        if qa_generator:
            try:
                test_response = requests.get("http://localhost:1234/v1/models", timeout=5)
                system_status["llm_api_status"] = "connected" if test_response.status_code == 200 else "error"
            except:
                system_status["llm_api_status"] = "disconnected"
        
        # ìƒ˜í”Œ Q&A ìƒì„± í…ŒìŠ¤íŠ¸
        sample_text = """
        ì œ1ì¡° (ëª©ì ) ì´ ì§€ì¹¨ì€ íšŒì‚¬ì˜ íš¨ìœ¨ì ì¸ ì—…ë¬´ ìˆ˜í–‰ì„ ìœ„í•´ ì œì •ë˜ì—ˆìŠµë‹ˆë‹¤.
        ì œ2ì¡° (ì ìš©ë²”ìœ„) ë³¸ ì§€ì¹¨ì€ ì „ ì§ì›ì—ê²Œ ì ìš©ë©ë‹ˆë‹¤.
        ì œ3ì¡° (ì¤€ìˆ˜ì‚¬í•­) ì§ì›ì€ ë³¸ ì§€ì¹¨ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        
        if qa_generator:
            sample_result = qa_generator.generate_qa_from_text(
                text=sample_text,
                category_name="í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬",
                qa_count=3,
                company_name="í…ŒìŠ¤íŠ¸ íšŒì‚¬"
            )
            system_status["sample_qa_generation"] = sample_result['status']
            system_status["sample_qa_count"] = sample_result.get('generated_count', 0)
        
        return {
            "status": "healthy" if system_status["qa_generator_ready"] and system_status["rag_system_ready"] else "partial",
            "message": "PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì™„ë£Œ",
            "system_details": system_status,
            "usage_guide": {
                "upload_endpoint": "/pdf/upload",
                "text_to_qa_endpoint": "/pdf/text_to_qa",
                "supported_parameters": {
                    "qa_count": "5-50ê°œ",
                    "max_file_size": "10MB",
                    "supported_formats": "PDF"
                }
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ PDF ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

# ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ë“¤ (ê°„ì†Œí™”ëœ ë²„ì „)
@app.get("/health", response_model=HealthResponse, summary="ëŒ€í­ í™•ì¥ëœ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸", description="ëŒ€í­ í™•ì¥ëœ RAG ì‹œìŠ¤í…œì˜ ìƒíƒœì™€ í†µê³„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    if rag_system is None:
        raise HTTPException(
            status_code=503,
            detail={
                "status": "initializing_or_error",
                "rag_ready": False,
                "regulations_count": 0,
                "main_categories_count": 0,
                "improvements": "ëŒ€í­ í™•ì¥ëœ ì •ë³´ëŸ‰, 2.5ë°° ê²€ìƒ‰ ë²”ìœ„, ì •í™•ë„ ê°œì„ , PDF ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€",
                "message": "ëŒ€í­ í™•ì¥ëœ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ì´ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
        )

    stats = rag_system.get_stats()
    
    status = "healthy" if stats['is_ready'] else "unhealthy"

    return HealthResponse(
        status=status,
        rag_ready=stats['is_ready'],
        regulations_count=stats['total_documents'],
        main_categories_count=stats.get('total_main_categories', 0),
        chunk_statistics=stats.get('chunk_statistics', {}),
        improvements="ëŒ€í­ í™•ì¥ëœ ì •ë³´ëŸ‰, 2.5ë°° ê²€ìƒ‰ ë²”ìœ„, ì •í™•ë„ ê°œì„ , PDF ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€",
        enhanced_features=stats.get('enhanced_features', []) + ["PDF to JSON ë³€í™˜", "ìë™ Q&A ìƒì„±"]
    )

@app.post("/search", response_model=SearchResponse, summary="ëŒ€í­ í™•ì¥ëœ ë‚´ê·œ ê²€ìƒ‰", description="ëŒ€í­ í™•ì¥ëœ ë‹¤ì¤‘ ì¿¼ë¦¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë‚´ê·œë¥¼ ì°¾ìŠµë‹ˆë‹¤.")
async def search_regulations(request: SearchRequest):
    """ëŒ€í­ í™•ì¥ëœ íšŒì‚¬ ë‚´ê·œ ê²€ìƒ‰"""
    if not rag_system:
        raise HTTPException(status_code=503, detail="ëŒ€í­ í™•ì¥ëœ RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    results = rag_system.search_with_enhanced_retrieval(
        request.query, 
        request.top_k,
        request.main_category_filter, 
        min_relevance_score=0.3
    )
    
    search_results = [SearchResult(**result) for result in results]
    
    return SearchResponse(
        query=request.query,
        results=search_results,
        count=len(results),
        main_category_filter=request.main_category_filter,
        search_type="ëŒ€í­ í™•ì¥ëœ ë‹¤ì¤‘ì¿¼ë¦¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰",
        min_relevance=0.3,
        enhanced_features=[
            "ë‹¤ì¤‘ ì¿¼ë¦¬ ë³€í˜•",
            "í–¥ìƒëœ ì²­í‚¹",
            "í•˜ì´ë¸Œë¦¬ë“œ ì¬ë­í‚¹",
            "ë‹¤ì–‘ì„± ë³´ì¥",
            f"2.5ë°° í™•ì¥ëœ ê²€ìƒ‰ ë²”ìœ„ (ìµœëŒ€ {request.top_k}ê°œ)",
            "PDF ìƒì„± Q&A í¬í•¨"
        ]
    )

# ì‹œì‘ ì´ë²¤íŠ¸
@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    initialize_system()

if __name__ == '__main__':
    print("=" * 80)
    print("ğŸ¢ FastAPI ê¸°ë°˜ PDF to JSON RAG Converter + ëŒ€í­ í™•ì¥ëœ ì •ë³´ëŸ‰ íšŒì‚¬ ë‚´ê·œ RAG ì„œë²„")
    print("=" * 80)
    print("ğŸ†• ìƒˆë¡œìš´ PDF ì²˜ë¦¬ ê¸°ëŠ¥:")
    print("   ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    print("   ğŸ¤– LLM ê¸°ë°˜ ìë™ Q&A ìƒì„± (qwen3-30b-a3b-mlx)")
    print("   ğŸ“ RAGìš© JSON í˜•ì‹ ìë™ ë³€í™˜")
    print("   ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyPDF2 + pdfplumber)")
    print("   âš¡ ìµœëŒ€ 50ê°œ Q&A ìë™ ìƒì„±")
    print("   ğŸ’¾ ìë™ JSON íŒŒì¼ ì €ì¥")
    print("=" * 80)
    print("âœ¨ ê¸°ì¡´ RAG ì‹œìŠ¤í…œ ê°œì„ ì‚¬í•­:")
    print("   ğŸ” ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ (ì›ë³¸ + ë³€í˜• ì¿¼ë¦¬ë¡œ ë‹¤ê°ë„ ê²€ìƒ‰)")
    print("   ğŸ“ í–¥ìƒëœ ì²­í‚¹ ì „ëµ (QA + ì§ˆë¬¸ + ë‹µë³€ + ë¬¸ì¥ ë‹¨ìœ„)")
    print("   ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì¬ë­í‚¹ (ë²¡í„° + í‚¤ì›Œë“œ + ë‹¤ì¤‘ì¿¼ë¦¬ ë§¤ì¹­)")
    print("   ğŸ“Š ëŒ€í­ í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° (10ê°œ â†’ 25ê°œ, 2.5ë°°)")
    print("   ğŸŒŸ ë‹¤ì–‘ì„± ë³´ì¥ ì•Œê³ ë¦¬ì¦˜ (ì¤‘ë³µ ì œê±° + í’ˆì§ˆ ìœ ì§€)")
    print("   ğŸš€ ëŒ€í­ í™•ì¥ëœ ê²€ìƒ‰ ë²”ìœ„ (8ê°œ â†’ 20ê°œ, 2.5ë°°)")
    print("=" * 80)
    print("ğŸ”§ FastAPI ì—”ë“œí¬ì¸íŠ¸:")
    print("   ğŸ†• GET  /models/check          : ğŸ“‹ ë¡œì»¬ ëª¨ë¸ ìƒíƒœ í™•ì¸")
    print("   ğŸ†• POST /models/download       : ğŸ“¦ ì¶”ì²œ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ")
    print("   ğŸ†• POST /pdf/upload           : ğŸ“„ PDF ì—…ë¡œë“œ â†’ JSON ë³€í™˜ ğŸ¯")
    print("   ğŸ†• POST /pdf/text_to_qa       : ğŸ“ í…ìŠ¤íŠ¸ â†’ Q&A ìƒì„±")
    print("   ğŸ†• GET  /pdf/test             : ğŸ§ª PDF ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("   - GET  /health                : ëŒ€í­ í™•ì¥ëœ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸")
    print("   - POST /search                : í™•ì¥ëœ ë‹¤ì¤‘ì¿¼ë¦¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰")
    print("   - GET  /docs                  : ğŸ¯ Swagger UI ë¬¸ì„œ (API í…ŒìŠ¤íŠ¸ ê°€ëŠ¥) ğŸ¯")
    print("=" * 80)
    print("ğŸ“– API ë¬¸ì„œ ë° í…ŒìŠ¤íŠ¸:")
    print("   http://localhost:5000/docs  â† ğŸ¯ ì—¬ê¸°ì„œ PDF ì—…ë¡œë“œ & ëª¨ë¸ ê´€ë¦¬ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”!")
    print("   http://localhost:5000/redoc â† ëŒ€ì•ˆ ë¬¸ì„œ")
    print("=" * 80)
    print("ğŸ”§ ëª¨ë¸ ì„¤ì • ê°€ì´ë“œ:")
    print("   1. ëª¨ë¸ ìƒíƒœ í™•ì¸: GET /models/check")
    print("   2. ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ: POST /models/download")
    print("      - nlpai-lab/KURE-v1 (í•œêµ­ì–´ íŠ¹í™”, ê¶Œì¥)")
    print("      - sentence-transformers/all-MiniLM-L6-v2 (ë‹¤êµ­ì–´)")
    print("   3. ë˜ëŠ” ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ:")
    print("      python -c \"from sentence_transformers import SentenceTransformer;")
    print("      SentenceTransformer('nlpai-lab/KURE-v1').save('./models/nlpai-lab-KURE-v1')\"")
    print("=" * 80)
    print("ğŸ“„ PDF ì²˜ë¦¬ ì‚¬ìš©ë²•:")
    print("   1. /docs í˜ì´ì§€ë¡œ ì´ë™")
    print("   2. 'POST /pdf/upload' ì„¹ì…˜ í´ë¦­")
    print("   3. 'Try it out' ë²„íŠ¼ í´ë¦­")
    print("   4. PDF íŒŒì¼ ì„ íƒ + ì¹´í…Œê³ ë¦¬ëª… ì…ë ¥")
    print("   5. Q&A ê°œìˆ˜ ì„¤ì • (5-50ê°œ)")
    print("   6. 'Execute' ë²„íŠ¼ìœ¼ë¡œ ì‹¤í–‰!")
    print("=" * 80)
    print("ğŸ“ ì‚¬ìš© ì˜ˆì œ (cURL):")
    print("   curl -X POST 'http://localhost:5000/pdf/upload' \\")
    print("        -F 'file=@your_document.pdf' \\")
    print("        -F 'category_name=ë²•ì¸ì¹´ë“œ ì‚¬ìš© ì§€ì¹¨' \\")
    print("        -F 'qa_count=20' \\")
    print("        -F 'company_name=ë”ì¼€ì´êµì§ì›ë‚˜ë¼(ì£¼)'")
    print("=" * 80)
    print("ğŸ¯ ê¸°ëŒ€ íš¨ê³¼:")
    print("   âœ… PDF ë¬¸ì„œë¥¼ ì¦‰ì‹œ RAGìš© JSONìœ¼ë¡œ ë³€í™˜")
    print("   âœ… ìˆ˜ë™ Q&A ì‘ì„± ì‹œê°„ ëŒ€í­ ë‹¨ì¶•")
    print("   âœ… ì¼ê´€ì„± ìˆëŠ” ê³ í’ˆì§ˆ Q&A ìë™ ìƒì„±")
    print("   âœ… ë‹¤ì–‘í•œ ì§ˆë¬¸ ìœ í˜• ìë™ ì»¤ë²„")
    print("   âœ… ê¸°ì¡´ RAG ì‹œìŠ¤í…œê³¼ ì¦‰ì‹œ ì—°ë™ ê°€ëŠ¥")
    print("=" * 80)
    print("âš ï¸ ì‚¬ì „ ì¤€ë¹„ì‚¬í•­:")
    print("   1. LM Studio ì‹¤í–‰ (http://localhost:1234)")
    print("   2. qwen3-30b-a3b-mlx ëª¨ë¸ ë¡œë“œ")
    print("   3. ./models ë””ë ‰í† ë¦¬ì— sentence-transformers ëª¨ë¸ ì¤€ë¹„")
    print("      (ì˜ˆ: nlpai-lab/KURE-v1, sentence-transformers/all-MiniLM-L6-v2)")
    print("   4. PDF íŒŒì¼ ì¤€ë¹„ (ìµœëŒ€ 10MB)")
    print("   5. pip install PyPDF2 pdfplumber sentence-transformers")
    print("=" * 80)
    print("ğŸ“ ë¡œì»¬ ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°:")
    print("   ./models/")
    print("   â”œâ”€â”€ nlpai-lab-KURE-v1/          # í•œêµ­ì–´ íŠ¹í™” (ê¶Œì¥)")
    print("   â”‚   â”œâ”€â”€ config.json")
    print("   â”‚   â”œâ”€â”€ pytorch_model.bin")
    print("   â”‚   â””â”€â”€ tokenizer.json")
    print("   â””â”€â”€ all-MiniLM-L6-v2/           # ë‹¤êµ­ì–´ ì§€ì›")
    print("       â”œâ”€â”€ config.json")
    print("       â””â”€â”€ pytorch_model.bin")
    print("=" * 80)
    
    # í–¥ìƒëœ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    initialize_system() 
    
    logger.info("ğŸš€ PDF to JSON RAG Converter + ëŒ€í­ í™•ì¥ëœ FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")
    
    # ì§ì ‘ FastAPI ì•± ì‹¤í–‰
    uvicorn.run(app, host='0.0.0.0', port=5000, log_level="info")