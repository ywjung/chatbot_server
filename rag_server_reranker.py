import json
import os
import glob
import chromadb
from sentence_transformers import SentenceTransformer
from fastapi import FastAPI, HTTPException, Request, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from fastapi.openapi.docs import get_swagger_ui_html, get_redoc_html
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union, Tuple
from enum import Enum
import requests
import logging
import uuid
import time
import re
import uvicorn
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import math
from datetime import datetime  # ì¶”ê°€
import pytz  # ì¶”ê°€ (í•œêµ­ ì‹œê°„ëŒ€ìš©, pip install pytz í•„ìš”)

# ë¡œê¹… ì„¤ì • (import ì§í›„ì— ì„¤ì •)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Transformers ê´€ë ¨ ì„í¬íŠ¸ (ì„ íƒì  - LM Studio ì‚¬ìš©ìœ¼ë¡œ í•„ìˆ˜ ì•„ë‹˜)
try:
    import torch
    from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
    logger.info("â„¹ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥ (ì„ íƒì‚¬í•­)")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.info("â„¹ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ (LM Studio ì‚¬ìš©ìœ¼ë¡œ í•„ìˆ˜ ì•„ë‹˜)")

# RerankerType Enum ì •ì˜
class RerankerType(str, Enum):
    qwen3 = "qwen3"
    llm_api = "llm_api"
    sentence_transformer = "sentence_transformer"

# Pydantic ëª¨ë¸ ì •ì˜
class SearchRequest(BaseModel):
    query: str = Field(..., description="ê²€ìƒ‰í•  ì§ˆì˜", example="íœ´ê°€ ì‹ ì²­ ë°©ë²•")
    top_k: int = Field(20, description="ë°˜í™˜í•  ê²°ê³¼ ìˆ˜", example=20, ge=1, le=50)
    main_category_filter: Optional[str] = Field(None, description="ëŒ€ë¶„ë¥˜ í•„í„°", example="ì¸ì‚¬")

class ConversationMessage(BaseModel):
    role: str = Field(..., description="ë©”ì‹œì§€ ì—­í•  (user ë˜ëŠ” assistant)", example="user")
    content: str = Field(..., description="ë©”ì‹œì§€ ë‚´ìš©", example="íœ´ê°€ ì‹ ì²­ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?")
    context: Optional[List[Dict[str, Any]]] = Field(None, description="ì‘ë‹µ ì»¨í…ìŠ¤íŠ¸ (assistant ë©”ì‹œì§€ì¸ ê²½ìš°)")

class ChatRequest(BaseModel):
    query: str = Field(..., description="ì§ˆë¬¸ ë‚´ìš©", example="21ë…„ì°¨ íœ´ê°€ëŠ” ë©°ì¹ ì¸ê°€ìš”?", min_length=1)
    main_category_filter: Optional[str] = Field(None, description="ëŒ€ë¶„ë¥˜ í•„í„°", example="ì¸ì‚¬")
    use_reranker: bool = Field(True, description="ê³ ê¸‰ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)", example=True)
    reranker_type: RerankerType = Field(RerankerType.qwen3, description="ê²€ìƒ‰ ë°©ë²• ì„ íƒ")
    conversation_history: List[ConversationMessage] = Field(
        default_factory=list, 
        description="ì´ì „ ëŒ€í™” ê¸°ë¡"
    )

class StreamChatRequest(BaseModel):
    query: str = Field(..., description="ì§ˆë¬¸ ë‚´ìš©", example="21ë…„ì°¨ íœ´ê°€ëŠ” ë©°ì¹ ì¸ê°€ìš”?", min_length=1)
    main_category_filter: Optional[str] = Field(None, description="ëŒ€ë¶„ë¥˜ í•„í„°", example="ì¸ì‚¬")
    use_reranker: bool = Field(True, description="ê³ ê¸‰ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)", example=True)
    reranker_type: RerankerType = Field(RerankerType.qwen3, description="ê²€ìƒ‰ ë°©ë²• ì„ íƒ")
    conversation_history: Optional[List[Dict[str, Any]]] = Field(
        default_factory=list,
        description="ì´ì „ ëŒ€í™” ê¸°ë¡ (ìœ ì—°í•œ í˜•ì‹)"
    )

class SimpleTestRequest(BaseModel):
    query: str = Field(..., description="ì§ˆë¬¸", example="íœ´ê°€ ì‹ ì²­ ë°©ë²•", min_length=1)
    category: Optional[str] = Field(None, description="ì¹´í…Œê³ ë¦¬ í•„í„°", example="ì¸ì‚¬")
    use_reranker: bool = Field(True, description="ê³ ê¸‰ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)", example=True)
    reranker_type: RerankerType = Field(RerankerType.qwen3, description="ê²€ìƒ‰ ë°©ë²• ì„ íƒ")

class SearchResult(BaseModel):
    main_category: str
    sub_category: str
    question: str
    answer: str
    source_file: str
    chunk_type: str
    score: float
    rank: int
    # rerank_details í•„ë“œ ì œê±° - ì‚¬ìš©ìì—ê²Œ ë…¸ì¶œí•˜ì§€ ì•ŠìŒ

class HealthResponse(BaseModel):
    status: str
    rag_ready: bool
    regulations_count: int
    main_categories_count: int
    chunk_statistics: Optional[Dict[str, int]] = None
    improvements: str
    enhanced_features: Optional[List[str]] = None

class SearchResponse(BaseModel):
    query: str
    results: List[SearchResult]
    count: int
    main_category_filter: Optional[str]
    search_type: str
    min_relevance: float
    enhanced_features: List[str]

class ChatResponse(BaseModel):
    query: str
    response: str
    context: List[SearchResult]
    context_count: int
    context_quality: Dict[str, int]
    search_type: str
    response_type: str
    enhanced_features: List[str]

class AdvancedReranker:
    """ê³ ê¸‰ ì¬ë­í‚¹ ì‹œìŠ¤í…œ - LLM API, LM Studio Qwen3-Reranker, Sentence-Transformer ì§€ì›"""
    
    def __init__(self, sentence_model: SentenceTransformer, llm_api_url: str = "http://localhost:1234/v1/chat/completions"):
        self.sentence_model = sentence_model
        self.llm_api_url = llm_api_url
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.document_texts = []
        self.qwen3_working_model = None  # ì‘ë™í•˜ëŠ” ëª¨ë¸ëª… ìºì‹œ
        
        logger.info("ğŸ¯ ê³ ê¸‰ ì¬ë­í‚¹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (LM Studio Qwen3-Reranker ì§€ì›)")
        
        # ì´ˆê¸°í™” ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
        self._check_lm_studio_availability()
    
    def _check_lm_studio_availability(self) -> bool:
        """LM Studio ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ë° ëª¨ë¸ í™•ì¸"""
        try:
            # 1. LM Studio ì„œë²„ ì—°ê²° í™•ì¸
            response = requests.get("http://localhost:1234/v1/models", timeout=5)
            if response.status_code != 200:
                logger.warning("âš ï¸ LM Studio ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (http://localhost:1234)")
                return False
            
            # 2. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸
            models_data = response.json()
            available_models = [model['id'] for model in models_data.get('data', [])]
            logger.info(f"ğŸ“‹ LM Studio ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤: {available_models}")
            
            # 3. ì¬ë­í‚¹ ëª¨ë¸ ì°¾ê¸° ë° í…ŒìŠ¤íŠ¸
            potential_reranker_models = [
                "qwen.qwen3-reranker-0.6b"
            ]
            
            # 4. ì‹¤ì œ ëª¨ë¸ëª…ì—ì„œ ì¬ë­í‚¹ ëª¨ë¸ ì°¾ê¸°
            for model in available_models:
                if 'rerank' in model.lower():
                    potential_reranker_models.insert(0, model)  # ì•ì— ì¶”ê°€
            
            # 5. ê° ëª¨ë¸ í…ŒìŠ¤íŠ¸
            for model_name in potential_reranker_models:
                if model_name in available_models or model_name.startswith('qwen'):
                    if self._test_qwen3_model(model_name):
                        self.qwen3_working_model = model_name
                        logger.info(f"âœ… ì‘ë™í•˜ëŠ” Qwen3-Reranker ëª¨ë¸ ë°œê²¬: {model_name}")
                        return True
            
            logger.warning("âš ï¸ ì‘ë™í•˜ëŠ” Qwen3-Reranker ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
            
        except Exception as e:
            logger.warning(f"âš ï¸ LM Studio ì—°ê²° í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def _test_qwen3_model(self, model_name: str) -> bool:
        """íŠ¹ì • Qwen3 ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        try:
            logger.info(f"ğŸ§ª {model_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            response = requests.post(
                "http://localhost:1234/v1/chat/completions",
                json={
                    "model": model_name,
                    "messages": [
                        {"role": "user", "content": "Answer with 'yes' if you can understand this message."}
                    ],
                    "temperature": 0.0,
                    "max_tokens": 20,
                    "stream": False
                },
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()
                logger.info(f"âœ… {model_name} í…ŒìŠ¤íŠ¸ ì‘ë‹µ: '{content}'")
                
                # ì‘ë‹µì´ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ì„±ê³µ
                if content and len(content) > 0:
                    return True
                else:
                    logger.warning(f"âŒ {model_name}: ë¹ˆ ì‘ë‹µ")
                    return False
            else:
                logger.warning(f"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {response.status_code}")
                return False
                
        except Exception as e:
            logger.warning(f"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return False
    
    def calculate_qwen3_similarity(self, query: str, document: str) -> float:
        """LM Studioì˜ Qwen3-Rerankerë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (ê°œì„ ëœ ë²„ì „)"""
        try:
            # ì‘ë™í•˜ëŠ” ëª¨ë¸ì´ ì—†ìœ¼ë©´ fallback
            if not self.qwen3_working_model:
                logger.warning("âš ï¸ ì‘ë™í•˜ëŠ” Qwen3 ëª¨ë¸ì´ ì—†ìŒ, fallback ì‚¬ìš©")
                return self.calculate_semantic_similarity(query, document)
            
            # ë” ê°„ë‹¨í•˜ê³  ëª…í™•í•œ í”„ë¡¬í”„íŠ¸
            system_prompt = "You are a relevance judge. Respond only with 'yes' if the document is relevant to the query, or 'no' if not relevant. Do not explain."
            
            user_prompt = f"Query: {query[:200]}\nDocument: {document[:300]}\nRelevant?"
            
            response = requests.post(
                "http://localhost:1234/v1/chat/completions",
                json={
                    "model": self.qwen3_working_model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 30,  # ì¶©ë¶„í•œ í† í°
                    "stream": False,
                    "stop": ["\n", "Query:", "Document:"]  # ì ì ˆí•œ stop í† í°
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()
                
                # ë””ë²„ê¹… ë¡œê·¸ (ê³¼ë„í•œ ë¡œê·¸ ë°©ì§€)
                if len(content) == 0:
                    logger.debug(f"ğŸ” ë¹ˆ ì‘ë‹µ - ëª¨ë¸: {self.qwen3_working_model}")
                
                if content:
                    # íƒœê·¸ ì œê±° ë° ì •ë¦¬
                    import re
                    cleaned_content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    cleaned_content = re.sub(r'<[^>]*>', '', cleaned_content)
                    cleaned_content = cleaned_content.strip().lower()
                    
                    # yes/no íŒë‹¨ (ë” ìœ ì—°í•œ ë§¤ì¹­)
                    if any(word in cleaned_content for word in ['yes', 'relevant', 'related', 'match', 'similar']):
                        return 0.85
                    elif any(word in cleaned_content for word in ['no', 'not', 'irrelevant', 'unrelated', 'different']):
                        return 0.15
                    else:
                        # ì‘ë‹µì´ ìˆì§€ë§Œ yes/noê°€ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš°
                        logger.debug(f"ğŸŸ¡ ì• ë§¤í•œ ì‘ë‹µ: '{cleaned_content[:50]}'")
                        return 0.5
                else:
                    # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬ - ê³¼ë„í•œ ë¡œê·¸ ë°©ì§€
                    logger.debug(f"âš ï¸ ë¹ˆ ì‘ë‹µ from {self.qwen3_working_model}")
                    return 0.5
            else:
                logger.warning(f"âš ï¸ Qwen3-Reranker API ì˜¤ë¥˜: {response.status_code}")
                return self.calculate_semantic_similarity(query, document)
                
        except requests.exceptions.Timeout:
            logger.warning("âš ï¸ Qwen3-Reranker íƒ€ì„ì•„ì›ƒ")
            return self.calculate_semantic_similarity(query, document)
        except Exception as e:
            logger.warning(f"âš ï¸ Qwen3-Reranker ì˜¤ë¥˜: {e}")
            return self.calculate_semantic_similarity(query, document)
    
    def calculate_llm_api_similarity(self, query: str, document: str) -> float:
        """LLM APIë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            # ë§¤ìš° ëª…í™•í•˜ê³  ê°•ë ¥í•œ í”„ë¡¬í”„íŠ¸
            system_prompt = """You are a precise scoring system. Your ONLY job is to output a single decimal number between 0.0 and 1.0.

CRITICAL RULES:
- DO NOT use <think> tags
- DO NOT explain your reasoning  
- DO NOT write any text
- OUTPUT ONLY a number like: 0.7

Rate document relevance to query:
0.0-0.2: Irrelevant
0.3-0.4: Somewhat related
0.5-0.6: Partially relevant  
0.7-0.8: Highly relevant
0.9-1.0: Perfect match

Examples of CORRECT responses: 0.8, 0.3, 0.9"""

            user_prompt = f"""Query: {query}
Document: {document}
Score:"""

            response = requests.post(
                self.llm_api_url,
                json={
                    "model": "qwen3-30b-a3b-mlx",
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.0,  # ì™„ì „íˆ ê²°ì •ì ìœ¼ë¡œ
                    "max_tokens": 5,     # ë§¤ìš° ì§§ê²Œ
                    "stream": False,
                    "stop": ["\n", "<", " ", ".", ",", "think"]  # <think> ë°©ì§€
                },
                timeout=20
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()
                
                # 1. <think> íƒœê·¸ ë° ê¸°íƒ€ íƒœê·¸ ì œê±°
                import re
                cleaned_content = re.sub(r'<[^>]*>', '', content)  # ëª¨ë“  HTML/XML íƒœê·¸ ì œê±°
                cleaned_content = cleaned_content.strip()
                
                # 2. ìˆ«ì íŒ¨í„´ ì¶”ì¶œ (ë” ê°•ë ¥í•˜ê²Œ)
                number_patterns = [
                    r'\b([01]\.?\d*)\b',      # 0.xxx ë˜ëŠ” 1.xxx
                    r'\b(0\.\d+)\b',          # 0.xxx
                    r'\b(1\.0*)\b',           # 1 ë˜ëŠ” 1.0
                    r'\b(0)\b'                # 0
                ]
                
                for pattern in number_patterns:
                    matches = re.findall(pattern, cleaned_content)
                    if matches:
                        try:
                            score = float(matches[0])
                            if 0.0 <= score <= 1.0:
                                return score
                            elif 1.0 < score <= 10.0:  # 1-10 ìŠ¤ì¼€ì¼ ë³€í™˜
                                return score / 10.0
                        except ValueError:
                            continue
                
                # 3. ì›ë³¸ contentì—ì„œ ì§ì ‘ ìˆ«ì ì°¾ê¸°
                all_numbers = re.findall(r'\d*\.?\d+', content)
                for num_str in all_numbers:
                    try:
                        score = float(num_str)
                        if 0.0 <= score <= 1.0:
                            return score
                        elif 1.0 < score <= 10.0:
                            return score / 10.0
                    except ValueError:
                        continue
                
                # 4. ê¸¸ì´ ê¸°ë°˜ ì¶”ì • (ì‘ê¸‰ ì²˜ì¹˜)
                if len(cleaned_content) <= 3:  # ë§¤ìš° ì§§ìœ¼ë©´ ìˆ«ìì¼ ê°€ëŠ¥ì„±
                    try:
                        score = float(cleaned_content)
                        if 0.0 <= score <= 1.0:
                            return score
                    except ValueError:
                        pass
                
                # 5. í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ (ìµœí›„ ìˆ˜ë‹¨)
                content_lower = content.lower()
                if any(word in content_lower for word in ['perfect', 'excellent', '1.0', '0.9']):
                    return 0.9
                elif any(word in content_lower for word in ['good', 'relevant', '0.8', '0.7']):
                    return 0.7
                elif any(word in content_lower for word in ['partial', 'some', '0.5', '0.6']):
                    return 0.5
                elif any(word in content_lower for word in ['little', 'weak', '0.3', '0.4']):
                    return 0.3
                elif any(word in content_lower for word in ['irrelevant', 'unrelated', '0.0', '0.1']):
                    return 0.1
                
                # ë¡œê·¸ì— ë” ë§ì€ ì •ë³´ ì¶œë ¥
                logger.warning(f"âš ï¸ LLM API ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨ - ì›ë³¸: '{content[:50]}', ì •ë¦¬: '{cleaned_content[:30]}'")
                return 0.5  # ê¸°ë³¸ê°’
            else:
                logger.warning(f"âš ï¸ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                return 0.5
                
        except requests.exceptions.Timeout:
            logger.warning("âš ï¸ LLM API íƒ€ì„ì•„ì›ƒ")
            return 0.5
        except Exception as e:
            logger.warning(f"âš ï¸ LLM API ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def calculate_semantic_similarity(self, query: str, document: str) -> float:
        """Sentence-Transformerë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (ê¸°ë³¸ ë°©ë²•)"""
        try:
            query_embedding = self.sentence_model.encode([query])
            doc_embedding = self.sentence_model.encode([document])
            
            similarity = cosine_similarity(query_embedding, doc_embedding)[0][0]
            return max(0.0, float(similarity))
        except Exception as e:
            logger.warning(f"âš ï¸ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def get_semantic_similarity(self, query: str, document: str, reranker_type: str = "sentence_transformer") -> float:
        """ì¬ë­í‚¹ ë°©ë²•ì— ë”°ë¥¸ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°"""
        if reranker_type == "llm_api":
            return self.calculate_llm_api_similarity(query, document)
        elif reranker_type == "qwen3":
            return self.calculate_qwen3_similarity(query, document)
        else:  # sentence_transformer (ê¸°ë³¸)
            return self.calculate_semantic_similarity(query, document)
    
    def prepare_tfidf_index(self, documents: List[str]):
        """TF-IDF ì¸ë±ìŠ¤ ì¤€ë¹„ (í‚¤ì›Œë“œ ë§¤ì¹­ìš©)"""
        try:
            self.document_texts = documents
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=5000,
                stop_words=None,  # í•œêµ­ì–´ëŠ” ë³„ë„ ì²˜ë¦¬
                ngram_range=(1, 2),
                min_df=1,
                max_df=0.95
            )
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)
            logger.info(f"âœ… TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ, {self.tfidf_matrix.shape[1]}ê°œ íŠ¹ì„±")
        except Exception as e:
            logger.error(f"âŒ TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self.tfidf_vectorizer = None
            self.tfidf_matrix = None
    
    def calculate_keyword_relevance(self, query: str, document: str) -> float:
        """í‚¤ì›Œë“œ ê´€ë ¨ì„± ê³„ì‚° (TF-IDF ê¸°ë°˜)"""
        try:
            if self.tfidf_vectorizer is None:
                return 0.0
            
            query_vector = self.tfidf_vectorizer.transform([query])
            doc_vector = self.tfidf_vectorizer.transform([document])
            
            similarity = cosine_similarity(query_vector, doc_vector)[0][0]
            return max(0.0, float(similarity))
        except Exception as e:
            logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ê´€ë ¨ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def calculate_structural_relevance(self, query: str, candidate: Dict[str, Any]) -> float:
        """êµ¬ì¡°ì  ê´€ë ¨ì„± ê³„ì‚° (ì§ˆë¬¸-ë‹µë³€ êµ¬ì¡° ê³ ë ¤)"""
        try:
            question = candidate.get('question', '').lower()
            answer = candidate.get('answer', '').lower()
            query_lower = query.lower()
            
            # ì§ˆë¬¸ ë§¤ì¹­ ì ìˆ˜
            question_match = self._calculate_text_overlap(query_lower, question)
            
            # ë‹µë³€ ë§¤ì¹­ ì ìˆ˜
            answer_match = self._calculate_text_overlap(query_lower, answer)
            
            # ì²­í¬ íƒ€ì…ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
            chunk_type = candidate.get('chunk_type', 'qa_full')
            chunk_weights = {
                'qa_full': 1.0,
                'question_focused': 0.95,
                'answer_focused': 0.85,
                'sentence_level': 0.75
            }
            chunk_weight = chunk_weights.get(chunk_type, 0.5)
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            structural_score = (question_match * 0.6 + answer_match * 0.4) * chunk_weight
            return min(1.0, structural_score)
            
        except Exception as e:
            logger.warning(f"âš ï¸ êµ¬ì¡°ì  ê´€ë ¨ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_text_overlap(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ê°„ ì¤‘ë³µë„ ê³„ì‚°"""
        try:
            words1 = set(re.findall(r'\b[ê°€-í£]{2,}\b', text1))
            words2 = set(re.findall(r'\b[ê°€-í£]{2,}\b', text2))
            
            if not words1 or not words2:
                return 0.0
            
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            jaccard_similarity = intersection / union if union > 0 else 0.0
            coverage = intersection / len(words1) if len(words1) > 0 else 0.0
            
            return (jaccard_similarity + coverage) / 2
        except Exception as e:
            return 0.0
    
    def calculate_context_relevance(self, query: str, candidate: Dict[str, Any], 
                                  conversation_history: List[Dict[str, str]] = None) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ê³„ì‚° (ì´ì „ ëŒ€í™” ê³ ë ¤)"""
        try:
            if not conversation_history:
                return 0.0
            
            # ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
            context_keywords = set()
            for msg in conversation_history[-3:]:  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ë§Œ
                content = msg.get('content', '').lower()
                keywords = re.findall(r'\b[ê°€-í£]{2,}\b', content)
                context_keywords.update(keywords)
            
            if not context_keywords:
                return 0.0
            
            # í›„ë³´ ë¬¸ì„œì—ì„œ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ë§¤ì¹­
            question = candidate.get('question', '').lower()
            answer = candidate.get('answer', '').lower()
            doc_text = f"{question} {answer}"
            doc_keywords = set(re.findall(r'\b[ê°€-í£]{2,}\b', doc_text))
            
            # ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­ ì ìˆ˜
            matches = len(context_keywords.intersection(doc_keywords))
            context_score = matches / len(context_keywords) if len(context_keywords) > 0 else 0.0
            
            return min(1.0, context_score * 2)  # ë¶€ìŠ¤íŠ¸ ì ìš©
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def calculate_answer_quality_score(self, candidate: Dict[str, Any]) -> float:
        """ë‹µë³€ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            answer = candidate.get('answer', '')
            
            # ë‹µë³€ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸¸ë©´ ê°ì )
            length = len(answer)
            if length < 10:
                length_score = 0.3
            elif length < 50:
                length_score = 0.6
            elif length < 200:
                length_score = 1.0
            elif length < 500:
                length_score = 0.9
            else:
                length_score = 0.7
            
            # êµ¬ì¡°ì  ìš”ì†Œ ì ìˆ˜ (ìˆ«ì, ë‹¨ê³„, ì˜ˆì‹œ ë“±)
            structure_score = 0.5
            if re.search(r'\d+\.\s', answer):  # ë²ˆí˜¸ ëª©ë¡
                structure_score += 0.2
            if 'ë‹¨ê³„' in answer or 'ì ˆì°¨' in answer:  # ì ˆì°¨ ì„¤ëª…
                structure_score += 0.2
            if 'ì˜ˆ:' in answer or 'ì˜ˆì‹œ' in answer:  # ì˜ˆì‹œ í¬í•¨
                structure_score += 0.1
            
            structure_score = min(1.0, structure_score)
            
            # ì¢…í•© ì ìˆ˜
            quality_score = (length_score * 0.7) + (structure_score * 0.3)
            return quality_score
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë‹µë³€ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def rerank_candidates(self, query: str, candidates: List[Dict[str, Any]], 
                         conversation_history: List[Dict[str, str]] = None,
                         top_k: int = 20, reranker_type: str = "sentence_transformer") -> List[Dict[str, Any]]:
        """ê³ ê¸‰ ì¬ë­í‚¹ ìˆ˜í–‰ - ë‹¤ì¤‘ ë°©ë²• ì§€ì›"""
        logger.info(f"ğŸ¯ ê³ ê¸‰ ì¬ë­í‚¹ ì‹œì‘ ({reranker_type}): {len(candidates)}ê°œ í›„ë³´ â†’ {top_k}ê°œ ì„ ë³„")
        
        # Qwen3 ìƒíƒœ í™•ì¸
        if reranker_type == "qwen3" and not self.qwen3_working_model:
            logger.warning("âš ï¸ Qwen3 ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€, sentence_transformerë¡œ ë³€ê²½")
            reranker_type = "sentence_transformer"
        
        try:
            if not candidates:
                return []
            
            # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¤€ë¹„ (TF-IDFìš©)
            documents = [f"{c.get('question', '')} {c.get('answer', '')}" for c in candidates]
            self.prepare_tfidf_index(documents)
            
            reranked_candidates = []
            
            for i, candidate in enumerate(candidates):
                # 1. ì˜ë¯¸ì  ìœ ì‚¬ë„ (ì„ íƒëœ ë°©ë²• ì‚¬ìš©)
                doc_text = f"{candidate.get('question', '')} {candidate.get('answer', '')}"
                semantic_score = self.get_semantic_similarity(query, doc_text, reranker_type)
                
                # 2. í‚¤ì›Œë“œ ê´€ë ¨ì„± (TF-IDF ê¸°ë°˜)
                keyword_score = self.calculate_keyword_relevance(query, doc_text)
                
                # 3. êµ¬ì¡°ì  ê´€ë ¨ì„± (ì§ˆë¬¸-ë‹µë³€ êµ¬ì¡°)
                structural_score = self.calculate_structural_relevance(query, candidate)
                
                # 4. ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± (ì´ì „ ëŒ€í™” ê³ ë ¤)
                context_score = self.calculate_context_relevance(query, candidate, conversation_history)
                
                # 5. ë‹µë³€ í’ˆì§ˆ ì ìˆ˜
                quality_score = self.calculate_answer_quality_score(candidate)
                
                # 6. ì›ë³¸ ë²¡í„° ê²€ìƒ‰ ì ìˆ˜
                original_score = candidate.get('score', 0.0)
                
                # ë‹¤ì¤‘ ì ìˆ˜ ìœµí•© (ê°€ì¤‘ í‰ê· )
                final_score = (
                    semantic_score * 0.30 +      # ì˜ë¯¸ì  ìœ ì‚¬ë„ (ê°€ì¥ ì¤‘ìš”)
                    structural_score * 0.25 +    # êµ¬ì¡°ì  ê´€ë ¨ì„±
                    keyword_score * 0.20 +       # í‚¤ì›Œë“œ ë§¤ì¹­
                    original_score * 0.15 +      # ì›ë³¸ ë²¡í„° ì ìˆ˜
                    quality_score * 0.05 +       # ë‹µë³€ í’ˆì§ˆ
                    context_score * 0.05         # ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±
                )
                
                # ì¬ë­í‚¹ ìƒì„¸ ì •ë³´ ì €ì¥ (ë‚´ë¶€ìš© - ì‚¬ìš©ìì—ê²ŒëŠ” ë…¸ì¶œí•˜ì§€ ì•ŠìŒ)
                rerank_details = {
                    'semantic_score': round(semantic_score, 3),
                    'keyword_score': round(keyword_score, 3),
                    'structural_score': round(structural_score, 3),
                    'context_score': round(context_score, 3),
                    'quality_score': round(quality_score, 3),
                    'original_score': round(original_score, 3),
                    'final_score': round(final_score, 3),
                    'reranker_type': reranker_type
                }
                
                # í›„ë³´ ì—…ë°ì´íŠ¸
                enhanced_candidate = candidate.copy()
                enhanced_candidate['score'] = final_score
                enhanced_candidate['rerank_details'] = rerank_details  # ë‚´ë¶€ì ìœ¼ë¡œë§Œ ì‚¬ìš©
                
                reranked_candidates.append(enhanced_candidate)
            
            # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
            reranked_candidates.sort(key=lambda x: x['score'], reverse=True)
            
            # ë‹¤ì–‘ì„± í™•ë³´ (ê°™ì€ ì§ˆë¬¸ì˜ ì¤‘ë³µ ì œê±°)
            diverse_results = []
            seen_questions = set()
            
            for candidate in reranked_candidates:
                question = candidate.get('question', '')
                if question not in seen_questions or len(diverse_results) < top_k // 2:
                    diverse_results.append(candidate)
                    seen_questions.add(question)
                
                if len(diverse_results) >= top_k:
                    break
            
            # ìˆœìœ„ ì¬í• ë‹¹
            for i, result in enumerate(diverse_results):
                result['rank'] = i + 1
            
            logger.info(f"âœ… ê³ ê¸‰ ì¬ë­í‚¹ ì™„ë£Œ ({reranker_type}): {len(diverse_results)}ê°œ ìµœì¢… ì„ ë³„")
            
            # ì¬ë­í‚¹ ì„±ëŠ¥ ë¡œê·¸
            if diverse_results:
                avg_semantic = np.mean([r['rerank_details']['semantic_score'] for r in diverse_results])
                avg_structural = np.mean([r['rerank_details']['structural_score'] for r in diverse_results])
                avg_final = np.mean([r['score'] for r in diverse_results])
                logger.info(f"ğŸ“Š ì¬ë­í‚¹ í’ˆì§ˆ ({reranker_type}): ì˜ë¯¸ì ={avg_semantic:.3f}, êµ¬ì¡°ì ={avg_structural:.3f}, ìµœì¢…={avg_final:.3f}")
            
            return diverse_results
            
        except Exception as e:
            logger.error(f"âŒ ì¬ë­í‚¹ ì‹¤íŒ¨: {e}", exc_info=True)
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²°ê³¼ ë°˜í™˜
            return candidates[:top_k]
    
    def get_status(self) -> Dict[str, Any]:
        """ì¬ë­í‚¹ ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        return {
            "qwen3_available": self.qwen3_working_model is not None,
            "qwen3_model": self.qwen3_working_model,
            "sentence_transformer_available": self.sentence_model is not None,
            "tfidf_ready": self.tfidf_vectorizer is not None
        }

class CompanyRegulationsRAGSystem:
    def __init__(self, model_name: str = "nlpai-lab/KURE-v1", persist_directory: str = "./chroma_db"):
        """ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ ChromaDB RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        model_path = os.path.join("./models", model_name.replace("/", "-"))

        # ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(model_path):
            logger.info(f"ğŸ“¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: '{model_name}'")
            try:
                model = SentenceTransformer(model_name)
                model.save(model_path)
                logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
            except Exception as e:
                logger.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
                raise
        else:
            logger.info(f"ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {model_path}")

        self.model = SentenceTransformer(model_path)
        self.persist_directory = persist_directory
        
        # ê³ ê¸‰ ì¬ë­í‚¹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.reranker = AdvancedReranker(self.model)

        # ChromaDB ì´ˆê¸°í™”
        self.chroma_client = chromadb.PersistentClient(path=self.persist_directory)
        try:
            self.collection = self.chroma_client.get_collection(name="company_regulations")
            logger.info("ğŸ“ ê¸°ì¡´ company_regulations ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ")
        except Exception as get_error:
            logger.info(f"ğŸ“ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì¤‘...")
            try:
                self.collection = self.chroma_client.create_collection(
                    name="company_regulations",
                    metadata={"description": "ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ íšŒì‚¬ ë‚´ê·œ ë²¡í„° ê²€ìƒ‰ ì»¬ë ‰ì…˜"}
                )
                logger.info("ğŸ“ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
            except Exception as create_error:
                import time
                temp_name = f"company_regulations_advanced_{int(time.time())}"
                self.collection = self.chroma_client.create_collection(
                    name=temp_name,
                    metadata={"description": "ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ ì„ì‹œ ì»¬ë ‰ì…˜"}
                )
                logger.info(f"ğŸ“ ì„ì‹œ ì»¬ë ‰ì…˜ '{temp_name}' ìƒì„± ì™„ë£Œ")

        logger.info(f"âœ… ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_company_regulations_data(self, data_directory: str = "./data_json"):
        """íšŒì‚¬ ë‚´ê·œ ë°ì´í„° ë¡œë“œ"""
        logger.info(f"íšŒì‚¬ ë‚´ê·œ ë°ì´í„° ë¡œë“œ ì‹œì‘: {data_directory}")
        try:
            if not os.path.exists(data_directory):
                logger.error(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {data_directory}")
                return False
            
            json_files = glob.glob(os.path.join(data_directory, "*.json"))
            
            if not json_files:
                logger.warning(f"âš ï¸ JSON íŒŒì¼ ì—†ìŒ: {data_directory}")
                return False
            
            logger.info(f"ğŸ” ë°œê²¬ëœ JSON íŒŒì¼: {len(json_files)}ê°œ")
            
            self.regulations_data = []
            self.main_categories = {}
            
            for json_file in json_files:
                file_name = os.path.basename(json_file)
                main_category = os.path.splitext(file_name)[0]
                
                logger.info(f"ğŸ“‚ ë¡œë”©: {file_name} â†’ {main_category}")
                
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    category_count = 0
                    faq_count = 0
                    
                    for category_section in data:
                        sub_category = category_section['category']
                        category_count += 1
                        
                        for faq in category_section['faqs']:
                            question = faq['question']
                            answer = faq['answer']
                            
                            # ê¸°ë³¸ Q&A ë‹¨ìœ„ ì €ì¥
                            base_id = str(uuid.uuid4())
                            regulation_item = {
                                'id': base_id,
                                'main_category': main_category,
                                'sub_category': sub_category,
                                'question': question,
                                'answer': answer,
                                'text': f"{question} {answer}",
                                'source_file': file_name,
                                'chunk_type': 'qa_full',
                                'chunk_id': 0
                            }
                            self.regulations_data.append(regulation_item)
                            
                            # í–¥ìƒëœ ì²­í‚¹
                            question_item = {
                                'id': f"{base_id}_q",
                                'main_category': main_category,
                                'sub_category': sub_category,
                                'question': question,
                                'answer': answer,
                                'text': f"ì§ˆë¬¸: {question}",
                                'source_file': file_name,
                                'chunk_type': 'question_focused',
                                'chunk_id': 1
                            }
                            self.regulations_data.append(question_item)
                            
                            answer_item = {
                                'id': f"{base_id}_a",
                                'main_category': main_category,
                                'sub_category': sub_category,
                                'question': question,
                                'answer': answer,
                                'text': f"ë‹µë³€: {answer} (ê´€ë ¨ ì§ˆë¬¸: {question})",
                                'source_file': file_name,
                                'chunk_type': 'answer_focused',
                                'chunk_id': 2
                            }
                            self.regulations_data.append(answer_item)
                            
                            # ê¸´ ë‹µë³€ì˜ ê²½ìš° ë¬¸ì¥ ë¶„í• 
                            if len(answer) > 200:
                                sentences = re.split(r'[.!?]\s+', answer)
                                for i, sentence in enumerate(sentences):
                                    if len(sentence.strip()) > 20:
                                        sentence_item = {
                                            'id': f"{base_id}_s{i}",
                                            'main_category': main_category,
                                            'sub_category': sub_category,
                                            'question': question,
                                            'answer': answer,
                                            'text': f"{sentence.strip()} (ì¶œì²˜: {question})",
                                            'source_file': file_name,
                                            'chunk_type': 'sentence_level',
                                            'chunk_id': 10 + i
                                        }
                                        self.regulations_data.append(sentence_item)
                            
                            faq_count += 1
                    
                    self.main_categories[main_category] = {
                        'file_name': file_name,
                        'sub_categories': category_count,
                        'total_faqs': faq_count
                    }
                    
                    logger.info(f"  â†’ ì†Œë¶„ë¥˜: {category_count}ê°œ, ê·œì •: {faq_count}ê°œ")
                    
                except Exception as e:
                    logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {json_file}: {e}")
                    continue
            
            total_chunks = len(self.regulations_data)
            total_files = len(self.main_categories)
            
            logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: íŒŒì¼ {total_files}ê°œ, ì²­í¬ {total_chunks}ê°œ")
            return total_chunks > 0
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
            return False

    def build_index(self, force_rebuild=False):
        """ChromaDB ì¸ë±ìŠ¤ êµ¬ì¶• (force_rebuild=Trueì¼ ë•Œë§Œ ì¬êµ¬ì¶•)"""
        logger.info("ChromaDB ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
        try:
            existing_count = self.collection.count()
            
            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆê³  ê°•ì œ ì¬êµ¬ì¶•ì´ ì•„ë‹Œ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if existing_count > 0 and not force_rebuild:
                logger.info(f"âœ… ê¸°ì¡´ ë²¡í„° DB ì‚¬ìš©: {existing_count}ê°œ (ì¬êµ¬ì¶• í•˜ì§€ ì•ŠìŒ)")
                return True
            
            # ê°•ì œ ì¬êµ¬ì¶•ì´ê±°ë‚˜ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì§„í–‰
            if not hasattr(self, 'regulations_data') or not self.regulations_data:
                if existing_count > 0:
                    logger.info(f"âœ… ê¸°ì¡´ ë²¡í„° DB ì‚¬ìš©: {existing_count}ê°œ (data_json ì—†ìŒ)")
                    return True
                else:
                    logger.warning("âš ï¸ ë¡œë“œëœ ë°ì´í„°ì™€ ê¸°ì¡´ ë²¡í„° DB ëª¨ë‘ ì—†ìŒ")
                    return False

            if force_rebuild and existing_count > 0:
                logger.info("ğŸ”„ ê°•ì œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì¤‘...")
                self._clear_collection_safely()
            
            texts = [item['text'] for item in self.regulations_data]
            metadatas = [
                {
                    'main_category': item['main_category'],
                    'sub_category': item['sub_category'],
                    'question': item['question'],
                    'answer': item['answer'],
                    'source_file': item['source_file'],
                    'chunk_type': item.get('chunk_type', 'qa_full'),
                    'chunk_id': item.get('chunk_id', 0)
                } 
                for item in self.regulations_data
            ]
            ids = [item['id'] for item in self.regulations_data]
            
            logger.info(f"âš™ï¸ ì„ë² ë”© ë° ì €ì¥: {len(texts)}ê°œ ì²­í¬")
            
            batch_size = 500
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                batch_metadatas = metadatas[i:i+batch_size]
                batch_ids = ids[i:i+batch_size]
                
                try:
                    embeddings = self.model.encode(batch_texts, show_progress_bar=False).tolist()
                    
                    self.collection.add(
                        documents=batch_texts,
                        metadatas=batch_metadatas,
                        embeddings=embeddings,
                        ids=batch_ids
                    )
                    
                    logger.info(f"ğŸ“¦ ë°°ì¹˜ {i//batch_size + 1}/{total_batches} ì™„ë£Œ")
                    
                except Exception as batch_error:
                    logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {batch_error}")
                    continue
            
            final_count = self.collection.count()
            logger.info(f"âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {final_count}ê°œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}", exc_info=True)
            return False
    
    def _clear_collection_safely(self):
        """ì»¬ë ‰ì…˜ ì•ˆì „ ì‚­ì œ"""
        try:
            # ê°œë³„ ID ì‚­ì œ ì‹œë„
            try:
                all_results = self.collection.get()
                if all_results and 'ids' in all_results and all_results['ids']:
                    batch_size = 1000
                    total_ids = all_results['ids']
                    
                    for i in range(0, len(total_ids), batch_size):
                        batch_ids = total_ids[i:i+batch_size]
                        self.collection.delete(ids=batch_ids)
                return True
            except Exception:
                # ì»¬ë ‰ì…˜ ì¬ìƒì„±
                collection_name = self.collection.name
                self.chroma_client.delete_collection(name=collection_name)
                self.collection = self.chroma_client.create_collection(
                    name=collection_name,
                    metadata={"description": "ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ íšŒì‚¬ ë‚´ê·œ ë²¡í„° ê²€ìƒ‰ ì»¬ë ‰ì…˜"}
                )
                return True
        except Exception as e:
            logger.error(f"âŒ ì»¬ë ‰ì…˜ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _generate_query_variants(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ ë³€í˜• ìƒì„±"""
        variants = [query]  # ì›ë³¸ ì¿¼ë¦¬
        
        # ì˜ë¬¸ì‚¬ ì œê±°
        question_words = ['ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ì™œ', 'ëˆ„ê°€', 'ì–¼ë§ˆë‚˜', 'ëª‡']
        cleaned_query = query
        for word in question_words:
            cleaned_query = cleaned_query.replace(word, '').strip()
        if cleaned_query and cleaned_query != query:
            variants.append(cleaned_query)
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'\b[ê°€-í£]{2,}\b', query)
        if len(keywords) >= 2:
            keyword_query = ' '.join(keywords[:3])
            variants.append(keyword_query)
        
        # ë¬¸ì œ/ì ˆì°¨ ë³€í˜•
        if 'ë¬¸ì œ' in query or 'ì´ìŠˆ' in query:
            variants.append(query.replace('ë¬¸ì œ', 'í•´ê²°').replace('ì´ìŠˆ', 'ëŒ€ì‘'))
        
        if 'ë°©ë²•' in query or 'ì ˆì°¨' in query:
            procedure_query = query.replace('ë°©ë²•', 'ì ˆì°¨ ë‹¨ê³„').replace('ê³¼ì •', 'ì ˆì°¨')
            variants.append(procedure_query)
        
        return list(set(variants))
    
    def search_with_advanced_rerank(self, query: str, top_k: int = 20, 
                                   main_category_filter: str = None,
                                   conversation_history: List[Dict[str, str]] = None,
                                   min_relevance_score: float = 0.2,
                                   reranker_type: str = "sentence_transformer") -> List[Dict[str, Any]]:
        """ê³ ê¸‰ ì¬ë­í‚¹ ê¸°ë°˜ ê²€ìƒ‰ - 2ë‹¨ê³„ ê²€ìƒ‰ ì•„í‚¤í…ì²˜ (ë‹¤ì¤‘ ì¬ë­í‚¹ ë°©ë²• ì§€ì›)"""
        logger.info(f"ğŸ¯ ê³ ê¸‰ ê²€ìƒ‰ ì‹¤í–‰ ({reranker_type}): '{query[:50]}...', top_k={top_k}")
        try:
            if self.collection.count() == 0:
                logger.warning("âš ï¸ ë¹ˆ ì¸ë±ìŠ¤")
                return []
            
            # 1ë‹¨ê³„: ê´‘ë²”ìœ„í•œ í›„ë³´ ìˆ˜ì§‘ (Retrieval)
            query_variants = self._generate_query_variants(query)
            logger.info(f"ğŸ”€ ì¿¼ë¦¬ ë³€í˜•: {len(query_variants)}ê°œ")
            
            all_candidates = {}
            
            # í•„í„° ì¡°ê±´
            where_condition = None
            if main_category_filter:
                where_condition = {"main_category": main_category_filter}
            
            # ë” ë§ì€ í›„ë³´ ìˆ˜ì§‘ (ì¬ë­í‚¹ì„ ìœ„í•´)
            retrieval_limit = max(top_k * 4, 80)  # 4ë°° ë” ë§ì€ í›„ë³´ ìˆ˜ì§‘
            
            for i, variant_query in enumerate(query_variants):
                logger.info(f"  ğŸ” ê²€ìƒ‰ {i+1}: '{variant_query[:30]}...'")
                
                query_embedding = self.model.encode([variant_query]).tolist()
                
                results = self.collection.query(
                    query_embeddings=query_embedding,
                    n_results=retrieval_limit,
                    include=['documents', 'metadatas', 'distances'],
                    where=where_condition
                )
                
                if results['ids'] and len(results['ids'][0]) > 0:
                    for j in range(len(results['ids'][0])):
                        doc_id = results['ids'][0][j]
                        metadata = results['metadatas'][0][j]
                        distance = results['distances'][0][j]
                        
                        similarity_score = max(0, 1 - (distance / 2))
                        query_weight = 1.0 if i == 0 else 0.8
                        weighted_score = similarity_score * query_weight
                        
                        if doc_id not in all_candidates:
                            all_candidates[doc_id] = {
                                'metadata': metadata,
                                'score': weighted_score,
                                'distances': [distance],
                                'query_matches': [i]
                            }
                        else:
                            if weighted_score > all_candidates[doc_id]['score']:
                                all_candidates[doc_id]['score'] = weighted_score
                            all_candidates[doc_id]['distances'].append(distance)
                            all_candidates[doc_id]['query_matches'].append(i)
            
            logger.info(f"  ğŸ“Š ìˆ˜ì§‘ëœ í›„ë³´: {len(all_candidates)}ê°œ")
            
            # í›„ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            candidate_list = []
            for doc_id, candidate_data in all_candidates.items():
                metadata = candidate_data['metadata']
                candidate = {
                    'main_category': metadata['main_category'],
                    'sub_category': metadata['sub_category'],
                    'question': metadata['question'],
                    'answer': metadata['answer'],
                    'source_file': metadata['source_file'],
                    'chunk_type': metadata.get('chunk_type', 'qa_full'),
                    'score': candidate_data['score'],
                    'query_matches': len(set(candidate_data['query_matches']))
                }
                
                # ìµœì†Œ ê´€ë ¨ì„± í•„í„°ë§
                if candidate['score'] >= min_relevance_score:
                    candidate_list.append(candidate)
            
            logger.info(f"  ğŸ“Š í•„í„°ë§ í›„ í›„ë³´: {len(candidate_list)}ê°œ")
            
            # 2ë‹¨ê³„: ê³ ê¸‰ ì¬ë­í‚¹ (Rerank)
            reranked_results = self.reranker.rerank_candidates(
                query=query,
                candidates=candidate_list,
                conversation_history=conversation_history,
                top_k=top_k,
                reranker_type=reranker_type
            )
            
            logger.info(f"âœ… ê³ ê¸‰ ê²€ìƒ‰ ì™„ë£Œ ({reranker_type}): {len(reranked_results)}ê°œ ìµœì¢… ê²°ê³¼")
            
            return reranked_results
            
        except Exception as e:
            logger.error(f"âŒ ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return []
    
    def search_with_enhanced_retrieval(self, query: str, top_k: int = 20, 
                                      main_category_filter: str = None, 
                                      min_relevance_score: float = 0.3) -> List[Dict[str, Any]]:
        """ê¸°ì¡´ í–¥ìƒëœ ê²€ìƒ‰ (ì¬ë­í‚¹ ì—†ì´) - ë‹¤ì¤‘ ì¿¼ë¦¬, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        logger.info(f"í–¥ìƒëœ ê²€ìƒ‰ ì‹¤í–‰: '{query[:50]}...', top_k={top_k}")
        try:
            if self.collection.count() == 0:
                logger.warning("âš ï¸ ë¹ˆ ì¸ë±ìŠ¤")
                return []
            
            # ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±
            query_variants = self._generate_query_variants(query)
            logger.info(f"ğŸ”€ ì¿¼ë¦¬ ë³€í˜•: {len(query_variants)}ê°œ")
            
            all_candidates = {}
            
            # í•„í„° ì¡°ê±´
            where_condition = None
            if main_category_filter:
                where_condition = {"main_category": main_category_filter}
            
            search_limit = max(top_k * 3, 60)
            
            for i, variant_query in enumerate(query_variants):
                query_embedding = self.model.encode([variant_query]).tolist()
                
                results = self.collection.query(
                    query_embeddings=query_embedding,
                    n_results=search_limit,
                    include=['documents', 'metadatas', 'distances'],
                    where=where_condition
                )
                
                if results['ids'] and len(results['ids'][0]) > 0:
                    for j in range(len(results['ids'][0])):
                        doc_id = results['ids'][0][j]
                        metadata = results['metadatas'][0][j]
                        distance = results['distances'][0][j]
                        
                        similarity_score = max(0, 1 - (distance / 2))
                        query_weight = 1.0 if i == 0 else 0.8
                        weighted_score = similarity_score * query_weight
                        
                        if doc_id not in all_candidates:
                            all_candidates[doc_id] = {
                                'metadata': metadata,
                                'best_similarity': weighted_score,
                                'query_matches': [i],
                                'distances': [distance]
                            }
                        else:
                            if weighted_score > all_candidates[doc_id]['best_similarity']:
                                all_candidates[doc_id]['best_similarity'] = weighted_score
                            all_candidates[doc_id]['query_matches'].append(i)
                            all_candidates[doc_id]['distances'].append(distance)
            
            # ê¸°ì¡´ ì¬ë­í‚¹ (í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜)
            enhanced_candidates = []
            
            for doc_id, candidate_data in all_candidates.items():
                metadata = candidate_data['metadata']
                best_similarity = candidate_data['best_similarity']
                query_matches = candidate_data['query_matches']
                
                question = metadata['question'].lower()
                answer = metadata['answer'].lower()
                query_lower = query.lower()
                chunk_type = metadata.get('chunk_type', 'qa_full')
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ë¶„ì„
                query_words = set(re.findall(r'\b[ê°€-í£]{2,}\b', query_lower))
                question_words = set(re.findall(r'\b[ê°€-í£]{2,}\b', question))
                answer_words = set(re.findall(r'\b[ê°€-í£]{2,}\b', answer))
                
                question_match_count = len(query_words.intersection(question_words))
                question_match_ratio = question_match_count / max(len(query_words), 1)
                
                answer_match_count = len(query_words.intersection(answer_words))
                answer_match_ratio = answer_match_count / max(len(query_words), 1)
                
                # ì²­í¬ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
                chunk_weights = {
                    'qa_full': 1.0,
                    'question_focused': 0.9,
                    'answer_focused': 0.8,
                    'sentence_level': 0.7
                }
                chunk_weight = chunk_weights.get(chunk_type, 0.5)
                
                multi_query_bonus = len(set(query_matches)) * 0.1
                keyword_bonus = (question_match_ratio * 0.4) + (answer_match_ratio * 0.3)
                
                final_score = min(1.0, (best_similarity * chunk_weight) + keyword_bonus + multi_query_bonus)
                
                if final_score >= min_relevance_score:
                    enhanced_candidates.append({
                        'main_category': metadata['main_category'],
                        'sub_category': metadata['sub_category'],
                        'question': metadata['question'],
                        'answer': metadata['answer'],
                        'source_file': metadata['source_file'],
                        'chunk_type': chunk_type,
                        'score': final_score,
                        'similarity_score': best_similarity,
                        'keyword_bonus': keyword_bonus,
                        'multi_query_bonus': multi_query_bonus,
                        'chunk_weight': chunk_weight,
                        'query_matches': len(set(query_matches))
                    })
            
            # ì ìˆ˜ ê¸°ë°˜ ì •ë ¬ ë° ë‹¤ì–‘ì„± í™•ë³´
            enhanced_candidates.sort(key=lambda x: x['score'], reverse=True)
            
            # ë‹¤ì–‘ì„±ì„ ìœ„í•œ ì¤‘ë³µ ì œê±°
            seen_questions = set()
            diverse_results = []
            
            for candidate in enhanced_candidates:
                question = candidate['question']
                
                if question not in seen_questions:
                    diverse_results.append(candidate)
                    seen_questions.add(question)
                elif candidate['chunk_type'] == 'qa_full' and len(diverse_results) < top_k:
                    for i, existing in enumerate(diverse_results):
                        if existing['question'] == question and existing['chunk_type'] != 'qa_full':
                            diverse_results[i] = candidate
                            break
                
                if len(diverse_results) >= top_k:
                    break
            
            final_results = diverse_results[:top_k]
            
            # ë­í¬ í• ë‹¹
            for i, result in enumerate(final_results):
                result['rank'] = i + 1
            
            logger.info(f"âœ… í–¥ìƒëœ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼")
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ í–¥ìƒëœ ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return []
    
    def search(self, query: str, top_k: int = 20, main_category_filter: str = None, 
              conversation_history: List[Dict[str, str]] = None, use_reranker: bool = True,
              reranker_type: str = "sentence_transformer") -> List[Dict[str, Any]]:
        """í†µí•© ê²€ìƒ‰ ë©”ì„œë“œ - ê³ ê¸‰ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ ë° ë°©ë²• ì„ íƒ"""
        if use_reranker:
            logger.info(f"ğŸ¯ ê³ ê¸‰ ê²€ìƒ‰ ì‚¬ìš© ({reranker_type})")
            return self.search_with_advanced_rerank(
                query=query,
                top_k=top_k,
                main_category_filter=main_category_filter,
                conversation_history=conversation_history,
                reranker_type=reranker_type
            )
        else:
            logger.info(f"ğŸ“Š í–¥ìƒëœ ê²€ìƒ‰ ì‚¬ìš©")
            return self.search_with_enhanced_retrieval(
                query=query,
                top_k=top_k,
                main_category_filter=main_category_filter
            )
    
    def combine_contexts_with_history(self, current_context: List[Dict[str, Any]], 
                                    conversation_history: List[Dict[str, str]], 
                                    max_total_context: int = 25) -> List[Dict[str, Any]]:
        """ì»¨í…ìŠ¤íŠ¸ ê²°í•©"""
        logger.info(f"ì»¨í…ìŠ¤íŠ¸ ê²°í•©: í˜„ì¬ {len(current_context)}ê°œ, ìµœëŒ€ {max_total_context}ê°œ")
        try:
            # ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            previous_contexts = []
            if conversation_history:
                for msg in conversation_history:
                    if msg.get('role') == 'assistant' and msg.get('context'):
                        previous_contexts.extend(msg['context'])
            
            # ì¤‘ë³µ ì œê±°
            seen_questions = set()
            unique_contexts = []
            
            # í˜„ì¬ ê²€ìƒ‰ ê²°ê³¼ ìš°ì„  ì¶”ê°€
            for ctx in current_context:
                question = ctx.get('question', '')
                if question and question not in seen_questions:
                    seen_questions.add(question)
                    ctx_copy = ctx.copy()
                    ctx_copy['source_type'] = 'current'
                    unique_contexts.append(ctx_copy)
            
            # ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            remaining_slots = max_total_context - len(unique_contexts)
            if remaining_slots > 0:
                previous_sorted = sorted(
                    previous_contexts, 
                    key=lambda x: x.get('score', 0), 
                    reverse=True
                )
                
                for ctx in previous_sorted:
                    if len(unique_contexts) >= max_total_context:
                        break
                    
                    question = ctx.get('question', '')
                    if question and question not in seen_questions and ctx.get('score', 0) >= 0.2:
                        seen_questions.add(question)
                        ctx_copy = ctx.copy()
                        ctx_copy['score'] = ctx_copy.get('score', 0) * 0.85
                        ctx_copy['source_type'] = 'previous'
                        unique_contexts.append(ctx_copy)
            
            # ìµœì¢… ì •ë ¬
            unique_contexts.sort(key=lambda x: (x.get('source_type') == 'current', x.get('score', 0)), reverse=True)
            
            logger.info(f"âœ… ì»¨í…ìŠ¤íŠ¸ ê²°í•© ì™„ë£Œ: {len(unique_contexts)}ê°œ")
            
            return unique_contexts
            
        except Exception as e:
            logger.error(f"âŒ ì»¨í…ìŠ¤íŠ¸ ê²°í•© ì‹¤íŒ¨: {e}")
            return current_context

    def _natural_sort_key(self, text: str):
        """ìì—°ìŠ¤ëŸ¬ìš´ ìˆ«ì ì •ë ¬ì„ ìœ„í•œ í‚¤ í•¨ìˆ˜"""
        import re
        
        def convert(text_part):
            if text_part.isdigit():
                return int(text_part)
            else:
                return text_part.lower()
        
        # ìˆ«ìì™€ ë¬¸ìë¥¼ ë¶„ë¦¬í•´ì„œ ì •ë ¬ í‚¤ ìƒì„±
        return [convert(c) for c in re.split(r'(\d+)', text)]

    def get_categories_from_vector_db(self) -> Dict[str, Any]:
        """ChromaDBì—ì„œ ì§ì ‘ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¡°íšŒ (ë™ì )"""
        try:
            if self.collection.count() == 0:
                return {'main_categories': {}, 'total_main_categories': 0, 'total_regulations': 0}
            
            # ChromaDBì—ì„œ ëª¨ë“  ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            all_results = self.collection.get(include=['metadatas'])
            
            if not all_results or not all_results.get('metadatas'):
                return {'main_categories': {}, 'total_main_categories': 0, 'total_regulations': 0}
            
            # ëŒ€ë¶„ë¥˜ë³„ í†µê³„ ê³„ì‚°
            main_categories = {}
            total_documents = 0
            
            for metadata in all_results['metadatas']:
                # qa_full ì²­í¬ë§Œ ì¹´ìš´íŠ¸ (ì¤‘ë³µ ë°©ì§€)
                if metadata.get('chunk_type') != 'qa_full':
                    continue
                    
                main_cat = metadata.get('main_category', 'Unknown')
                sub_cat = metadata.get('sub_category', 'Unknown')
                source_file = metadata.get('source_file', f'{main_cat}.json')
                
                if main_cat not in main_categories:
                    main_categories[main_cat] = {
                        'file_name': source_file,
                        'sub_categories': set(),
                        'total_faqs': 0
                    }
                
                main_categories[main_cat]['sub_categories'].add(sub_cat)
                main_categories[main_cat]['total_faqs'] += 1
                total_documents += 1
            
            # setì„ ê°œìˆ˜ë¡œ ë³€í™˜
            for main_cat in main_categories:
                main_categories[main_cat]['sub_categories'] = len(main_categories[main_cat]['sub_categories'])
            
            # ìì—°ìŠ¤ëŸ¬ìš´ ìˆ«ì ì •ë ¬
            sorted_items = sorted(main_categories.items(), key=lambda x: self._natural_sort_key(x[0]))
            sorted_main_categories = dict(sorted_items)
            
            return {
                'main_categories': sorted_main_categories,
                'total_main_categories': len(main_categories),
                'total_regulations': total_documents
            }
            
        except Exception as e:
            logger.error(f"âŒ ChromaDB ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
            return self.get_categories_from_files()

    def get_categories_from_files(self) -> Dict[str, Any]:
        """ê¸°ì¡´ íŒŒì¼ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ (fallback)"""
        if not hasattr(self, 'main_categories'):
            return {'main_categories': {}, 'total_main_categories': 0, 'total_regulations': 0}
        
        # ìì—°ìŠ¤ëŸ¬ìš´ ìˆ«ì ì •ë ¬
        sorted_items = sorted(self.main_categories.items(), key=lambda x: self._natural_sort_key(x[0]))
        sorted_main_categories = dict(sorted_items)
        
        return {
            'main_categories': sorted_main_categories,
            'total_main_categories': len(self.main_categories),
            'total_regulations': len(self.regulations_data) if hasattr(self, 'regulations_data') else 0
        }

    def get_categories(self) -> Dict[str, Any]:
        """í†µí•© ì¹´í…Œê³ ë¦¬ ì •ë³´ ë°˜í™˜ (ë™ì  + fallback)"""
        # 1. ChromaDBì—ì„œ ë™ì ìœ¼ë¡œ ì¡°íšŒ ì‹œë„
        dynamic_categories = self.get_categories_from_vector_db()
        
        # 2. íŒŒì¼ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ì™€ ë³‘í•©
        file_categories = self.get_categories_from_files()
        
        # 3. ë” ë§ì€ ë°ì´í„°ë¥¼ ê°€ì§„ ìª½ì„ ìš°ì„  ì‚¬ìš©
        if dynamic_categories['total_main_categories'] >= file_categories['total_main_categories']:
            logger.info(f"âœ… ë™ì  ì¹´í…Œê³ ë¦¬ ì‚¬ìš©: {dynamic_categories['total_main_categories']}ê°œ ëŒ€ë¶„ë¥˜")
            return dynamic_categories
        else:
            logger.info(f"âœ… íŒŒì¼ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©: {file_categories['total_main_categories']}ê°œ ëŒ€ë¶„ë¥˜")
            return file_categories
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            count = self.collection.count()
            main_cats = self.main_categories if hasattr(self, 'main_categories') else {}
            
            chunk_stats = {}
            if hasattr(self, 'regulations_data'):
                for item in self.regulations_data:
                    chunk_type = item.get('chunk_type', 'unknown')
                    chunk_stats[chunk_type] = chunk_stats.get(chunk_type, 0) + 1
            
            return {
                'total_documents': count,
                'collection_name': self.collection.name,
                'persist_directory': self.persist_directory,
                'is_ready': count > 0,
                'main_categories': main_cats,
                'total_main_categories': len(main_cats),
                'chunk_statistics': chunk_stats,
                'enhanced_features': [
                    '2ë‹¨ê³„ ê²€ìƒ‰ ì•„í‚¤í…ì²˜ (ê²€ìƒ‰ + ê³ ê¸‰ ë¶„ì„)',
                    'ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„',
                    'í‚¤ì›Œë“œ ë§¤ì¹­ ìµœì í™”',
                    'êµ¬ì¡°ì  ê´€ë ¨ì„± ë¶„ì„',
                    'ë§¥ë½ ì¸ì‹ ê²€ìƒ‰',
                    'ë‹µë³€ í’ˆì§ˆ í‰ê°€',
                    'ë‹¤ì¤‘ ì ìˆ˜ ìœµí•© ì•Œê³ ë¦¬ì¦˜',
                    f'4ë°° í™•ì¥ëœ í›„ë³´ ìˆ˜ì§‘ ({count//4} â†’ {count})',
                    'ë‹¤ì–‘ì„± ë³´ì¥ ì¤‘ë³µ ì œê±°'
                ]
            }
        except Exception as e:
            logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'total_documents': 0, 'is_ready': False}
    
    def rebuild_index(self):
        """ì¸ë±ìŠ¤ ê°•ì œ ì¬êµ¬ì¶•"""
        logger.info("ê³ ê¸‰ ê²€ìƒ‰ ì¸ë±ìŠ¤ ê°•ì œ ì¬êµ¬ì¶• ì‹œì‘")
        try:
            current_collection_name = getattr(self.collection, 'name', 'company_regulations')
            
            try:
                self.chroma_client.delete_collection(name=current_collection_name)
                logger.info("âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ")
            except Exception:
                pass
            
            try:
                self.collection = self.chroma_client.create_collection(
                    name="company_regulations",
                    metadata={"description": "ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ íšŒì‚¬ ë‚´ê·œ ë²¡í„° ê²€ìƒ‰ ì»¬ë ‰ì…˜"}
                )
                logger.info("âœ… ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±")
            except Exception:
                import time
                backup_name = f"company_regulations_advanced_{int(time.time())}"
                self.collection = self.chroma_client.create_collection(
                    name=backup_name,
                    metadata={"description": "ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ ë°±ì—… ì»¬ë ‰ì…˜"}
                )
                logger.info(f"âœ… ë°±ì—… ì»¬ë ‰ì…˜ ìƒì„±: {backup_name}")
            
            return self.build_index(force_rebuild=True)
            
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return False

def get_user_friendly_features(use_reranker: bool, context_count: int, results_count: int) -> List[str]:
    """ì‚¬ìš©ì ì¹œí™”ì ì¸ ê¸°ëŠ¥ ì„¤ëª… (ì¬ë­í‚¹ ì •ë³´ ìˆ¨ê¹€)"""
    if use_reranker:
        return [
            f"ê³ ê¸‰ ê²€ìƒ‰ìœ¼ë¡œ {results_count}ê°œ ê³ í’ˆì§ˆ ì •ë³´ ì„ ë³„",
            f"ì´ {context_count}ê°œ ì»¨í…ìŠ¤íŠ¸ í™œìš©",
            "ë‹¤ì¤‘ ê´€ì  ë¶„ì„ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ",
            "ë§¥ë½ ì¸ì‹ ê²€ìƒ‰ ì ìš©"
        ]
    else:
        return [
            f"í–¥ìƒëœ ê²€ìƒ‰ìœ¼ë¡œ {results_count}ê°œ ì •ë³´ ìˆ˜ì§‘",
            f"ì´ {context_count}ê°œ ì»¨í…ìŠ¤íŠ¸ í™œìš©", 
            "ë‹¤ì–‘ì„± ë³´ì¥ ì•Œê³ ë¦¬ì¦˜ ì ìš©",
            "í‚¤ì›Œë“œ ë§¤ì¹­ ìµœì í™”"
        ]

class LLMClient:
    def __init__(self, api_url: str = "http://localhost:1234/v1/chat/completions"):
        """LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.api_url = api_url
        logger.info(f"LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: API URL = {self.api_url}")
    
    def _create_enhanced_system_prompt(self, context: List[Dict[str, Any]], 
                                     conversation_history: List[Dict[str, str]] = None) -> str:
        """ì¼ë°˜í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì¬ë­í‚¹ ì •ë³´ ìˆ¨ê¹€)"""
        
        # í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        try:
            korea_tz = pytz.timezone('Asia/Seoul')
            now = datetime.now(korea_tz)
            current_date = now.strftime("%Yë…„ %mì›” %dì¼")
            current_time = now.strftime("%Hì‹œ %Më¶„")
            current_weekday = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"][now.weekday()]
            datetime_info = f"{current_date} {current_weekday} {current_time}"
        except Exception:
            now = datetime.now()
            datetime_info = now.strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
                    
        # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ (ë‚´ë¶€ì ìœ¼ë¡œë§Œ)
        high_quality = [c for c in context if c.get('score', 0) >= 0.8]
        medium_quality = [c for c in context if 0.6 <= c.get('score', 0) < 0.8]
        low_quality = [c for c in context if 0.3 <= c.get('score', 0) < 0.6]
        
        system_prompt = f"""ë‹¹ì‹ ì€ ìš°ë¦¬ íšŒì‚¬ì˜ ë‚´ê·œë¥¼ ì •í™•íˆ ì•„ëŠ” ì¹œê·¼í•œ ë™ë£Œì…ë‹ˆë‹¤. ì§ì›ë“¤ì˜ ê¶ê¸ˆí•œ ì ì„ ìì—°ìŠ¤ëŸ½ì§€ë§Œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ğŸ¯ í•µì‹¬ ì›ì¹™ (ì ˆëŒ€ ì¤€ìˆ˜):
â€¢ **ì •í™•í•œ ì •ë³´ í™œìš©**: ì œê³µëœ {len(context)}ê°œì˜ ê³ í’ˆì§ˆ ë‚´ê·œ ì •ë³´ë¥¼ í™œìš©
â€¢ **ë‚´ê·œ ê¸°ë°˜ ë‹µë³€ í•„ìˆ˜**: ë°˜ë“œì‹œ ì œê³µëœ ë‚´ê·œ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€
â€¢ **ì¶”ì¸¡ ê¸ˆì§€**: ë‚´ê·œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ì•Šê¸°
â€¢ **ë‹¤ê°ë„ ê²€í† **: ì—¬ëŸ¬ ê´€ë ¨ ë‚´ê·œë¥¼ ì¢…í•©ì ìœ¼ë¡œ í™œìš©
â€¢ **ë¶ˆí™•ì‹¤ì‹œ ëª…ì‹œ**: ê´€ë ¨ ë‚´ê·œê°€ ì—†ìœ¼ë©´ "ë‚´ê·œì—ì„œ í™•ì¸ì´ ì–´ë ¤ì›Œìš”"ë¼ê³  ëª…í™•íˆ í‘œí˜„
â€¢ **ë‹´ë‹¹ ë¶€ì„œ ì•ˆë‚´**: ì •í™•í•œ ë‹µë³€ì´ ì–´ë ¤ìš¸ ë•ŒëŠ” ê²½ì˜ì§€ì›íŒ€ ë˜ëŠ” ê°ì‚¬íŒ€ ë¬¸ì˜ ì•ˆë‚´
â€¢ **ì—°ì°¨ ê³„ì‚°**: ì—°ì°¨ì¼ìˆ˜ = min(ê¸°ë³¸ì—°ì°¨ì¼ìˆ˜ + âŒŠ (ê·¼ì†ì—°ìˆ˜(ë…„) âˆ’ 1) Ã· 2 âŒ‹, 25)
â€¢ **ë‚ ì§œ ê´€ë ¨ ì§ˆë¬¸**: í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ ê³„ì‚°ê³¼ ì•ˆë‚´ ì œê³µ

ğŸ’¬ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ë°©ì‹:
â€¢ ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ì„¤ëª…í•˜ë˜, ë‚´ê·œ ë‚´ìš©ì€ ì •í™•íˆ ì „ë‹¬
â€¢ ê³ í’ˆì§ˆ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©
â€¢ ë³µì¡í•œ ë‚´ìš©ì€ "ì‰½ê²Œ ë§í•˜ë©´", "ì •ë¦¬í•˜ë©´" ë“±ìœ¼ë¡œ í’€ì–´ì„œ ì„¤ëª…
â€¢ ì´ì „ ëŒ€í™”ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ë˜, ë‚´ê·œ ë²”ìœ„ ë‚´ì—ì„œë§Œ
â€¢ ë‚ ì§œë‚˜ ê¸°ê°„ ê´€ë ¨ ì§ˆë¬¸ ì‹œ í˜„ì¬ ë‚ ì§œ({datetime_info})ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •í™•í•œ ê³„ì‚° ì œê³µ

ğŸ“‹ ë‹µë³€ ë°©ì‹:
â€¢ ì‹ ë¢°ë„ê°€ ë†’ì€ ì •ë³´ ìš°ì„  í™œìš©
â€¢ ì—¬ëŸ¬ ê·œì •ì´ ê´€ë ¨ë˜ë©´ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë¦¬
â€¢ ë¶€ë¶„ì  ì •ë³´ë§Œ ìˆìœ¼ë©´ í™•ì‹¤í•œ ë¶€ë¶„ê³¼ ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ êµ¬ë¶„
â€¢ ì ˆëŒ€ ë‚´ê·œì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ê±°ë‚˜ ì„ì˜ í•´ì„ ê¸ˆì§€
â€¢ ë‚ ì§œ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš° í˜„ì¬ ë‚ ì§œ({datetime_info})ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ê³„ì‚°"""

        if context:
            system_prompt += f"\n\nğŸ“š ì°¸ê³  ë‚´ê·œ ì •ë³´ (ì´ {len(context)}ê°œ, ê´€ë ¨ë„ ìˆœ ì •ë ¬):\n"
            
            # í˜„ì¬ ê²€ìƒ‰ ê²°ê³¼
            current_contexts = [c for c in context if c.get('source_type') == 'current']
            if current_contexts:
                system_prompt += f"\n[ì´ë²ˆ ì§ˆë¬¸ ê´€ë ¨ ë‚´ê·œ: {len(current_contexts)}ê°œ]\n"
                for i, item in enumerate(current_contexts, 1):
                    score = item.get('score', 0)
                    # ì¬ë­í‚¹ ì ìˆ˜ ëŒ€ì‹  ì¼ë°˜ì ì¸ ê´€ë ¨ë„ í‘œí˜„
                    if score >= 0.8:
                        relevance = "ë§¤ìš° ê´€ë ¨ ë†’ìŒ"
                    elif score >= 0.6:
                        relevance = "ê´€ë ¨ ë†’ìŒ"
                    else:
                        relevance = "ê´€ë ¨ ìˆìŒ"
                    
                    system_prompt += f"{i}. [{item.get('main_category', 'N/A')} > {item.get('sub_category', 'N/A')}]\n"
                    system_prompt += f"   Q: {item.get('question', 'N/A')}\n"
                    system_prompt += f"   A: {item.get('answer', 'N/A')}\n"
                    system_prompt += f"   (ê´€ë ¨ë„: {relevance})\n\n"
                
                system_prompt += f"ğŸ’¡ ì´ {len(current_contexts)}ê°œì˜ ê³ í’ˆì§ˆ ë‚´ê·œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê´€ë ¨ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ìš°ì„  í™œìš©í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\n"
            
            # ì´ì „ ëŒ€í™” ê´€ë ¨ ë‚´ê·œ
            previous_contexts = [c for c in context if c.get('source_type') == 'previous']
            if previous_contexts:
                system_prompt += f"\n[ì´ì „ ëŒ€í™” ê´€ë ¨ ë‚´ê·œ: {len(previous_contexts)}ê°œ]\n"
                for i, item in enumerate(previous_contexts, len(current_contexts) + 1):
                    system_prompt += f"{i}. [{item.get('main_category', 'N/A')} > {item.get('sub_category', 'N/A')}]\n"
                    system_prompt += f"   Q: {item.get('question', 'N/A')}\n"
                    system_prompt += f"   A: {item.get('answer', 'N/A')}\n"
                    system_prompt += f"   (ì´ì „ ëŒ€í™” ì°¸ê³ )\n\n"
            
            # í’ˆì§ˆì— ë”°ë¥¸ ì•ˆë‚´ (ì¬ë­í‚¹ ì–¸ê¸‰ ì œê±°)
            if len(high_quality) == 0 and len(medium_quality) == 0:
                system_prompt += "\nâš ï¸ ì£¼ì˜: ì§ì ‘ ê´€ë ¨ëœ ë‚´ê·œë¥¼ ì°¾ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ë¶€ë¶„ì  ì •ë³´ë§Œ ìˆìœ¼ë¯€ë¡œ í™•ì‹¤í•œ ë¶€ë¶„ë§Œ ë‹µë³€í•˜ê³  ë‹´ë‹¹ ë¶€ì„œ ë¬¸ì˜ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”.\n"
            elif len(high_quality) >= 3:
                system_prompt += f"\nâœ… ìš°ìˆ˜: ê´€ë ¨ì„±ì´ ë§¤ìš° ë†’ì€ ë‚´ê·œ {len(high_quality)}ê°œë¥¼ í¬í•¨í•˜ì—¬ ì´ {len(context)}ê°œì˜ ì •í™•í•œ ì •ë³´ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤. ì´ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì™„ì „í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\n"
            else:
                system_prompt += f"\nğŸ’¡ ì–‘í˜¸: {len(medium_quality)}ê°œì˜ ê´€ë ¨ ë‚´ê·œì™€ ì´ {len(context)}ê°œì˜ ì •ë³´ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤. ê´€ë ¨ë„ê°€ ë†’ì€ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.\n"
            
            # ì¼ë°˜ì ì¸ ê²€ìƒ‰ í’ˆì§ˆ ì•ˆë‚´
            system_prompt += f"\nğŸ¯ ê²€ìƒ‰ í’ˆì§ˆ ì•ˆë‚´: ì œê³µëœ ì •ë³´ëŠ” ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ë¶„ì„ë˜ì–´ ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ê³ í’ˆì§ˆ ì •ë³´ì…ë‹ˆë‹¤. ê´€ë ¨ë„ê°€ ë†’ì„ìˆ˜ë¡ ë” ì •í™•í•˜ê³  ê´€ë ¨ì„±ì´ ë†’ì€ ì •ë³´ì´ë¯€ë¡œ, ì´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•´ì£¼ì„¸ìš”.\n"
        
        # ì´ì „ ëŒ€í™” ë§¥ë½ ì¶”ê°€
        if conversation_history and len(conversation_history) > 0:
            system_prompt += "\n\nğŸ’¬ ì´ì „ ëŒ€í™” ë§¥ë½:\n"
            recent_history = conversation_history[-3:]
            for msg in recent_history:
                role = "ì§ì›" if msg.get('role') == 'user' else "ë‚˜"
                content = msg.get('content', '')
                truncated_content = content[:150] + ('...' if len(content) > 150 else '')
                system_prompt += f"â€¢ {role}: {truncated_content}\n"
            system_prompt += "\nìœ„ ëŒ€í™”ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n"
        
        system_prompt += "\n\nğŸ“ ì¶”ê°€ ë¬¸ì˜ì²˜:\nâ€¢ ê²½ì˜ì§€ì›íŒ€: ì¸ì‚¬, ê¸‰ì—¬, ë³µë¦¬í›„ìƒ, ì´ë¬´, ì¬ë¬´, íšŒê³„ ê´€ë ¨\nâ€¢ ê°ì‚¬íŒ€: ë²•ì  íŒë‹¨, ì»´í”Œë¼ì´ì–¸ìŠ¤, ê·œì • í•´ì„ ê´€ë ¨"
        
        return system_prompt
    
    def generate_response_with_history(self, query: str, context: List[Dict[str, Any]], 
                                     conversation_history: List[Dict[str, str]] = None):
        """ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ ì¼ë°˜ ì‘ë‹µ ìƒì„±"""
        logger.info(f"ğŸ“ ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ ì‘ë‹µ ìƒì„±: '{query[:50]}...' (ì»¨í…ìŠ¤íŠ¸: {len(context)}ê°œ)")
        
        try:
            # ì¼ë°˜í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            system_content = self._create_enhanced_system_prompt(context, conversation_history)
            
            messages = [{"role": "system", "content": system_content}]
            
            if conversation_history:
                recent_history = conversation_history[-2:] if len(conversation_history) > 2 else conversation_history
                for msg in recent_history:
                    if msg.get('role') in ['user', 'assistant']:
                        messages.append({
                            "role": msg['role'],
                            "content": msg['content']
                        })
            
            messages.append({"role": "user", "content": query})

            response = requests.post(
                self.api_url,
                json={
                    "model": "qwen3-30b-a3b-mlx",
                    "messages": messages,
                    "temperature": 0.05,  # ì •í™•ì„± ê°•í™”
                    "max_tokens": 3000,
                    "top_p": 0.85,
                    "frequency_penalty": 0.1,
                    "presence_penalty": 0.05,
                    "stream": False
                },
                timeout=150
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result['choices'][0]['message']['content']
                
                if not generated_text.strip():
                    return "ì£„ì†¡í•´ìš”, ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”."
                
                logger.info(f"âœ… ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(generated_text)}ì)")
                return generated_text
            else:
                return "ì£„ì†¡í•´ìš”, ì§€ê¸ˆ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆëŠ” ê²ƒ ê°™ì•„ìš”."
                
        except requests.exceptions.Timeout:
            return "ì •ë³´ ì²˜ë¦¬ì— ì‹œê°„ì´ ê±¸ë¦¬ê³  ìˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        except Exception as e:
            logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return "ì£„ì†¡í•´ìš”, ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”."

    def generate_response_stream_with_history(self, query: str, context: List[Dict[str, Any]], 
                                            conversation_history: List[Dict[str, str]] = None):
        """ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± - ìˆ˜ì •ëœ ë²„ì „"""
        logger.info(f"ğŸ“ ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±: '{query[:50]}...' (ì»¨í…ìŠ¤íŠ¸: {len(context)}ê°œ)")
        
        try:
            # ì¼ë°˜í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            system_content = self._create_enhanced_system_prompt(context, conversation_history)
            
            messages = [{"role": "system", "content": system_content}]
            
            if conversation_history:
                recent_history = conversation_history[-2:] if len(conversation_history) > 2 else conversation_history
                for msg in recent_history:
                    if msg.get('role') in ['user', 'assistant']:
                        messages.append({
                            "role": msg['role'],
                            "content": msg['content']
                        })
            
            messages.append({"role": "user", "content": query})

            response = requests.post(
                self.api_url,
                json={
                    "model": "qwen3-30b-a3b-mlx",
                    "messages": messages,
                    "temperature": 0.05,  # ì •í™•ì„± ê°•í™”
                    "max_tokens": 3000,
                    "top_p": 0.85,
                    "frequency_penalty": 0.1,
                    "presence_penalty": 0.05,
                    "stream": True
                },
                timeout=150,
                stream=True
            )
            
            if response.status_code == 200:
                generated_text = ""
                
                for line in response.iter_lines():
                    if line:
                        line_text = line.decode('utf-8')
                        if line_text.startswith('data: '):
                            data_str = line_text[6:]
                            
                            if data_str.strip() == '[DONE]':
                                break
                                
                            try:
                                data = json.loads(data_str)
                                delta = data['choices'][0]['delta']
                                
                                if 'content' in delta:
                                    content = delta['content']
                                    generated_text += content
                                    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ yield
                                    yield {
                                        "type": "content",
                                        "content": content
                                    }
                                
                                if 'finish_reason' in data['choices'][0] and data['choices'][0]['finish_reason']:
                                    yield {
                                        "type": "finish",
                                        "content": ""
                                    }
                                    break
                                    
                            except json.JSONDecodeError:
                                continue
                            except (KeyError, IndexError):
                                continue
                
                if not generated_text.strip():
                    yield {
                        "type": "error",
                        "content": "ì£„ì†¡í•´ìš”, ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”."
                    }
                
                logger.info(f"âœ… ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(generated_text)}ì)")
            else:
                yield {
                    "type": "error",
                    "content": "ì£„ì†¡í•´ìš”, ì§€ê¸ˆ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆëŠ” ê²ƒ ê°™ì•„ìš”."
                }
                
        except requests.exceptions.Timeout:
            yield {
                "type": "error",
                "content": "ì •ë³´ ì²˜ë¦¬ì— ì‹œê°„ì´ ê±¸ë¦¬ê³  ìˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            }
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            yield {
                "type": "error",
                "content": "ì£„ì†¡í•´ìš”, ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”."
            }

# FastAPI ì•± ì„¤ì •
app = FastAPI(
    title="ê³ í’ˆì§ˆ ê²€ìƒ‰ ê¸°ë°˜ íšŒì‚¬ ë‚´ê·œ RAG ì‹œìŠ¤í…œ (ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›)",
    description="""
    **FastAPI ê¸°ë°˜ ê³ í’ˆì§ˆ ê²€ìƒ‰ íšŒì‚¬ ë‚´ê·œ RAG ì‹œìŠ¤í…œ (ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›)**
    
    ## ì£¼ìš” íŠ¹ì§•
    - ğŸ§  **ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜**: ë‹¤ì¤‘ ê´€ì  ë¶„ì„ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ ì œê³µ
    - ğŸ¯ **2ë‹¨ê³„ ê²€ìƒ‰ ì•„í‚¤í…ì²˜**: ê´‘ë²”ìœ„í•œ í›„ë³´ ìˆ˜ì§‘ â†’ ì •êµí•œ í’ˆì§ˆ ë¶„ì„
    - ğŸ¤– **ë‹¤ì¤‘ ê²€ìƒ‰ ë°©ë²•**: ê³ ê¸‰ ê²€ìƒ‰ ë˜ëŠ” í–¥ìƒëœ ê²€ìƒ‰ ì„ íƒ ê°€ëŠ¥
    - ğŸ“Š **ë‹¤ì¤‘ ì ìˆ˜ ìœµí•©**: ì˜ë¯¸ì  ìœ ì‚¬ë„ + í‚¤ì›Œë“œ ë§¤ì¹­ + êµ¬ì¡°ì  ê´€ë ¨ì„±
    - ğŸ§  **ë§¥ë½ ì¸ì‹ ê²€ìƒ‰**: ì´ì „ ëŒ€í™”ë¥¼ ê³ ë ¤í•œ ë™ì  ì ìˆ˜ ì¡°ì •
    - ğŸ“ **í‚¤ì›Œë“œ ë§¤ì¹­ ìµœì í™”**: ì „í†µì  ì •ë³´ ê²€ìƒ‰ ê¸°ë²• í™œìš©
    - ğŸ—ï¸ **êµ¬ì¡°ì  ê´€ë ¨ì„± ë¶„ì„**: ì§ˆë¬¸-ë‹µë³€ êµ¬ì¡° ê³ ë ¤
    - ğŸ’ **ë‹µë³€ í’ˆì§ˆ í‰ê°€**: ê¸¸ì´, êµ¬ì¡°, ì˜ˆì‹œ í¬í•¨ ì—¬ë¶€ ë“± ì¢…í•© í‰ê°€
    - ğŸ”„ **ë™ì  ì¹´í…Œê³ ë¦¬ ê´€ë¦¬**: ChromaDBì—ì„œ ì‹¤ì‹œê°„ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ (ì‹ ê·œ ë°ì´í„° ì¦‰ì‹œ ë°˜ì˜)
    
    ## ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜
    - **ì˜ë¯¸ì  ìœ ì‚¬ë„**: 30% (ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„)
    - **êµ¬ì¡°ì  ê´€ë ¨ì„±**: 25% (ì§ˆë¬¸-ë‹µë³€ ë§¤ì¹­)
    - **í‚¤ì›Œë“œ ê´€ë ¨ì„±**: 20% (í‚¤ì›Œë“œ ë§¤ì¹­)
    - **ì›ë³¸ ë²¡í„° ì ìˆ˜**: 15% (ê¸°ë³¸ ìœ ì‚¬ë„)
    - **ë‹µë³€ í’ˆì§ˆ**: 5% (êµ¬ì¡°ì  ì™„ì„±ë„)
    - **ë§¥ë½ ê´€ë ¨ì„±**: 5% (ì´ì „ ëŒ€í™” ê³ ë ¤)
    
    ## ë™ì  ì¹´í…Œê³ ë¦¬ ê¸°ëŠ¥
    - **ì‹¤ì‹œê°„ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ**: ChromaDBì—ì„œ ì§ì ‘ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ
    - **ì‹ ê·œ ë°ì´í„° ì¦‰ì‹œ ë°˜ì˜**: ìƒˆë¡œ ì¶”ê°€ëœ ë‚´ê·œê°€ ë°”ë¡œ ì¹´í…Œê³ ë¦¬ì— í‘œì‹œ
    - **ê¸°ì¡´ ë°ì´í„° í˜¸í™˜**: íŒŒì¼ ê¸°ë°˜ ë°ì´í„°ì™€ ë²¡í„° DB ë°ì´í„° í†µí•© ê´€ë¦¬
    - **ìë™ fallback**: ChromaDB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ìë™ ì „í™˜
    
    ## ì„±ëŠ¥ ì§€í‘œ
    - **í›„ë³´ ìˆ˜ì§‘**: 4ë°° í™•ì¥ (top_k Ã— 4ê°œ í›„ë³´)
    - **ê²€ìƒ‰ ì •í™•ë„**: ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœê³  ìˆ˜ì¤€
    - **ë‹¤ì–‘ì„± ë³´ì¥**: ì¤‘ë³µ ì œê±° + í’ˆì§ˆ ìœ ì§€
    - **ë§¥ë½ ì¸ì‹**: ì´ì „ ëŒ€í™” ë§¥ë½ ë°˜ì˜
    - **ì¹´í…Œê³ ë¦¬ ê´€ë¦¬**: ë™ì  + ì •ì  í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹
    """,
    version="4.2.0-dynamic-categories",
    contact={
        "name": "ê³ í’ˆì§ˆ ê²€ìƒ‰ RAG ì‹œìŠ¤í…œ ê°œë°œíŒ€ (ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›)",
        "email": "dev@company.com"
    }
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬ê¸°
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    logger.error(f"âŒ ìš”ì²­ ê²€ì¦ ì‹¤íŒ¨: {exc}")
    
    error_details = []
    for error in exc.errors():
        field = " -> ".join(str(loc) for loc in error["loc"])
        error_details.append({
            "field": field,
            "message": error["msg"],
            "type": error["type"]
        })
    
    return JSONResponse(
        status_code=422,
        content={
            "error": "ìš”ì²­ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨",
            "details": error_details,
            "help": {
                "message": "API ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ìš”ì²­í•´ì£¼ì„¸ìš”.",
                "docs_url": "/docs",
                "search_features": [
                    "ê³ ê¸‰ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ ëŒ€í­ í–¥ìƒ",
                    "2ë‹¨ê³„ ê²€ìƒ‰ ì•„í‚¤í…ì²˜ ì ìš©",
                    "ë‹¤ì¤‘ ì ìˆ˜ ìœµí•© ì•Œê³ ë¦¬ì¦˜",
                    "ë§¥ë½ ì¸ì‹ ê²€ìƒ‰",
                    "ë™ì  ì¹´í…Œê³ ë¦¬ ê´€ë¦¬"
                ]
            }
        }
    )

@app.exception_handler(500)
async def internal_server_error_handler(request: Request, exc: Exception):
    logger.error(f"âŒ ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "error": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "message": "ì‹œìŠ¤í…œ ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
            "request_id": id(request)
        }
    )

# ì „ì—­ ê°ì²´ë“¤
rag_system: CompanyRegulationsRAGSystem = None
llm_client: LLMClient = None

def initialize_system():
    """ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global rag_system, llm_client
    
    if rag_system is not None and llm_client is not None:
        logger.info("ì‹œìŠ¤í…œì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    logger.info("ğŸ¯ ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘... (ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›)")
    
    try:
        rag_system = CompanyRegulationsRAGSystem()
        llm_client = LLMClient()
        
        data_directory = "./data_json"
        
        if os.path.exists(data_directory):
            logger.info("íšŒì‚¬ ë‚´ê·œ ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì¶•...")
            if rag_system.load_company_regulations_data(data_directory):
                if rag_system.build_index():
                    stats = rag_system.get_stats()
                    logger.info(f"âœ… ê³ ê¸‰ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›): {stats['total_documents']}ê°œ ì²­í¬")
                    logger.info(f"ğŸ“Š ì²­í‚¹ í†µê³„: {stats.get('chunk_statistics', {})}")
                    logger.info(f"ğŸ¯ ê³ ê¸‰ ê¸°ëŠ¥: {', '.join(stats.get('enhanced_features', [])[:3])}")
                else:
                    logger.error("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
            else:
                logger.error("âŒ ë‚´ê·œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        else:
            logger.warning(f"âš ï¸ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {data_directory} (ë™ì  ì¹´í…Œê³ ë¦¬ë§Œ ì‚¬ìš©)")

    except Exception as e:
        logger.critical(f"ğŸ”¥ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        rag_system = None
        llm_client = None

@app.get("/api/health", response_model=HealthResponse, summary="ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸", description="ê³ í’ˆì§ˆ ê²€ìƒ‰ ê¸°ë°˜ RAG ì‹œìŠ¤í…œì˜ ìƒíƒœì™€ í†µê³„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤ (ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›).")
async def health_check():
    """ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬"""
    if rag_system is None or llm_client is None:
        raise HTTPException(
            status_code=503,
            detail={
                "status": "initializing_or_error",
                "rag_ready": False,
                "regulations_count": 0,
                "main_categories_count": 0,
                "improvements": "ê³ ê¸‰ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ ëŒ€í­ í–¥ìƒ + ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›",
                "message": "RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ì´ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
        )

    stats = rag_system.get_stats()
    
    status = "healthy" if stats['is_ready'] else "unhealthy"

    return HealthResponse(
        status=status,
        rag_ready=stats['is_ready'],
        regulations_count=stats['total_documents'],
        main_categories_count=stats.get('total_main_categories', 0),
        chunk_statistics=stats.get('chunk_statistics', {}),
        improvements="ê³ ê¸‰ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ ëŒ€í­ í–¥ìƒ + ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›",
        enhanced_features=stats.get('enhanced_features', []) + ["ë™ì  ì¹´í…Œê³ ë¦¬ ê´€ë¦¬", "ì‹¤ì‹œê°„ ë°ì´í„° ë°˜ì˜"]
    )

@app.get("/api/categories", summary="ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¡°íšŒ (ë™ì )", description="ë™ì  ì¹´í…Œê³ ë¦¬ ì¡°íšŒ: ChromaDBì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ì¡°íšŒí•˜ì—¬ ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°ë„ ì¦‰ì‹œ ë°˜ì˜í•©ë‹ˆë‹¤.")
async def get_categories():
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    categories_info = rag_system.get_categories()
    
    # ì¶”ê°€ì ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œë„ ì œê³µ (ìì—°ìŠ¤ëŸ¬ìš´ ìˆ«ì ì •ë ¬ëœ ìˆœì„œë¡œ)
    sorted_categories_list = []
    for main_category, info in categories_info['main_categories'].items():
        sorted_categories_list.append({
            'main_category': main_category,
            'file_name': info['file_name'],
            'sub_categories': info['sub_categories'],
            'total_faqs': info['total_faqs']
        })
    
    return {
        "categories": categories_info,
        "sorted_categories_list": sorted_categories_list,  # ìì—°ìŠ¤ëŸ¬ìš´ ìˆ«ì ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ì¶”ê°€
        "database": "ChromaDB (Advanced Search + Dynamic Categories)",
        "system_type": "ê³ í’ˆì§ˆ ê²€ìƒ‰ ê¸°ë°˜ íšŒì‚¬ ë‚´ê·œ ì‹œìŠ¤í…œ (ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›)",
        "data_source": "ChromaDB ìš°ì„  + íŒŒì¼ fallback",
        "sorting_info": {
            "type": "natural_numeric_sorting",
            "description": "1-1, 1-2, 1-3, ..., 1-9, 1-10, 1-11, 1-12 ìˆœì„œë¡œ ì •ë ¬ë©ë‹ˆë‹¤"
        }
    }

@app.get("/api/categories_dynamic", summary="ë™ì  ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¡°íšŒ (ChromaDB ì§ì ‘)", 
         description="ChromaDBì—ì„œ ì§ì ‘ ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ë™ì ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤ (ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„° í¬í•¨).")
async def get_categories_dynamic():
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    categories_info = rag_system.get_categories_from_vector_db()
    
    # ì¶”ê°€ì ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œë„ ì œê³µ
    sorted_categories_list = []
    for main_category, info in categories_info['main_categories'].items():
        sorted_categories_list.append({
            'main_category': main_category,
            'file_name': info['file_name'],
            'sub_categories': info['sub_categories'],
            'total_faqs': info['total_faqs']
        })
    
    return {
        "categories": categories_info,
        "sorted_categories_list": sorted_categories_list,
        "database": "ChromaDB (Dynamic Search)",
        "system_type": "ë™ì  ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹œìŠ¤í…œ",
        "data_source": "ChromaDB ì§ì ‘ ì¡°íšŒ (ì‹¤ì‹œê°„)",
        "sorting_info": {
            "type": "natural_numeric_sorting",
            "description": "1-1, 1-2, 1-3, ..., 1-9, 1-10, 1-11, 1-12 ìˆœì„œë¡œ ì •ë ¬ë©ë‹ˆë‹¤"
        }
    }

@app.post("/api/refresh_categories", summary="ì¹´í…Œê³ ë¦¬ ìºì‹œ ìƒˆë¡œê³ ì¹¨",
          description="ChromaDBì—ì„œ ìµœì‹  ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤.")
async def refresh_categories():
    """ì¹´í…Œê³ ë¦¬ ì •ë³´ ìƒˆë¡œê³ ì¹¨"""
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # ChromaDBì—ì„œ ìµœì‹  ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¡°íšŒ
        dynamic_categories = rag_system.get_categories_from_vector_db()
        
        return {
            "success": True,
            "message": "ì¹´í…Œê³ ë¦¬ ì •ë³´ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "updated_categories": dynamic_categories['total_main_categories'],
            "total_regulations": dynamic_categories['total_regulations'],
            "main_categories": list(dynamic_categories['main_categories'].keys()),
            "refreshed_at": datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
            "data_source": "ChromaDB ì§ì ‘ ì¡°íšŒ"
        }
        
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì¹´í…Œê³ ë¦¬ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/search", response_model=SearchResponse, summary="ê³ í’ˆì§ˆ ë‚´ê·œ ê²€ìƒ‰", description="ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì •í™•í•œ ê´€ë ¨ ë‚´ê·œë¥¼ ì°¾ìŠµë‹ˆë‹¤.")
async def search_regulations(request: SearchRequest):
    """ê³ í’ˆì§ˆ íšŒì‚¬ ë‚´ê·œ ê²€ìƒ‰"""
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ê³ ê¸‰ ê²€ìƒ‰ ì‚¬ìš©
    results = rag_system.search_with_advanced_rerank(
        request.query, 
        request.top_k,
        request.main_category_filter,
        conversation_history=None,
        min_relevance_score=0.2
    )
    
    # rerank_details ì œê±°
    clean_results = []
    for result in results:
        clean_result = {k: v for k, v in result.items() if k != 'rerank_details'}
        clean_results.append(SearchResult(**clean_result))
    
    return SearchResponse(
        query=request.query,
        results=clean_results,
        count=len(results),
        main_category_filter=request.main_category_filter,
        search_type="ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë°˜ 2ë‹¨ê³„ ê²€ìƒ‰",
        min_relevance=0.2,
        enhanced_features=[
            "ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„",
            "í‚¤ì›Œë“œ ë§¤ì¹­ ìµœì í™”",
            "êµ¬ì¡°ì  ê´€ë ¨ì„± ë¶„ì„",
            "ë‹¤ì¤‘ ì ìˆ˜ ìœµí•© ì•Œê³ ë¦¬ì¦˜",
            f"4ë°° í™•ì¥ëœ í›„ë³´ ìˆ˜ì§‘ (ìµœëŒ€ {request.top_k * 4}ê°œ)"
        ]
    )

@app.post("/api/chat", response_model=ChatResponse, summary="ë‚´ê·œ ìƒë‹´", description="ê³ í’ˆì§ˆ ê²€ìƒ‰ì„ í†µí•œ ë‚´ê·œ ìƒë‹´ì„ ì œê³µí•©ë‹ˆë‹¤.")
async def chat_with_rag(request: ChatRequest):
    """ë‚´ê·œ ìƒë‹´ (ì¬ë­í‚¹ ì •ë³´ ìˆ¨ê¹€)"""
    try:
        if not rag_system or not llm_client:
            raise HTTPException(status_code=503, detail="ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # conversation_historyë¥¼ Dict í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        conversation_history = []
        for msg in request.conversation_history:
            conversation_history.append({
                "role": msg.role,
                "content": msg.content,
                "context": getattr(msg, 'context', None)
            })
        
        # ê²€ìƒ‰ (ë‚´ë¶€ì ìœ¼ë¡œëŠ” ì¬ë­í‚¹ ì‚¬ìš©í•˜ë˜ ì‚¬ìš©ìì—ê²ŒëŠ” ìˆ¨ê¹€)
        if request.use_reranker:
            current_results = rag_system.search_with_advanced_rerank(
                request.query, 
                top_k=20,
                main_category_filter=request.main_category_filter,
                conversation_history=conversation_history,
                min_relevance_score=0.2,
                reranker_type=request.reranker_type.value
            )
            search_type = "ê³ ê¸‰ ê²€ìƒ‰"
            response_type = "ê³ í’ˆì§ˆ ì •í™•ì„± ê°•í™” ëŒ€í™”í˜•"
        else:
            current_results = rag_system.search_with_enhanced_retrieval(
                request.query, 
                top_k=20,
                main_category_filter=request.main_category_filter,
                min_relevance_score=0.3
            )
            search_type = "í–¥ìƒëœ ê²€ìƒ‰"
            response_type = "í–¥ìƒëœ ëŒ€í™”í˜•"
        
        # ì»¨í…ìŠ¤íŠ¸ ê²°í•©
        combined_context = rag_system.combine_contexts_with_history(
            current_results, 
            conversation_history, 
            max_total_context=25
        )
        
        # ì‘ë‹µ ìƒì„±
        response = llm_client.generate_response_with_history(
            request.query, 
            combined_context, 
            conversation_history
        )
        
        # í’ˆì§ˆ ë¶„ì„ (ë‚´ë¶€ì ìœ¼ë¡œ)
        if request.use_reranker:
            high_quality = len([c for c in combined_context if c.get('score', 0) >= 0.8])
            medium_quality = len([c for c in combined_context if 0.6 <= c.get('score', 0) < 0.8])
            low_quality = len([c for c in combined_context if 0.3 <= c.get('score', 0) < 0.6])
        else:
            high_quality = len([c for c in combined_context if c.get('score', 0) >= 0.7])
            medium_quality = len([c for c in combined_context if 0.5 <= c.get('score', 0) < 0.7])
            low_quality = len([c for c in combined_context if 0.3 <= c.get('score', 0) < 0.5])
        
        # SearchResult ìƒì„± ì‹œ rerank_details ì œê±°
        context_results = []
        for ctx in combined_context:
            clean_ctx = {k: v for k, v in ctx.items() if k != 'rerank_details'}
            context_results.append(SearchResult(**clean_ctx))
        
        # ì‚¬ìš©ì ì¹œí™”ì ì¸ ê¸°ëŠ¥ ì„¤ëª…
        enhanced_features = get_user_friendly_features(
            request.use_reranker, 
            len(combined_context), 
            len(current_results)
        )
        
        return ChatResponse(
            query=request.query,
            response=response,
            context=context_results,
            context_count=len(combined_context),
            context_quality={
                "high_relevance": high_quality,
                "medium_relevance": medium_quality,
                "low_relevance": low_quality
            },
            search_type=search_type,
            response_type=response_type,
            enhanced_features=enhanced_features
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ìƒë‹´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, 
            detail=f"ìƒë‹´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@app.post("/api/chat_stream", summary="ì‹¤ì‹œê°„ ë‚´ê·œ ìƒë‹´", description="ê³ í’ˆì§ˆ ê²€ìƒ‰ì„ í†µí•œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë‚´ê·œ ìƒë‹´ì„ ì œê³µí•©ë‹ˆë‹¤.")
async def chat_with_rag_stream(request: StreamChatRequest):
    """ìŠ¤íŠ¸ë¦¬ë° ìƒë‹´ (ì¬ë­í‚¹ ì •ë³´ ìˆ¨ê¹€)"""
    try:
        if not rag_system or not llm_client:
            raise HTTPException(status_code=503, detail="ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        conversation_history = request.conversation_history or []
        
        # ê²€ìƒ‰ (ë‚´ë¶€ì ìœ¼ë¡œëŠ” ì¬ë­í‚¹ ì‚¬ìš©í•˜ë˜ ì‚¬ìš©ìì—ê²ŒëŠ” ìˆ¨ê¹€)
        if request.use_reranker:
            current_results = rag_system.search_with_advanced_rerank(
                request.query, 
                top_k=20,
                main_category_filter=request.main_category_filter,
                conversation_history=conversation_history,
                min_relevance_score=0.2,
                reranker_type=request.reranker_type.value
            )
            search_type = "ê³ ê¸‰ ê²€ìƒ‰"
        else:
            current_results = rag_system.search_with_enhanced_retrieval(
                request.query, 
                top_k=20,
                main_category_filter=request.main_category_filter,
                min_relevance_score=0.3
            )
            search_type = "í–¥ìƒëœ ê²€ìƒ‰"
        
        # ì»¨í…ìŠ¤íŠ¸ ê²°í•©
        combined_context = rag_system.combine_contexts_with_history(
            current_results, 
            conversation_history, 
            max_total_context=25
        )
        
        def generate():
            try:
                # í’ˆì§ˆ ë¶„ì„ (ë‚´ë¶€ì ìœ¼ë¡œ)
                if request.use_reranker:
                    high_quality = len([c for c in combined_context if c.get('score', 0) >= 0.8])
                    medium_quality = len([c for c in combined_context if 0.6 <= c.get('score', 0) < 0.8])
                    low_quality = len([c for c in combined_context if 0.3 <= c.get('score', 0) < 0.6])
                else:
                    high_quality = len([c for c in combined_context if c.get('score', 0) >= 0.7])
                    medium_quality = len([c for c in combined_context if 0.5 <= c.get('score', 0) < 0.7])
                    low_quality = len([c for c in combined_context if 0.3 <= c.get('score', 0) < 0.5])
                
                # ì‚¬ìš©ì ì¹œí™”ì ì¸ ê¸°ëŠ¥ ì„¤ëª…
                enhanced_features = get_user_friendly_features(
                    request.use_reranker, 
                    len(combined_context), 
                    len(current_results)
                )
                
                # rerank_details ì œê±°
                clean_combined_context = []
                for ctx in combined_context:
                    clean_ctx = {k: v for k, v in ctx.items() if k != 'rerank_details'}
                    clean_combined_context.append(clean_ctx)
                
                context_data = {
                    "type": "context",
                    "query": request.query,
                    "context": clean_combined_context,
                    "context_count": len(combined_context),
                    "context_quality": {
                        "high_relevance": high_quality,
                        "medium_relevance": medium_quality,
                        "low_relevance": low_quality
                    },
                    "search_type": search_type,
                    "enhanced_features": enhanced_features
                }
                
                yield f"data: {json.dumps(context_data, ensure_ascii=False)}\n\n"
                
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± - ìˆ˜ì •ëœ ë¶€ë¶„
                for chunk in llm_client.generate_response_stream_with_history(
                    request.query, 
                    combined_context, 
                    conversation_history
                ):
                    # chunkëŠ” ì´ì œ ë”•ì…”ë„ˆë¦¬ì´ë¯€ë¡œ ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                
                yield f"data: {json.dumps({'type': 'stream_end'}, ensure_ascii=False)}\n\n"
                
            except Exception as stream_error:
                logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {stream_error}", exc_info=True)
                error_data = {
                    "type": "error",
                    "content": f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(stream_error)}"
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        
        return StreamingResponse(
            generate(),
            media_type='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type',
                'Access-Control-Allow-Methods': 'POST'
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ìƒë‹´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, 
            detail=f"ìŠ¤íŠ¸ë¦¬ë° ìƒë‹´ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}"
        )

@app.post("/api/rebuild_index", summary="ì¸ë±ìŠ¤ ì¬êµ¬ì¶•", description="ë‚´ê·œ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•©ë‹ˆë‹¤.")
async def rebuild_index():
    """ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if rag_system.load_company_regulations_data("./data_json"):
        if rag_system.rebuild_index():
            stats = rag_system.get_stats()
            return {
                "message": "ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì¬êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤",
                "regulations_count": stats['total_documents'],
                "chunk_statistics": stats.get('chunk_statistics', {}),
                "improvements": "ê³ ê¸‰ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ ëŒ€í­ í–¥ìƒ + ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›",
                "enhanced_features": stats.get('enhanced_features', []),
                "search_performance": {
                    "retrieval_expansion": "4ë°° í›„ë³´ ìˆ˜ì§‘",
                    "quality_algorithm": "ë‹¤ì¤‘ ì ìˆ˜ ìœµí•©",
                    "accuracy_boost": "ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„",
                    "context_awareness": "ì´ì „ ëŒ€í™” ë§¥ë½ ë°˜ì˜",
                    "dynamic_categories": "ì‹¤ì‹œê°„ ì¹´í…Œê³ ë¦¬ ê´€ë¦¬"
                }
            }
        else:
            raise HTTPException(status_code=500, detail="ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨")
    else:
        raise HTTPException(status_code=500, detail="ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")

@app.get("/api/stats", summary="ì‹œìŠ¤í…œ í†µê³„", description="ê³ í’ˆì§ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ìƒì„¸ í†µê³„ì™€ ì„±ëŠ¥ ì§€í‘œë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
async def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´ (ì¬ë­í‚¹ ì •ë³´ ì¼ë°˜í™”)"""
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    stats = rag_system.get_stats()
    stats.update({
        "system_type": "ê³ í’ˆì§ˆ íšŒì‚¬ ë‚´ê·œ ê²€ìƒ‰ ì‹œìŠ¤í…œ (ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›)",
        "search_features": [
            "ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜",
            "ë‹¤ì¤‘ ê´€ì  ë¶„ì„",
            "ë§¥ë½ ì¸ì‹ ê²€ìƒ‰",
            "êµ¬ì¡°ì  ê´€ë ¨ì„± ë¶„ì„",
            "í‚¤ì›Œë“œ ë§¤ì¹­ ìµœì í™”",
            "ë‹µë³€ í’ˆì§ˆ í‰ê°€",
            "ë‹¤ì–‘ì„± ë³´ì¥ ì•Œê³ ë¦¬ì¦˜",
            "ë™ì  ì¹´í…Œê³ ë¦¬ ê´€ë¦¬"
        ],
        "quality_metrics": {
            "accuracy_enhancement": "ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ ì ìš©",
            "context_awareness": "ì´ì „ ëŒ€í™” ê³ ë ¤",
            "relevance_scoring": "ë‹¤ì¤‘ ì ìˆ˜ ìœµí•©",
            "diversity_guarantee": "ì¤‘ë³µ ì œê±° ë³´ì¥",
            "dynamic_categories": "ì‹¤ì‹œê°„ ì¹´í…Œê³ ë¦¬ ë°˜ì˜"
        },
        "performance_metrics": {
            "search_expansion": "í™•ì¥ëœ í›„ë³´ ìˆ˜ì§‘",
            "quality_algorithm": "ë‹¤ì¤‘ ê´€ì  ë¶„ì„",
            "accuracy_improvement": "êµ¬ì¡°ì  + í‚¤ì›Œë“œ + ì˜ë¯¸ì ",
            "context_window": 25,
            "response_temperature": 0.05,
            "min_relevance": "ë™ì  ì¡°ì •",
            "category_management": "ChromaDB ìš°ì„  + íŒŒì¼ fallback"
        }
    })
    
    # ì¬ë­í‚¹ ê´€ë ¨ ê¸°ìˆ ì  ë‚´ìš© ì œê±°
    if 'enhanced_features' in stats:
        stats['enhanced_features'] = [
            feature.replace('ì¬ë­í‚¹', 'ê³ ê¸‰ ê²€ìƒ‰').replace('Rerank', 'ê³ ê¸‰ ê²€ìƒ‰')
            .replace('Cross-encoder', 'ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜').replace('TF-IDF', 'í‚¤ì›Œë“œ ë§¤ì¹­')
            for feature in stats['enhanced_features']
        ] + ["ë™ì  ì¹´í…Œê³ ë¦¬ ê´€ë¦¬", "ì‹¤ì‹œê°„ ë°ì´í„° ë°˜ì˜"]
    
    return stats

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/test/simple_chat", summary="ì±„íŒ… í…ŒìŠ¤íŠ¸", description="ê³ í’ˆì§ˆ ê²€ìƒ‰ì„ í†µí•œ ì±„íŒ… ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
async def simple_chat_test(request: SimpleTestRequest):
    """ê°„ë‹¨í•œ ì±„íŒ… í…ŒìŠ¤íŠ¸ (ì¬ë­í‚¹ ì •ë³´ ìˆ¨ê¹€)"""
    try:
        if not rag_system or not llm_client:
            raise HTTPException(status_code=503, detail="ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ê²€ìƒ‰ (ë‚´ë¶€ì ìœ¼ë¡œëŠ” ì¬ë­í‚¹ ì‚¬ìš©)
        if request.use_reranker:
            results = rag_system.search_with_advanced_rerank(
                request.query, 
                top_k=15, 
                main_category_filter=request.category,
                conversation_history=None,
                min_relevance_score=0.2,
                reranker_type=request.reranker_type.value
            )
            search_method = "ê³ ê¸‰ ê²€ìƒ‰"
        else:
            results = rag_system.search_with_enhanced_retrieval(
                request.query, 
                top_k=15, 
                main_category_filter=request.category,
                min_relevance_score=0.3
            )
            search_method = "í–¥ìƒëœ ê²€ìƒ‰"
        
        if results:
            response = llm_client.generate_response_with_history(request.query, results, [])
            
            # ì¼ë°˜ì ì¸ ë¶„ì„ ì •ë³´ë§Œ ì œê³µ
            analysis_info = {
                "total_candidates": len(results),
                "avg_score": sum(r['score'] for r in results) / len(results) if results else 0,
                "search_method": search_method,
                "high_quality_count": len([r for r in results if r.get('score', 0) >= 0.7])
            }
            
            info_message = f"{search_method}ìœ¼ë¡œ {len(results)}ê°œ ê³ í’ˆì§ˆ ì •ë³´ ì œê³µ"
            
            return {
                "query": request.query,
                "response": response,
                "found_results": len(results),
                "status": "success",
                "performance": analysis_info,
                "info": info_message
            }
        else:
            return {
                "query": request.query,
                "response": f"ê´€ë ¨ëœ ë‚´ê·œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({search_method} ê¸°ì¤€)",
                "found_results": 0,
                "status": "no_results",
                "search_info": f"{search_method} ìµœì†Œ ì ìˆ˜ ë¯¸ë§Œ"
            }
            
    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/test/stream_simple", summary="ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸", description="ê³ í’ˆì§ˆ ê²€ìƒ‰ì„ ì´ìš©í•œ ê°„ë‹¨í•œ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸")
async def simple_stream_test(
    query: str = Query("íœ´ê°€ ì‹ ì²­ ë°©ë²•", description="ê²€ìƒ‰í•  ì§ˆì˜", example="íœ´ê°€ ì‹ ì²­ ë°©ë²•"),
    use_reranker: bool = Query(True, description="ê³ ê¸‰ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€", example=True),
    reranker_type: RerankerType = Query(RerankerType.qwen3, description="ê²€ìƒ‰ ë°©ë²• ì„ íƒ")
):
    """ê°„ë‹¨í•œ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸"""
    try:
        def generate():
            try:
                search_method = "ê³ ê¸‰ ê²€ìƒ‰" if use_reranker else "í–¥ìƒëœ ê²€ìƒ‰"
                yield f"data: {json.dumps({'type': 'start', 'message': f'{search_method} ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: {query}'}, ensure_ascii=False)}\n\n"
                
                if not rag_system or not llm_client:
                    yield f"data: {json.dumps({'type': 'error', 'content': 'ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                    return
                
                # ê²€ìƒ‰
                if use_reranker:
                    results = rag_system.search_with_advanced_rerank(
                        query, 
                        top_k=10,
                        conversation_history=None,
                        min_relevance_score=0.2,
                        reranker_type=reranker_type.value
                    )
                else:
                    results = rag_system.search_with_enhanced_retrieval(
                        query, 
                        top_k=10,
                        min_relevance_score=0.3
                    )
                
                # ê²°ê³¼ ë¶„ì„
                analysis_info = {
                    "total_results": len(results),
                    "avg_score": sum(r['score'] for r in results) / len(results) if results else 0,
                    "search_method": search_method,
                    "high_quality_count": len([r for r in results if r.get('score', 0) >= 0.7])
                }
                
                avg_score = analysis_info["avg_score"]
                info_message = f'{search_method} ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼, í‰ê·  ì ìˆ˜ {avg_score:.3f}'
                yield f"data: {json.dumps({'type': 'info', 'message': info_message}, ensure_ascii=False)}\n\n"
                
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ - ìˆ˜ì •ëœ ë¶€ë¶„
                for chunk in llm_client.generate_response_stream_with_history(query, results, []):
                    # chunkëŠ” ì´ì œ ë”•ì…”ë„ˆë¦¬ì´ë¯€ë¡œ ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                    
                yield f"data: {json.dumps({'type': 'analysis_summary', 'content': analysis_info}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'stream_end'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
                yield f"data: {json.dumps({'type': 'error', 'content': f'ì˜¤ë¥˜: {str(e)}'}, ensure_ascii=False)}\n\n"
        
        return StreamingResponse(
            generate(),
            media_type='text/event-stream',
            headers={'Cache-Control': 'no-cache'}
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/test/search_demo", summary="ê²€ìƒ‰ ì„±ëŠ¥ ë°ëª¨", description="ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ì‹œì—°í•©ë‹ˆë‹¤.")
async def search_demo(
    query: str = Query("ì—°ì°¨ íœ´ê°€", description="ê²€ìƒ‰í•  ì§ˆì˜", example="ì—°ì°¨ íœ´ê°€"), 
    top_k: int = Query(5, description="ë°˜í™˜í•  ê²°ê³¼ ìˆ˜", example=5, ge=1, le=20),
    reranker_type: RerankerType = Query(RerankerType.qwen3, description="ê²€ìƒ‰ ë°©ë²• ì„ íƒ")
):
    """ê²€ìƒ‰ ì„±ëŠ¥ ë°ëª¨"""
    try:
        if not rag_system:
            raise HTTPException(status_code=503, detail="ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ê³ ê¸‰ ê²€ìƒ‰
        results = rag_system.search_with_advanced_rerank(
            query, 
            top_k=top_k,
            conversation_history=None,
            min_relevance_score=0.1,  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ ê²°ê³¼ ë³´ê¸°
            reranker_type=reranker_type.value
        )
        
        demo_results = []
        for i, result in enumerate(results):
            # ë‚´ë¶€ ì •ë³´ ìˆ¨ê¹€ ì²˜ë¦¬
            demo_results.append({
                "rank": i + 1,
                "question": result.get('question', ''),
                "category": f"{result.get('main_category', '')} > {result.get('sub_category', '')}",
                "final_score": result.get('score', 0),
                "relevance_level": "ë§¤ìš° ë†’ìŒ" if result.get('score', 0) >= 0.8 else "ë†’ìŒ" if result.get('score', 0) >= 0.6 else "ë³´í†µ",
                "chunk_type": result.get('chunk_type', ''),
                "answer_preview": result.get('answer', '')[:100] + "..." if len(result.get('answer', '')) > 100 else result.get('answer', '')
            })
        
        return {
            "query": query,
            "search_method": "ê³ ê¸‰ ê²€ìƒ‰",
            "search_algorithm": {
                "name": "ë‹¤ì¤‘ ì ìˆ˜ ìœµí•© ê³ ê¸‰ ê²€ìƒ‰",
                "description": "ì˜ë¯¸ì  ë¶„ì„, í‚¤ì›Œë“œ ë§¤ì¹­, êµ¬ì¡°ì  ê´€ë ¨ì„±ì„ ì¢…í•©í•œ ê³ í’ˆì§ˆ ê²€ìƒ‰",
                "features": [
                    "ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„",
                    "í‚¤ì›Œë“œ ë§¤ì¹­ ìµœì í™”",
                    "ì§ˆë¬¸-ë‹µë³€ êµ¬ì¡° ë¶„ì„",
                    "ë‹µë³€ í’ˆì§ˆ í‰ê°€",
                    "ë‹¤ì–‘ì„± ë³´ì¥ ì•Œê³ ë¦¬ì¦˜"
                ]
            },
            "results": demo_results,
            "performance_summary": {
                "total_results": len(results),
                "avg_score": sum(r['score'] for r in results) / len(results) if results else 0,
                "high_quality_count": len([r for r in results if r.get('score', 0) >= 0.8]),
                "score_distribution": {
                    "excellent": len([r for r in results if r.get('score', 0) >= 0.8]),
                    "good": len([r for r in results if 0.6 <= r.get('score', 0) < 0.8]),
                    "fair": len([r for r in results if 0.4 <= r.get('score', 0) < 0.6]),
                    "poor": len([r for r in results if r.get('score', 0) < 0.4])
                }
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ë°ëª¨ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ë°ëª¨ ì‹¤íŒ¨: {str(e)}")

# ì‚¬ìš©ì ì •ì˜ docs ì„¤ì •
@app.get("/api/docs", include_in_schema=False)
async def custom_swagger_ui_html():
    return get_swagger_ui_html(
        openapi_url=app.openapi_url,
        title=app.title + " - Swagger UI",
        oauth2_redirect_url=app.swagger_ui_oauth2_redirect_url,
        swagger_js_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui-bundle.js",
        swagger_css_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui.css",
    )

@app.get("/api/redoc", include_in_schema=False)
async def redoc_html():
    return get_redoc_html(
        openapi_url=app.openapi_url,
        title=app.title + " - ReDoc",
        redoc_js_url="https://cdn.jsdelivr.net/npm/redoc@2.1.0/bundles/redoc.standalone.js",
    )

# ì‹œì‘ ì´ë²¤íŠ¸
@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    initialize_system()

if __name__ == '__main__':
    print("=" * 90)
    print("ğŸ¯ FastAPI ê¸°ë°˜ ê³ í’ˆì§ˆ ê²€ìƒ‰ íšŒì‚¬ ë‚´ê·œ RAG ì„œë²„ (ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›)")
    print("=" * 90)
    print("ğŸš€ ë‹¤ì¤‘ ê²€ìƒ‰ ë°©ë²•:")
    print("   ğŸ§  ê³ ê¸‰ ê²€ìƒ‰: ë‹¤ì¤‘ ê´€ì  ë¶„ì„ìœ¼ë¡œ ìµœê³  ì •í™•ë„ (ê¸°ë³¸)")
    print("   ğŸ“Š í–¥ìƒëœ ê²€ìƒ‰: ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ ê°œì„  ë²„ì „")
    print("   âš™ï¸ ì˜µì…˜ ì„ íƒ ê°€ëŠ¥: ê³ ê¸‰ ê²€ìƒ‰ ì‚¬ìš©/ë¯¸ì‚¬ìš© ì„ íƒ")
    print("=" * 90)
    print("ğŸ”§ ê²€ìƒ‰ ë°©ë²• ë¹„êµ:")
    print("   ê³ ê¸‰ ê²€ìƒ‰        : ë‹¤ì¤‘ ì ìˆ˜ ìœµí•©, ë§¤ìš° ë†’ì€ ì •í™•ë„, ë³´í†µ ì†ë„ (ê¸°ë³¸)")
    print("   í–¥ìƒëœ ê²€ìƒ‰      : í‚¤ì›Œë“œ ë§¤ì¹­, ë†’ì€ ì •í™•ë„, ë¹ ë¥¸ ì†ë„")
    print("=" * 90)
    print("ğŸ“Š ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ê°€ì¤‘ì¹˜:")
    print("   ì˜ë¯¸ì  ìœ ì‚¬ë„     : 30% (ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„)")
    print("   êµ¬ì¡°ì  ê´€ë ¨ì„±     : 25% (ì§ˆë¬¸-ë‹µë³€ ë§¤ì¹­)")
    print("   í‚¤ì›Œë“œ ê´€ë ¨ì„±     : 20% (í‚¤ì›Œë“œ ë§¤ì¹­)")
    print("   ì›ë³¸ ë²¡í„° ì ìˆ˜    : 15% (ê¸°ë³¸ ìœ ì‚¬ë„)")
    print("   ë‹µë³€ í’ˆì§ˆ        : 5%  (êµ¬ì¡°ì  ì™„ì„±ë„)")
    print("   ë§¥ë½ ê´€ë ¨ì„±      : 5%  (ì´ì „ ëŒ€í™” ê³ ë ¤)")
    print("=" * 90)
    print("ğŸ”„ ë™ì  ì¹´í…Œê³ ë¦¬ ê´€ë¦¬:")
    print("   â€¢ ChromaDB ìš°ì„  ì¡°íšŒ: ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„° ì¦‰ì‹œ ë°˜ì˜")
    print("   â€¢ íŒŒì¼ ê¸°ë°˜ fallback: ChromaDB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ìë™ ì „í™˜")  
    print("   â€¢ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸: ë²¡í„° DB ì¶”ê°€ ì‹œ ì¹´í…Œê³ ë¦¬ ìë™ ì—…ë°ì´íŠ¸")
    print("   â€¢ ìì—°ìŠ¤ëŸ¬ìš´ ì •ë ¬: 1-1, 1-2, ..., 1-10, 1-11 ìˆœì„œ")
    print("=" * 90)
    print("âš™ï¸ ì‚¬ìš© ì˜µì…˜:")
    print("   use_reranker=true + reranker_type='qwen3'       : ê³ ê¸‰ ê²€ìƒ‰ (ê¸°ë³¸)")
    print("   use_reranker=true + reranker_type='llm_api'     : ê³ ê¸‰ ê²€ìƒ‰ (LLM)")
    print("   use_reranker=true + reranker_type='sentence_transformer': ê³ ê¸‰ ê²€ìƒ‰ (ë²¡í„°)")
    print("   use_reranker=false                              : í–¥ìƒëœ ê²€ìƒ‰")
    print("=" * 90)
    print("ğŸ”§ FastAPI ì—”ë“œí¬ì¸íŠ¸:")
    print("   - GET  /api/health              : ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸")
    print("   - GET  /api/categories          : ì¹´í…Œê³ ë¦¬ ì¡°íšŒ (ë™ì  + fallback)")
    print("   - GET  /api/categories_dynamic  : ë™ì  ì¹´í…Œê³ ë¦¬ ì¡°íšŒ (ChromaDB ì§ì ‘)")
    print("   - POST /api/refresh_categories  : ì¹´í…Œê³ ë¦¬ ìºì‹œ ìƒˆë¡œê³ ì¹¨")
    print("   - POST /api/search              : ê³ í’ˆì§ˆ ê²€ìƒ‰")
    print("   - POST /api/chat                : ë‹¤ì¤‘ ê²€ìƒ‰ ë°©ë²• ìƒë‹´")
    print("   - POST /api/chat_stream         : ë‹¤ì¤‘ ê²€ìƒ‰ ë°©ë²• ì‹¤ì‹œê°„ ìƒë‹´ âš¡")
    print("   - POST /api/rebuild_index       : ì¸ë±ìŠ¤ ì¬êµ¬ì¶•")
    print("   - GET  /api/stats               : ì„±ëŠ¥ ì§€í‘œ í¬í•¨ í†µê³„")
    print("   - POST /api/test/simple_chat    : ğŸ§ª ë‹¤ì¤‘ ê²€ìƒ‰ ë°©ë²• í…ŒìŠ¤íŠ¸")
    print("   - GET  /api/test/stream_simple  : ğŸ§ª ë‹¤ì¤‘ ê²€ìƒ‰ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸")
    print("   - GET  /api/test/search_demo    : ğŸ¯ ê²€ìƒ‰ ì„±ëŠ¥ ë°ëª¨ (ì ìˆ˜ ë¶„ì„)")
    print("   - GET  /docs                    : ğŸ¯ Swagger UI ë¬¸ì„œ (API í…ŒìŠ¤íŠ¸) ğŸ¯")
    print("=" * 90)
    print("ğŸ“– API ë¬¸ì„œ ë° í…ŒìŠ¤íŠ¸:")
    print("   http://localhost:5000/docs  â† ğŸ¯ ê³ í’ˆì§ˆ ê²€ìƒ‰ API í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”!")
    print("=" * 90)
    print("ğŸ§ª ë‹¤ì¤‘ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ë°©ë²•:")
    print("   1. ê³ ê¸‰ ê²€ìƒ‰ (ê¸°ë³¸, ë§¤ìš° ë†’ì€ ì •í™•ë„):")
    print("      {\"query\": \"íœ´ê°€ì‹ ì²­\", \"use_reranker\": true, \"reranker_type\": \"qwen3\"}")
    print("   2. ê³ ê¸‰ ê²€ìƒ‰ (LLM ë°©ì‹):")
    print("      {\"query\": \"íœ´ê°€ì‹ ì²­\", \"use_reranker\": true, \"reranker_type\": \"llm_api\"}")
    print("   3. ê³ ê¸‰ ê²€ìƒ‰ (ë²¡í„° ë°©ì‹):")
    print("      {\"query\": \"íœ´ê°€ì‹ ì²­\", \"use_reranker\": true, \"reranker_type\": \"sentence_transformer\"}")
    print("   4. í–¥ìƒëœ ê²€ìƒ‰ (ê°€ì¥ ë¹ ë¦„):")
    print("      {\"query\": \"íœ´ê°€ì‹ ì²­\", \"use_reranker\": false}")
    print("=" * 90)
    print("ğŸ“ ë‹¤ì¤‘ ê²€ìƒ‰ ì˜ˆì œ:")
    print("   # ê³ ê¸‰ ê²€ìƒ‰ (ê¸°ë³¸)")
    print("   curl -X POST 'http://localhost:5000/api/test/simple_chat' \\")
    print("        -H 'Content-Type: application/json' \\")
    print("        -d '{\"query\": \"íœ´ê°€ì‹ ì²­\", \"reranker_type\": \"qwen3\"}'")
    print("   # í–¥ìƒëœ ê²€ìƒ‰")
    print("   curl -X POST 'http://localhost:5000/api/test/simple_chat' \\")
    print("        -H 'Content-Type: application/json' \\")
    print("        -d '{\"query\": \"íœ´ê°€ì‹ ì²­\", \"use_reranker\": false}'")
    print("=" * 90)
    print("ğŸ¯ ê²€ìƒ‰ ë°©ë²• ì„ íƒ ê°€ì´ë“œ:")
    print("   ğŸ§  ê³ ê¸‰ ê²€ìƒ‰      : ìµœê³  ì •í™•ë„, ë‹¤ì¤‘ ê´€ì  ë¶„ì„, ì•ˆì •ì  ì„±ëŠ¥ (ê¸°ë³¸)")
    print("   ğŸ“Š í–¥ìƒëœ ê²€ìƒ‰    : ë¹ ë¥¸ ì†ë„, í‚¤ì›Œë“œ ë§¤ì¹­, ê°„ë‹¨í•œ ì§ˆë¬¸ì— ì í•©")
    print("=" * 90)
    print("ğŸ” ì²˜ë¦¬ ê³¼ì •:")
    print("   ê³ ê¸‰ ê²€ìƒ‰: 4x í›„ë³´ìˆ˜ì§‘ â†’ ì˜ë¯¸ë¶„ì„ â†’ 6ê°€ì§€ì ìˆ˜ìœµí•© â†’ ìƒìœ„ì„ ë³„")
    print("   í–¥ìƒëœ ê²€ìƒ‰: 3x í›„ë³´ìˆ˜ì§‘ â†’ í‚¤ì›Œë“œë§¤ì¹­ â†’ ì ìˆ˜ì¡°ì • â†’ ìƒìœ„ì„ ë³„")
    print("=" * 90)
    print("ğŸ”„ ë™ì  ì¹´í…Œê³ ë¦¬ ì›Œí¬í”Œë¡œìš°:")
    print("   1. í¬íŠ¸ 5001ì—ì„œ ìƒˆ ë‚´ê·œ ë²¡í„° DB ì¶”ê°€")
    print("   2. í¬íŠ¸ 5000ì˜ /api/categories ìë™ìœ¼ë¡œ ìƒˆ ë°ì´í„° ì¸ì‹")
    print("   3. ì¬ì‹œì‘ ì—†ì´ ì¦‰ì‹œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì— ë°˜ì˜")
    print("   4. ChromaDB ìš°ì„ , íŒŒì¼ ê¸°ë°˜ fallbackìœ¼ë¡œ ì•ˆì •ì„± ë³´ì¥")
    print("=" * 90)
    print("âš ï¸ ì¤‘ìš” ì‚¬í•­:")
    print("   â€¢ ê³ ê¸‰ ê²€ìƒ‰: ë‹¤ì¤‘ ì ìˆ˜ ìœµí•©ìœ¼ë¡œ ìµœê³  ì •í™•ë„")
    print("   â€¢ í–¥ìƒëœ ê²€ìƒ‰: í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ìœ¼ë¡œ ë¹ ë¥¸ ì†ë„")
    print("   â€¢ ë™ì  ì¹´í…Œê³ ë¦¬: ìƒˆ ë°ì´í„° ì¶”ê°€ ì‹œ ì¦‰ì‹œ ë°˜ì˜")
    print("   â€¢ ì‚¬ìš©ìì—ê²ŒëŠ” ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ ìˆ¨ê¹€ ì²˜ë¦¬")
    print("=" * 90)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    initialize_system() 
    
    logger.info("ğŸ¯ ê³ í’ˆì§ˆ ê²€ìƒ‰ FastAPI ì„œë²„ ì‹œì‘ ì¤‘... (ë™ì  ì¹´í…Œê³ ë¦¬ ì§€ì›)")
    
    # FastAPI ì•± ì‹¤í–‰
    uvicorn.run(app, host='0.0.0.0', port=5000, log_level="info")