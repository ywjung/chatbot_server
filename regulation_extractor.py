import json
import os
import glob
import logging
import requests
import uuid
import re
import urllib.parse
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import fitz  # PyMuPDF for PDF processing
from datetime import datetime
import pytz

# ë²¡í„° DB ê´€ë ¨ imports
import chromadb
from sentence_transformers import SentenceTransformer
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# FastAPI ì•± ì„¤ì •
app = FastAPI(
    title="íšŒì‚¬ ë‚´ê·œ PDF JSON ì¶”ì¶œ + ë²¡í„° DB í†µí•© ì‹œìŠ¤í…œ",
    description="""
    **íšŒì‚¬ ë‚´ê·œ PDFì—ì„œ êµ¬ì¡°í™”ëœ JSONì„ ì¶”ì¶œí•˜ê³  ë²¡í„° DBì— ì €ì¥í•˜ëŠ” í†µí•© AI ì‹œìŠ¤í…œ**
    
    ## ì£¼ìš” ê¸°ëŠ¥
    - ğŸ“„ **PDF ì—…ë¡œë“œ ë° íŒŒì‹±**: ë³µì¡í•œ ë‚´ê·œ ë¬¸ì„œ ìë™ ì²˜ë¦¬
    - ğŸ§  **AI ê¸°ë°˜ êµ¬ì¡° ë¶„ì„**: LLMì„ í™œìš©í•œ ì •êµí•œ ë‚´ìš© ë¶„ì„
    - ğŸ“Š **í‘œì¤€ JSON êµ¬ì¡°**: ê¸°ì¡´ ë‚´ê·œ í˜•ì‹ê³¼ ì¼ì¹˜í•˜ëŠ” êµ¬ì¡°í™”ëœ ì¶œë ¥
    - ğŸ¯ **ì§ˆë¬¸-ë‹µë³€ ìë™ ìƒì„±**: ëª¨ë“  ë‚´ê·œ ì¡°í•­ì„ ì§ˆë¬¸-ë‹µë³€ í˜•íƒœë¡œ ë³€í™˜
    - ğŸ“ **ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜**: ë‚´ìš© ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì¹´í…Œê³ ë¦¬ êµ¬ë¶„
    - ğŸ’¾ **íŒŒì¼ ê´€ë¦¬**: JSON ì €ì¥, ëª©ë¡ ì¡°íšŒ, ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
    - ğŸ” **ë²¡í„° DB ì €ì¥**: RAG ì„œë²„ì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ChromaDBì— ì €ì¥
    - ğŸ¯ **ë²¡í„° ê²€ìƒ‰**: ê³ ì„±ëŠ¥ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì§€ì›
    
    ## AI ë¶„ì„ íŠ¹ì§•
    - **í¬ê´„ì  ì§ˆë¬¸ ìƒì„±**: ì¡°í•­ë³„, ì ˆì°¨ë³„, ì˜ˆì™¸ì‚¬í•­ë³„ ë‹¤ê°ë„ ì§ˆë¬¸ ìƒì„±
    - **ì •í™•í•œ ë‹µë³€ ì¶”ì¶œ**: ì›ë¬¸ ê¸°ë°˜ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ ì œê³µ
    - **êµ¬ì¡°ì  ë¶„ì„**: ê³„ì¸µì  ë¬¸ì„œ êµ¬ì¡° ì¸ì‹ ë° ë°˜ì˜
    - **ë§¥ë½ ì¸ì‹**: ì—°ê´€ ì¡°í•­ ê°„ì˜ ê´€ê³„ íŒŒì•… ë° í†µí•©
    
    ## ë²¡í„° DB ê¸°ëŠ¥ (RAG ì„œë²„ í˜¸í™˜)
    - **ChromaDB í†µí•©**: RAG ì„œë²„ì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥
    - **ì²­í‚¹ ì „ëµ**: qa_full, question_focused, answer_focused, sentence_level
    - **ì„ë² ë”© ìƒì„±**: SentenceTransformer ê¸°ë°˜ í•œêµ­ì–´ ì„ë² ë”©
    - **ë©”íƒ€ë°ì´í„° ê´€ë¦¬**: RAG ì„œë²„ì™€ ì™„ì „ í˜¸í™˜ë˜ëŠ” ë©”íƒ€ë°ì´í„° êµ¬ì¡°
    
    ## API ì‚¬ìš©ë²•
    1. `/api/extract_regulation_json`: PDF ì—…ë¡œë“œ â†’ JSON ì¶”ì¶œ
    2. `/api/extract_regulation_json_v2`: PDF ì—…ë¡œë“œ â†’ JSON ì¶”ì¶œ (ë²¡í„° DB ì €ì¥ìš©)
    3. `/api/save_extracted_json_v2`: JSON Bodyë¡œ ì €ì¥ (ê¶Œì¥)
    4. `/api/store_json_to_vector`: ì €ì¥ëœ JSONì„ ë²¡í„° DBì— ì €ì¥ (RAG ì„œë²„ í˜¸í™˜)
    5. `/api/store_extracted_json_to_vector`: ì¶”ì¶œëœ JSONì„ ì§ì ‘ ë²¡í„° DBì— ì €ì¥ (RAG ì„œë²„ í˜¸í™˜)
    6. `/api/vector_search`: ë²¡í„° DBì—ì„œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
    7. `/api/vector_stats`: ë²¡í„° DB í†µê³„ ì •ë³´ ì¡°íšŒ
    """,
    version="3.1.0-rag-compatible",
    contact={
        "name": "ë‚´ê·œ JSON ì¶”ì¶œ + ë²¡í„° DB í†µí•© ì‹œìŠ¤í…œ (RAG ì„œë²„ í˜¸í™˜)",
        "email": "dev@company.com"
    }
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic ëª¨ë¸ë“¤
class RegulationFAQ(BaseModel):
    question: str = Field(..., description="ë‚´ê·œ ê´€ë ¨ ì§ˆë¬¸")
    answer: str = Field(..., description="ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€")

class RegulationCategory(BaseModel):
    category: str = Field(..., description="ì¹´í…Œê³ ë¦¬ëª… (í•œê¸€ + ì˜ë¬¸)")
    faqs: List[RegulationFAQ] = Field(..., description="ì§ˆë¬¸-ë‹µë³€ ëª©ë¡")

class PDFExtractionRequest(BaseModel):
    regulation_type: Optional[str] = Field("ì¼ë°˜ë‚´ê·œ", description="ë‚´ê·œ ìœ í˜• (ì˜ˆ: ì§ì œê·œì •, ì¸ì‚¬ê·œì •, ê¸‰ì—¬ê·œì • ë“±)")
    extract_mode: Optional[str] = Field("comprehensive", description="ì¶”ì¶œ ëª¨ë“œ (comprehensive: í¬ê´„ì , focused: í•µì‹¬ë§Œ)")

class PDFExtractionResponse(BaseModel):
    success: bool
    regulation_type: str
    categories_count: int
    total_faqs: int
    extraction_info: Dict[str, Any]
    regulations_data: List[RegulationCategory]
    processing_time: float

class PDFExtractionV2Response(BaseModel):
    success: bool
    regulation_data: List[RegulationCategory] = Field(..., description="ì¶”ì¶œëœ ë‚´ê·œ ë°ì´í„° (ë²¡í„° DB ì €ì¥ìš©)")
    main_category_name: str = Field(..., description="ëŒ€ë¶„ë¥˜ëª…")
    extraction_summary: Dict[str, Any] = Field(..., description="ì¶”ì¶œ ìš”ì•½ ì •ë³´")

class SaveResponse(BaseModel):
    success: bool
    message: str
    file_path: str
    categories_count: int
    total_faqs: int
    saved_at: str
    file_size_kb: Optional[float] = None

class SaveRequestV2(BaseModel):
    regulation_data: List[RegulationCategory]
    filename: str = Field(..., description="ì €ì¥í•  íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)")

class FileInfo(BaseModel):
    filename: str
    file_path: str
    size_kb: float
    created_at: str
    modified_at: str

class SavedFilesResponse(BaseModel):
    files: List[FileInfo]
    total_count: int
    total_size_kb: float

# ë²¡í„° DB ê´€ë ¨ ëª¨ë¸ë“¤
class VectorStoreRequest(BaseModel):
    filename: str = Field(..., description="ë²¡í„° DBì— ì €ì¥í•  JSON íŒŒì¼ëª…")
    main_category_name: Optional[str] = Field(None, description="ëŒ€ë¶„ë¥˜ëª… (íŒŒì¼ëª… ëŒ€ì‹  ì‚¬ìš©í•  ê²½ìš°)")

class VectorStoreDirectRequest(BaseModel):
    regulation_data: List[RegulationCategory]
    main_category_name: str = Field(..., description="ëŒ€ë¶„ë¥˜ëª…")

class VectorSearchRequest(BaseModel):
    query: str = Field(..., description="ê²€ìƒ‰í•  ì§ˆì˜", example="íœ´ê°€ ì‹ ì²­ ë°©ë²•")
    top_k: int = Field(10, description="ë°˜í™˜í•  ê²°ê³¼ ìˆ˜", example=10, ge=1, le=50)
    main_category_filter: Optional[str] = Field(None, description="ëŒ€ë¶„ë¥˜ í•„í„°", example="ì¸ì‚¬")

class VectorSearchResult(BaseModel):
    main_category: str
    sub_category: str
    question: str
    answer: str
    source_file: str
    chunk_type: str
    score: float
    rank: int

class VectorSearchResponse(BaseModel):
    query: str
    results: List[VectorSearchResult]
    count: int
    main_category_filter: Optional[str]
    search_type: str

class VectorStoreResponse(BaseModel):
    success: bool
    message: str
    main_category: str
    total_chunks: int
    chunk_statistics: Dict[str, int]
    vector_db_stats: Dict[str, Any]
    stored_at: str

class VectorStatsResponse(BaseModel):
    total_documents: int
    collection_name: str
    persist_directory: str
    chroma_db_path: str
    chroma_db_size_mb: float
    model_name: str
    main_categories: Dict[str, Any]
    total_main_categories: int
    chunk_statistics: Dict[str, int]
    vector_db_ready: bool

class LLMClient:
    def __init__(self, api_url: str = "http://localhost:1234/v1/chat/completions"):
        self.api_url = api_url
        logger.info(f"LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: {api_url}")
    
    def create_regulation_extraction_prompt(self, text_content: str, regulation_type: str, extract_mode: str) -> str:
        """ë‚´ê·œ JSON ì¶”ì¶œìš© ì •êµí•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        current_date = datetime.now(pytz.timezone('Asia/Seoul')).strftime("%Yë…„ %mì›” %dì¼")
        
        system_prompt = f"""ë‹¹ì‹ ì€ íšŒì‚¬ ë‚´ê·œ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•œ JSON êµ¬ì¡°ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ AIì…ë‹ˆë‹¤.

ğŸ¯ **í•µì‹¬ ì„ë¬´**: ì œê³µëœ ë‚´ê·œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸-ë‹µë³€ í˜•íƒœì˜ êµ¬ì¡°í™”ëœ JSONì„ ìƒì„±í•˜ì„¸ìš”.

ğŸ“‹ **ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì¤€ìˆ˜)**:
```json
[
  {{
    "category": "ì¹´í…Œê³ ë¦¬ëª… (í•œê¸€ëª…ì¹­ + ì˜ë¬¸ëª…ì¹­)",
    "faqs": [
      {{
        "question": "êµ¬ì²´ì ì¸ ì§ˆë¬¸",
        "answer": "ëª…í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ (ì¡°í•­ ë²ˆí˜¸ í¬í•¨)"
      }}
    ]
  }}
]
```

ğŸ” **ë¶„ì„ ëŒ€ìƒ**: {regulation_type} ({extract_mode} ëª¨ë“œ)
ğŸ“… **ë¶„ì„ ì¼ì**: {current_date}

ğŸ“š **ì§ˆë¬¸ ìƒì„± ì „ëµ (ëª¨ë“  ê²½ìš° ëŒ€ì‘)**:

1. **ê¸°ë³¸ ì •ë³´ ì§ˆë¬¸**:
   - "ì´ ê·œì •ì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
   - "ì ìš© ë²”ìœ„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
   - "ìš©ì–´ì˜ ì •ì˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"

2. **ì ˆì°¨ ë° ë°©ë²• ì§ˆë¬¸**:
   - "~ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?" / "~í•˜ëŠ” ë°©ë²•ì€?"
   - "~ì‹ ì²­ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
   - "~ì²˜ë¦¬ ê¸°ê°„ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?"

3. **ìê²© ë° ìš”ê±´ ì§ˆë¬¸**:
   - "~í•  ìˆ˜ ìˆëŠ” ìê²©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
   - "~ì˜ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?"
   - "ëˆ„ê°€ ~í•  ìˆ˜ ìˆë‚˜ìš”?"

4. **ê¸°ì¤€ ë° ì¡°ê±´ ì§ˆë¬¸**:
   - "~ì˜ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
   - "ì–´ë–¤ ê²½ìš°ì— ~ê°€ ê°€ëŠ¥í•œê°€ìš”?"
   - "~ì˜ ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?"

5. **ê¶Œí•œ ë° ì±…ì„ ì§ˆë¬¸**:
   - "ëˆ„ê°€ ~ì„ ë‹´ë‹¹í•˜ë‚˜ìš”?"
   - "~ì˜ ê¶Œí•œì€ ëˆ„êµ¬ì—ê²Œ ìˆë‚˜ìš”?"
   - "~ì— ëŒ€í•œ ì±…ì„ì€ ëˆ„ê°€ ì§€ë‚˜ìš”?"

6. **ê¸°ê°„ ë° ì‹œê¸° ì§ˆë¬¸**:
   - "~ì€ ì–¸ì œ í•˜ë‚˜ìš”?"
   - "~ì˜ ê¸°ê°„ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?"
   - "~ì€ ëª‡ ë…„ì°¨ë¶€í„° ê°€ëŠ¥í•œê°€ìš”?"

7. **ì˜ˆì™¸ ë° íŠ¹ë³„ ìƒí™© ì§ˆë¬¸**:
   - "ì–´ë–¤ ê²½ìš°ì— ì˜ˆì™¸ê°€ ì¸ì •ë˜ë‚˜ìš”?"
   - "íŠ¹ë³„í•œ ìƒí™©ì—ì„œëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
   - "~ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ëŠ” ì–¸ì œì¸ê°€ìš”?"

8. **ê³„ì‚° ë° ìˆ˜ì¹˜ ì§ˆë¬¸**:
   - "~ì€ ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?"
   - "~ì˜ ê¸ˆì•¡/ì¼ìˆ˜ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
   - "~ì˜ ë¹„ìœ¨ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"

9. **ë³€ê²½ ë° ìˆ˜ì • ì§ˆë¬¸**:
   - "~ì„ ë³€ê²½í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
   - "~ì„ ìˆ˜ì •í•  ìˆ˜ ìˆë‚˜ìš”?"
   - "~ì˜ ê°œì • ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"

10. **ìœ„ë°˜ ë° ì œì¬ ì§ˆë¬¸**:
    - "~ì„ ìœ„ë°˜í•˜ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
    - "ì œì¬ ì¡°ì¹˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    - "ë²Œì¹™ ê·œì •ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"

ğŸ¯ **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ê°€ì´ë“œ**:
- **ì´ì¹™ (General Provisions)**: ëª©ì , ì ìš©ë²”ìœ„, ìš©ì–´ì •ì˜
- **ì¸ì‚¬ (Personnel)**: ì±„ìš©, ìŠ¹ì§„, ì „ë³´, í‡´ì§
- **ê¸‰ì—¬ ë° ìˆ˜ë‹¹ (Salary and Allowances)**: ë´‰ê¸‰, ìˆ˜ë‹¹, ìƒì—¬ê¸ˆ
- **ê·¼ë¬´ (Work)**: ê·¼ë¬´ì‹œê°„, íœ´ë¬´, ì¶œì¥
- **íœ´ê°€ (Leave)**: ì—°ì°¨, ë³‘ê°€, íŠ¹ë³„íœ´ê°€
- **ë³µë¦¬í›„ìƒ (Welfare)**: ê°ì¢… ë³µë¦¬í›„ìƒ ì œë„
- **êµìœ¡ ë° ì—°ìˆ˜ (Education and Training)**: êµìœ¡, ì—°ìˆ˜, ìê²©ì¦
- **í‰ê°€ ë° í¬ìƒ (Evaluation and Rewards)**: ì„±ê³¼í‰ê°€, í¬ìƒ, ì§•ê³„
- **ì•ˆì „ ë° ë³´ê±´ (Safety and Health)**: ì•ˆì „ê´€ë¦¬, ë³´ê±´
- **ì‹œí–‰ ë° ë¶€ì¹™ (Enforcement and Supplementary)**: ì‹œí–‰ì¼, ê²½ê³¼ì¡°ì¹˜

âš¡ **í’ˆì§ˆ ê¸°ì¤€**:
- **ì •í™•ì„±**: ì›ë¬¸ ë‚´ìš©ì„ ì •í™•íˆ ë°˜ì˜
- **ì™„ì „ì„±**: ëª¨ë“  ì¤‘ìš” ì¡°í•­ì„ ë¹ ì§ì—†ì´ í¬í•¨
- **ëª…í™•ì„±**: ì´í•´í•˜ê¸° ì‰¬ìš´ ì§ˆë¬¸ê³¼ ë‹µë³€
- **êµ¬ì²´ì„±**: êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì§ˆë¬¸ ìƒì„±
- **ì¼ê´€ì„±**: ì¼ê´€ëœ í˜•ì‹ê³¼ ìŠ¤íƒ€ì¼ ìœ ì§€

ğŸš¨ **í•„ìˆ˜ ì¤€ìˆ˜ì‚¬í•­**:
1. **ì›ë¬¸ ì¶©ì‹¤ì„±**: ì›ë¬¸ì˜ ì¡°í•­ ë²ˆí˜¸ì™€ ë‚´ìš©ì„ ì •í™•íˆ ì¸ìš©
2. **JSON í˜•ì‹**: ì˜¬ë°”ë¥¸ JSON êµ¬ì¡°ë§Œ ì¶œë ¥ (ì„¤ëª… í…ìŠ¤íŠ¸ ê¸ˆì§€)
3. **í•œêµ­ì–´ ì‚¬ìš©**: ëª¨ë“  ì§ˆë¬¸ê³¼ ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±
4. **ì‹¤ìš©ì  ì§ˆë¬¸**: ì‹¤ì œ ì§ì›ë“¤ì´ ê¶ê¸ˆí•´í•  ë§Œí•œ ì‹¤ìš©ì  ì§ˆë¬¸ ìƒì„±
5. **í¬ê´„ì  ì»¤ë²„ë¦¬ì§€**: ë¬¸ì„œì˜ ëª¨ë“  ì¤‘ìš” ë‚´ìš©ì„ ì§ˆë¬¸-ë‹µë³€ìœ¼ë¡œ ë³€í™˜
5. **ë¹ ì§€ì§€ ì•Šì€ ë‹µë³€**: ì§ˆë¬¸-ë‹µë³€ ë³€í™˜ì‹œ ë¹ ì§ì—†ì´ ëª¨ë“  ì¡°í•­ì„ í¬í•¨

ğŸ’¡ **ì˜ˆì‹œ ì§ˆë¬¸ íŒ¨í„´**:
- "ì œ1ì¡°ì—ì„œ ê·œì •í•˜ëŠ” ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
- "21ë…„ì°¨ ì§ì›ì˜ ì—°ì°¨ ì¼ìˆ˜ëŠ” ë©°ì¹ ì¸ê°€ìš”?"
- "íœ´ê°€ ì‹ ì²­ì€ ì–¸ì œê¹Œì§€ í•´ì•¼ í•˜ë‚˜ìš”?"
- "ìœ¡ì•„íœ´ì§ ëŒ€ìƒìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"
- "ì„±ê³¼ê¸‰ ì§€ê¸‰ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"

ì´ì œ ì œê³µëœ ë‚´ê·œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì™„ë²½í•œ JSONì„ ìƒì„±í•˜ì„¸ìš”."""

        return system_prompt
    
    def extract_json_from_regulation(self, text_content: str, regulation_type: str = "ì¼ë°˜ë‚´ê·œ", 
                                   extract_mode: str = "comprehensive") -> List[Dict[str, Any]]:
        """ë‚´ê·œ í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ"""
        logger.info(f"ë‚´ê·œ JSON ì¶”ì¶œ ì‹œì‘: {regulation_type} ({extract_mode} ëª¨ë“œ)")
        
        try:
            system_prompt = self.create_regulation_extraction_prompt(text_content, regulation_type, extract_mode)
            
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            max_chunk_size = 8000  # í† í° ì œí•œ ê³ ë ¤
            
            if len(text_content) <= max_chunk_size:
                # ë‹¨ì¼ ì²˜ë¦¬
                result = self._process_text_chunk(text_content, system_prompt)
            else:
                # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ í›„ ë³‘í•©
                result = self._process_text_in_chunks(text_content, system_prompt, max_chunk_size)
            
            if result:
                logger.info(f"âœ… JSON ì¶”ì¶œ ì™„ë£Œ: {len(result)}ê°œ ì¹´í…Œê³ ë¦¬")
                return result
            else:
                logger.error("âŒ JSON ì¶”ì¶œ ì‹¤íŒ¨: ë¹ˆ ê²°ê³¼")
                return []
                
        except Exception as e:
            logger.error(f"âŒ JSON ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return []
    
    def _process_text_chunk(self, text_chunk: str, system_prompt: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ì²­í¬ ì²˜ë¦¬"""
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ë‹¤ìŒ ë‚´ê·œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ JSONì„ ìƒì„±í•˜ì„¸ìš”:\n\n{text_chunk}"}
            ]
            
            response = requests.post(
                self.api_url,
                json={
                    "model": "qwen3-30b-a3b-mlx",
                    "messages": messages,
                    "temperature": 0.1,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                    "max_tokens": 40000,
                    "stream": False
                },
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()
                
                # JSON ì¶”ì¶œ ë° íŒŒì‹±
                extracted_json = self._extract_and_parse_json(content)
                return extracted_json
            else:
                logger.error(f"âŒ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def _process_text_in_chunks(self, text_content: str, system_prompt: str, max_chunk_size: int) -> List[Dict[str, Any]]:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"""
        try:
            # ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
            paragraphs = text_content.split('\n\n')
            chunks = []
            current_chunk = ""
            
            for paragraph in paragraphs:
                if len(current_chunk + paragraph) <= max_chunk_size:
                    current_chunk += paragraph + "\n\n"
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = paragraph + "\n\n"
            
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            logger.info(f"ğŸ“„ í…ìŠ¤íŠ¸ë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
            
            # ê° ì²­í¬ ì²˜ë¦¬
            all_results = []
            for i, chunk in enumerate(chunks, 1):
                logger.info(f"ğŸ” ì²­í¬ {i}/{len(chunks)} ì²˜ë¦¬ ì¤‘...")
                chunk_result = self._process_text_chunk(chunk, system_prompt)
                if chunk_result:
                    all_results.extend(chunk_result)
            
            # ì¤‘ë³µ ì œê±° ë° ë³‘í•©
            merged_results = self._merge_categories(all_results)
            logger.info(f"âœ… ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ: {len(merged_results)}ê°œ ì¹´í…Œê³ ë¦¬")
            
            return merged_results
            
        except Exception as e:
            logger.error(f"âŒ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_and_parse_json(self, content: str) -> List[Dict[str, Any]]:
        """ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±"""
        try:
            # JSON ë¸”ë¡ ì°¾ê¸°
            json_patterns = [
                r'```json\s*(\[.*?\])\s*```',
                r'```\s*(\[.*?\])\s*```',
                r'(\[.*?\])',
                r'```json\s*(\{.*?\})\s*```',
                r'```\s*(\{.*?\})\s*```',
                r'(\{.*?\})'
            ]
            
            extracted_json = None
            
            for pattern in json_patterns:
                matches = re.findall(pattern, content, re.DOTALL)
                if matches:
                    json_str = matches[0]
                    try:
                        extracted_json = json.loads(json_str)
                        break
                    except json.JSONDecodeError:
                        continue
            
            if extracted_json is None:
                # ì§ì ‘ JSON íŒŒì‹± ì‹œë„
                try:
                    extracted_json = json.loads(content)
                except json.JSONDecodeError:
                    logger.error("âŒ JSON íŒŒì‹± ì‹¤íŒ¨")
                    return []
            
            # í˜•ì‹ ê²€ì¦ ë° ì •ê·œí™”
            if isinstance(extracted_json, dict) and 'category' in extracted_json:
                extracted_json = [extracted_json]
            
            if isinstance(extracted_json, list):
                validated_data = []
                for item in extracted_json:
                    if isinstance(item, dict) and 'category' in item and 'faqs' in item:
                        validated_data.append(item)
                
                return validated_data
            
            return []
            
        except Exception as e:
            logger.error(f"âŒ JSON ì¶”ì¶œ ë° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _merge_categories(self, all_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ ì¤‘ë³µ ì œê±° ë° ë³‘í•©"""
        try:
            category_map = {}
            
            for item in all_results:
                category_name = item.get('category', '')
                if category_name in category_map:
                    # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ì— FAQ ì¶”ê°€
                    existing_questions = {faq['question'] for faq in category_map[category_name]['faqs']}
                    for faq in item.get('faqs', []):
                        if faq['question'] not in existing_questions:
                            category_map[category_name]['faqs'].append(faq)
                            existing_questions.add(faq['question'])
                else:
                    category_map[category_name] = item
            
            return list(category_map.values())
            
        except Exception as e:
            logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ë³‘í•© ì‹¤íŒ¨: {e}")
            return all_results

class PDFProcessor:
    """PDF ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def extract_text_from_pdf(pdf_content: bytes) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            doc = fitz.open(stream=pdf_content, filetype="pdf")
            full_text = ""
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text = page.get_text()
                full_text += f"\n\n=== í˜ì´ì§€ {page_num + 1} ===\n{text}"
            
            doc.close()
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            cleaned_text = PDFProcessor._clean_extracted_text(full_text)
            logger.info(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(cleaned_text)}ì")
            
            return cleaned_text
            
        except Exception as e:
            logger.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=400, detail=f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    @staticmethod
    def _clean_extracted_text(text: str) -> str:
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        try:
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = re.sub(r' {2,}', ' ', text)
            
            # í˜ì´ì§€ êµ¬ë¶„ì ì •ë¦¬
            text = re.sub(r'=== í˜ì´ì§€ \d+ ===\n', '', text)
            
            # ë¹ˆ ì¤„ ì •ë¦¬
            text = text.strip()
            
            return text
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return text

class VectorDBSystem:
    """ë²¡í„° DB ì‹œìŠ¤í…œ í´ë˜ìŠ¤ (RAG ì„œë²„ í˜¸í™˜)"""
    
    def __init__(self, model_name: str = "nlpai-lab/KURE-v1", persist_directory: str = "./chroma_db"):
        """ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™” (RAG ì„œë²„ì™€ ë™ì¼í•œ ë°©ì‹)"""
        self.model_name = model_name
        self.persist_directory = persist_directory
        
        # ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.persist_directory, exist_ok=True)
        logger.info(f"ğŸ“ ChromaDB ì €ì¥ ê²½ë¡œ: {os.path.abspath(self.persist_directory)}")
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (RAG ì„œë²„ì™€ ë™ì¼í•œ ë°©ì‹)
        model_path = os.path.join("./models", model_name.replace("/", "-"))
        
        # ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(model_path):
            logger.info(f"ğŸ“¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: '{model_name}'")
            try:
                os.makedirs("./models", exist_ok=True)
                model = SentenceTransformer(model_name)
                model.save(model_path)
                logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
            except Exception as e:
                logger.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
                raise
        else:
            logger.info(f"ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {model_path}")

        self.model = SentenceTransformer(model_path)
        
        # ChromaDB ì´ˆê¸°í™” (RAG ì„œë²„ì™€ ë™ì¼í•œ ë°©ì‹)
        logger.info(f"ğŸ” ChromaDB ì´ˆê¸°í™” ì¤‘: {self.persist_directory}")
        self.chroma_client = chromadb.PersistentClient(path=self.persist_directory)
        try:
            self.collection = self.chroma_client.get_collection(name="company_regulations")
            logger.info(f"ğŸ“ ê¸°ì¡´ company_regulations ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ (ê²½ë¡œ: {self.persist_directory})")
        except Exception as get_error:
            logger.info(f"ğŸ“ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì¤‘... (ê²½ë¡œ: {self.persist_directory})")
            try:
                self.collection = self.chroma_client.create_collection(
                    name="company_regulations",
                    metadata={
                        "description": "íšŒì‚¬ ë‚´ê·œ ë²¡í„° ê²€ìƒ‰ ì»¬ë ‰ì…˜ (RAG ì„œë²„ í˜¸í™˜)",
                        "persist_directory": self.persist_directory,
                        "created_at": datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                    }
                )
                logger.info(f"ğŸ“ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ (ê²½ë¡œ: {self.persist_directory})")
            except Exception as create_error:
                import time
                temp_name = f"company_regulations_{int(time.time())}"
                self.collection = self.chroma_client.create_collection(
                    name=temp_name,
                    metadata={
                        "description": "ì„ì‹œ ì»¬ë ‰ì…˜",
                        "persist_directory": self.persist_directory,
                        "created_at": datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                    }
                )
                logger.info(f"ğŸ“ ì„ì‹œ ì»¬ë ‰ì…˜ '{temp_name}' ìƒì„± ì™„ë£Œ (ê²½ë¡œ: {self.persist_directory})")

        logger.info(f"âœ… ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (ChromaDB: {self.persist_directory}, RAG ì„œë²„ í˜¸í™˜)")
    
    def store_regulation_data(self, regulation_data: List[Dict[str, Any]], main_category_name: str) -> Dict[str, Any]:
        """ë‚´ê·œ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ì €ì¥ (RAG ì„œë²„ì™€ ì™„ì „ ë™ì¼í•œ ë°©ì‹)"""
        logger.info(f"ğŸ” ë²¡í„° DB ì €ì¥ ì‹œì‘ (RAG í˜¸í™˜): {main_category_name} (ChromaDB: {self.persist_directory})")
        
        try:
            # RAG ì„œë²„ì˜ regulations_data êµ¬ì¡°ë¡œ ë³€í™˜
            regulations_data = []
            
            for category_section in regulation_data:
                sub_category = category_section['category']
                
                for faq in category_section['faqs']:
                    question = faq['question']
                    answer = faq['answer']
                    
                    # RAG ì„œë²„ì™€ ë™ì¼í•œ ì²­í‚¹ ì „ëµ
                    # 1. ê¸°ë³¸ Q&A ë‹¨ìœ„ ì €ì¥
                    base_id = str(uuid.uuid4())
                    regulation_item = {
                        'id': base_id,
                        'main_category': main_category_name,
                        'sub_category': sub_category,
                        'question': question,
                        'answer': answer,
                        'text': f"{question} {answer}",
                        'source_file': f"{main_category_name}.json",
                        'chunk_type': 'qa_full',
                        'chunk_id': 0
                    }
                    regulations_data.append(regulation_item)
                    
                    # 2. í–¥ìƒëœ ì²­í‚¹ (RAG ì„œë²„ì™€ ë™ì¼)
                    question_item = {
                        'id': f"{base_id}_q",
                        'main_category': main_category_name,
                        'sub_category': sub_category,
                        'question': question,
                        'answer': answer,
                        'text': f"ì§ˆë¬¸: {question}",
                        'source_file': f"{main_category_name}.json",
                        'chunk_type': 'question_focused',
                        'chunk_id': 1
                    }
                    regulations_data.append(question_item)
                    
                    answer_item = {
                        'id': f"{base_id}_a",
                        'main_category': main_category_name,
                        'sub_category': sub_category,
                        'question': question,
                        'answer': answer,
                        'text': f"ë‹µë³€: {answer} (ê´€ë ¨ ì§ˆë¬¸: {question})",
                        'source_file': f"{main_category_name}.json",
                        'chunk_type': 'answer_focused',
                        'chunk_id': 2
                    }
                    regulations_data.append(answer_item)
                    
                    # 3. ê¸´ ë‹µë³€ì˜ ê²½ìš° ë¬¸ì¥ ë¶„í•  (RAG ì„œë²„ì™€ ë™ì¼)
                    if len(answer) > 200:
                        sentences = re.split(r'[.!?]\s+', answer)
                        for i, sentence in enumerate(sentences):
                            if len(sentence.strip()) > 20:
                                sentence_item = {
                                    'id': f"{base_id}_s{i}",
                                    'main_category': main_category_name,
                                    'sub_category': sub_category,
                                    'question': question,
                                    'answer': answer,
                                    'text': f"{sentence.strip()} (ì¶œì²˜: {question})",
                                    'source_file': f"{main_category_name}.json",
                                    'chunk_type': 'sentence_level',
                                    'chunk_id': 10 + i
                                }
                                regulations_data.append(sentence_item)
            
            # ì²­í‚¹ í†µê³„ ê³„ì‚° (RAG ì„œë²„ì™€ ë™ì¼)
            chunk_statistics = {}
            for item in regulations_data:
                chunk_type = item.get('chunk_type', 'unknown')
                chunk_statistics[chunk_type] = chunk_statistics.get(chunk_type, 0) + 1
            
            # ChromaDBì— ì €ì¥ (RAG ì„œë²„ì™€ ë™ì¼í•œ ë°©ì‹)
            logger.info(f"âš™ï¸ ì„ë² ë”© ë° ì €ì¥: {len(regulations_data)}ê°œ ì²­í¬")
            
            texts = [item['text'] for item in regulations_data]
            metadatas = [
                {
                    'main_category': item['main_category'],
                    'sub_category': item['sub_category'],
                    'question': item['question'],
                    'answer': item['answer'],
                    'source_file': item['source_file'],
                    'chunk_type': item.get('chunk_type', 'qa_full'),
                    'chunk_id': item.get('chunk_id', 0)
                } 
                for item in regulations_data
            ]
            ids = [item['id'] for item in regulations_data]
            
            # ë°°ì¹˜ ì²˜ë¦¬ (RAG ì„œë²„ì™€ ë™ì¼)
            batch_size = 500
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                batch_metadatas = metadatas[i:i+batch_size]
                batch_ids = ids[i:i+batch_size]
                
                try:
                    embeddings = self.model.encode(batch_texts, show_progress_bar=False).tolist()
                    
                    self.collection.add(
                        documents=batch_texts,
                        metadatas=batch_metadatas,
                        embeddings=embeddings,
                        ids=batch_ids
                    )
                    
                    logger.info(f"ğŸ“¦ ë°°ì¹˜ {i//batch_size + 1}/{total_batches} ì™„ë£Œ")
                    
                except Exception as batch_error:
                    logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {batch_error}")
                    continue
            
            final_count = self.collection.count()
            logger.info(f"âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ (RAG í˜¸í™˜): {len(regulations_data)}ê°œ ì²­í¬ ì €ì¥ (ChromaDB: {self.persist_directory})")
            
            return {
                "total_chunks": len(regulations_data),
                "chunk_statistics": chunk_statistics,
                "vector_db_total": final_count,
                "batch_count": total_batches
            }
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def search(self, query: str, top_k: int = 10, main_category_filter: str = None) -> List[Dict[str, Any]]:
        """ë²¡í„° DBì—ì„œ ê²€ìƒ‰ (RAG ì„œë²„ì™€ ë™ì¼í•œ ë°©ì‹)"""
        logger.info(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰: '{query[:50]}...', top_k={top_k} (ChromaDB: {self.persist_directory})")
        
        try:
            if self.collection.count() == 0:
                logger.warning("âš ï¸ ë¹ˆ ì¸ë±ìŠ¤")
                return []
            
            # í•„í„° ì¡°ê±´
            where_condition = None
            if main_category_filter:
                where_condition = {"main_category": main_category_filter}
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.model.encode([query]).tolist()
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=top_k,
                include=['documents', 'metadatas', 'distances'],
                where=where_condition
            )
            
            # ê²°ê³¼ ì²˜ë¦¬ (RAG ì„œë²„ì™€ ë™ì¼)
            search_results = []
            if results['ids'] and len(results['ids'][0]) > 0:
                for i in range(len(results['ids'][0])):
                    metadata = results['metadatas'][0][i]
                    distance = results['distances'][0][i]
                    
                    # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜
                    similarity_score = max(0, 1 - (distance / 2))
                    
                    search_results.append({
                        'main_category': metadata['main_category'],
                        'sub_category': metadata['sub_category'],
                        'question': metadata['question'],
                        'answer': metadata['answer'],
                        'source_file': metadata['source_file'],
                        'chunk_type': metadata.get('chunk_type', 'qa_full'),
                        'score': similarity_score,
                        'rank': i + 1
                    })
            
            logger.info(f"âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼ (ChromaDB: {self.persist_directory})")
            return search_results
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """ë²¡í„° DB í†µê³„ ì •ë³´ ë°˜í™˜ (RAG ì„œë²„ì™€ ë™ì¼)"""
        try:
            count = self.collection.count()
            
            # ChromaDB ë””ë ‰í† ë¦¬ ì •ë³´
            chroma_db_path = os.path.abspath(self.persist_directory)
            chroma_db_size = 0
            if os.path.exists(self.persist_directory):
                for root, dirs, files in os.walk(self.persist_directory):
                    for file in files:
                        chroma_db_size += os.path.getsize(os.path.join(root, file))
            
            # ë©”íƒ€ë°ì´í„° ë¶„ì„ì„ ìœ„í•´ ìƒ˜í”Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            if count > 0:
                sample_results = self.collection.get(limit=min(1000, count))
                main_categories = {}
                chunk_statistics = {}
                
                if sample_results and 'metadatas' in sample_results:
                    for metadata in sample_results['metadatas']:
                        main_cat = metadata.get('main_category', 'Unknown')
                        chunk_type = metadata.get('chunk_type', 'unknown')
                        
                        if main_cat not in main_categories:
                            main_categories[main_cat] = {
                                'source_file': metadata.get('source_file', 'Unknown'),
                                'count': 0
                            }
                        main_categories[main_cat]['count'] += 1
                        
                        chunk_statistics[chunk_type] = chunk_statistics.get(chunk_type, 0) + 1
            else:
                main_categories = {}
                chunk_statistics = {}
            
            return {
                'total_documents': count,
                'collection_name': self.collection.name,
                'persist_directory': self.persist_directory,
                'chroma_db_path': chroma_db_path,
                'chroma_db_size_mb': round(chroma_db_size / (1024 * 1024), 2),
                'model_name': self.model_name,
                'vector_db_ready': count > 0,
                'main_categories': main_categories,
                'total_main_categories': len(main_categories),
                'chunk_statistics': chunk_statistics
            }
        except Exception as e:
            logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'total_documents': 0, 
                'vector_db_ready': False,
                'persist_directory': self.persist_directory,
                'chroma_db_path': os.path.abspath(self.persist_directory),
                'error': str(e)
            }

# ì „ì—­ ê°ì²´
llm_client = LLMClient()
vector_db_system = None

def initialize_vector_db():
    """ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global vector_db_system
    
    try:
        if vector_db_system is None:
            logger.info("ğŸ” ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ì €ì¥ ìœ„ì¹˜: ./chroma_db, RAG ì„œë²„ í˜¸í™˜)")
            vector_db_system = VectorDBSystem(persist_directory="./chroma_db")
            logger.info("âœ… ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (ChromaDB: ./chroma_db, RAG ì„œë²„ í˜¸í™˜)")
        return True
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

@app.post("/api/extract_regulation_json", response_model=PDFExtractionResponse, 
         summary="ë‚´ê·œ PDF JSON ì¶”ì¶œ", 
         description="ì—…ë¡œë“œëœ ë‚´ê·œ PDFë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ JSON í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
async def extract_regulation_json(
    file: UploadFile = File(..., description="ë‚´ê·œ PDF íŒŒì¼"),
    regulation_type: str = Form("ì¼ë°˜ë‚´ê·œ", description="ë‚´ê·œ ìœ í˜• (ì˜ˆ: ì§ì œê·œì •, ì¸ì‚¬ê·œì •, ê¸‰ì—¬ê·œì •)"),
    extract_mode: str = Form("comprehensive", description="ì¶”ì¶œ ëª¨ë“œ (comprehensive: í¬ê´„ì , focused: í•µì‹¬ë§Œ)")
):
    """ë‚´ê·œ PDFì—ì„œ JSON ì¶”ì¶œ"""
    start_time = datetime.now()
    
    try:
        # íŒŒì¼ ê²€ì¦
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        logger.info(f"ğŸ“„ ë‚´ê·œ PDF ì²˜ë¦¬ ì‹œì‘: {file.filename} ({regulation_type}, {extract_mode})")
        
        # PDF ë‚´ìš© ì½ê¸°
        pdf_content = await file.read()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text = PDFProcessor.extract_text_from_pdf(pdf_content)
        
        if len(extracted_text.strip()) < 100:
            raise HTTPException(status_code=400, detail="PDFì—ì„œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # LLMìœ¼ë¡œ JSON ì¶”ì¶œ
        regulations_data = llm_client.extract_json_from_regulation(
            extracted_text, 
            regulation_type, 
            extract_mode
        )
        
        if not regulations_data:
            raise HTTPException(status_code=500, detail="ë‚´ê·œ JSON ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # í†µê³„ ê³„ì‚°
        total_faqs = sum(len(category.get('faqs', [])) for category in regulations_data)
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # ì¶”ì¶œ ì •ë³´
        extraction_info = {
            "original_filename": file.filename,
            "file_size_mb": round(len(pdf_content) / (1024 * 1024), 2),
            "extracted_text_length": len(extracted_text),
            "processing_time_seconds": round(processing_time, 2),
            "extraction_timestamp": datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
            "llm_model": "qwen3-30b-a3b-mlx",
            "extraction_strategy": {
                "mode": extract_mode,
                "question_generation": "ë‹¤ê°ë„ ì§ˆë¬¸ ìƒì„± (10ê°€ì§€ íŒ¨í„´)",
                "category_classification": "ì˜ë¯¸ ê¸°ë°˜ ìë™ ë¶„ë¥˜",
                "quality_assurance": "ì›ë¬¸ ì¶©ì‹¤ì„± + ì‹¤ìš©ì„± ê²€ì¦"
            }
        }
        
        logger.info(f"âœ… ë‚´ê·œ JSON ì¶”ì¶œ ì™„ë£Œ: {len(regulations_data)}ê°œ ì¹´í…Œê³ ë¦¬, {total_faqs}ê°œ FAQ")
        
        return PDFExtractionResponse(
            success=True,
            regulation_type=regulation_type,
            categories_count=len(regulations_data),
            total_faqs=total_faqs,
            extraction_info=extraction_info,
            regulations_data=[RegulationCategory(**data) for data in regulations_data],
            processing_time=processing_time
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë‚´ê·œ JSON ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, 
            detail=f"ë‚´ê·œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/api/extract_regulation_json_v2", response_model=PDFExtractionV2Response, 
         summary="ë‚´ê·œ PDF JSON ì¶”ì¶œ (ë²¡í„° DB ì €ì¥ìš©)", 
         description="ì—…ë¡œë“œëœ ë‚´ê·œ PDFë¥¼ ë¶„ì„í•˜ì—¬ ë²¡í„° DBì— ë°”ë¡œ ì €ì¥í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ JSONì„ ë³€í™˜í•©ë‹ˆë‹¤.")
async def extract_regulation_json_v2(
    file: UploadFile = File(..., description="ë‚´ê·œ PDF íŒŒì¼"),
    regulation_type: str = Form("ì¼ë°˜ë‚´ê·œ", description="ë‚´ê·œ ìœ í˜• (ì˜ˆ: ì§ì œê·œì •, ì¸ì‚¬ê·œì •, ê¸‰ì—¬ê·œì •)"),
    extract_mode: str = Form("comprehensive", description="ì¶”ì¶œ ëª¨ë“œ (comprehensive: í¬ê´„ì , focused: í•µì‹¬ë§Œ)")
):
    """ë‚´ê·œ PDFì—ì„œ JSON ì¶”ì¶œ (ë²¡í„° DB ì €ì¥ìš© í˜•íƒœë¡œ ë°˜í™˜)"""
    start_time = datetime.now()
    
    try:
        # íŒŒì¼ ê²€ì¦
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        logger.info(f"ğŸ“„ ë‚´ê·œ PDF ì²˜ë¦¬ ì‹œì‘ (v2): {file.filename} ({regulation_type}, {extract_mode})")
        
        # PDF ë‚´ìš© ì½ê¸°
        pdf_content = await file.read()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text = PDFProcessor.extract_text_from_pdf(pdf_content)
        
        if len(extracted_text.strip()) < 100:
            raise HTTPException(status_code=400, detail="PDFì—ì„œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # LLMìœ¼ë¡œ JSON ì¶”ì¶œ
        regulations_data = llm_client.extract_json_from_regulation(
            extracted_text, 
            regulation_type, 
            extract_mode
        )
        
        if not regulations_data:
            raise HTTPException(status_code=500, detail="ë‚´ê·œ JSON ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # í†µê³„ ê³„ì‚°
        total_faqs = sum(len(category.get('faqs', [])) for category in regulations_data)
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # ëŒ€ë¶„ë¥˜ëª… ê²°ì • (íŒŒì¼ëª… ê¸°ë°˜ ë˜ëŠ” ë‚´ê·œ ìœ í˜• ê¸°ë°˜)
        main_category_name = regulation_type
        if main_category_name == "ì¼ë°˜ë‚´ê·œ":
            # íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ ì‹œë„
            filename_without_ext = os.path.splitext(file.filename)[0]
            # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ì •ë¦¬
            cleaned_filename = re.sub(r'[^\wê°€-í£]', '', filename_without_ext)
            if cleaned_filename:
                main_category_name = cleaned_filename
        
        # RegulationCategory ê°ì²´ë¡œ ë³€í™˜
        regulation_categories = [RegulationCategory(**data) for data in regulations_data]
        
        # ì¶”ì¶œ ìš”ì•½ ì •ë³´
        extraction_summary = {
            "original_filename": file.filename,
            "main_category_name": main_category_name,
            "regulation_type": regulation_type,
            "extract_mode": extract_mode,
            "categories_count": len(regulations_data),
            "total_faqs": total_faqs,
            "file_size_mb": round(len(pdf_content) / (1024 * 1024), 2),
            "extracted_text_length": len(extracted_text),
            "processing_time_seconds": round(processing_time, 2),
            "extraction_timestamp": datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
            "llm_model": "qwen3-30b-a3b-mlx",
            "ready_for_vector_storage": True,
            "vector_db_compatibility": "RAG ì„œë²„ í˜¸í™˜ í˜•ì‹",
            "usage_note": "ì´ ë°ì´í„°ëŠ” /api/store_extracted_json_to_vector APIì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }
        
        logger.info(f"âœ… ë‚´ê·œ JSON ì¶”ì¶œ ì™„ë£Œ (v2): {len(regulations_data)}ê°œ ì¹´í…Œê³ ë¦¬, {total_faqs}ê°œ FAQ â†’ ëŒ€ë¶„ë¥˜: {main_category_name}")
        
        return PDFExtractionV2Response(
            success=True,
            regulation_data=regulation_categories,
            main_category_name=main_category_name,
            extraction_summary=extraction_summary
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë‚´ê·œ JSON ì¶”ì¶œ ì‹¤íŒ¨ (v2): {e}", exc_info=True)
        raise HTTPException(
            status_code=500, 
            detail=f"ë‚´ê·œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/api/save_extracted_json_v2", 
         response_model=SaveResponse,
         summary="ì¶”ì¶œëœ JSON ì €ì¥ (JSON Body)", 
         description="JSON bodyë¡œ ë‚´ê·œ ë°ì´í„°ë¥¼ ë°›ì•„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. (ê¶Œì¥ ë°©ì‹)")
async def save_extracted_json_v2(request_data: SaveRequestV2):
    """JSON bodyë¡œ ì¶”ì¶œëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    start_time = datetime.now()
    logger.info(f"ğŸ“ JSON ì €ì¥ ìš”ì²­ ë°›ìŒ (v2): filename={request_data.filename}")
    
    try:
        # ë°ì´í„° ê²€ì¦ì€ Pydantic ëª¨ë¸ì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
        regulation_data = request_data.regulation_data
        filename = request_data.filename
        
        # ì €ì¥í•  ë””ë ‰í† ë¦¬ í™•ì¸
        save_directory = "./extracted_regulations"
        os.makedirs(save_directory, exist_ok=True)
        
        # íŒŒì¼ëª… ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        safe_filename = re.sub(r'[^\w\-_ê°€-í£]', '_', filename)
        if not safe_filename:
            safe_filename = f"regulation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        file_path = os.path.join(save_directory, f"{safe_filename}.json")
        
        # íŒŒì¼ ì¤‘ë³µ ì²´í¬ ë° ë²ˆí˜¸ ì¶”ê°€
        counter = 1
        original_path = file_path
        while os.path.exists(file_path):
            name, ext = os.path.splitext(original_path)
            file_path = f"{name}_{counter}{ext}"
            counter += 1
        
        # JSON ë°ì´í„° ë³€í™˜
        output_data = [category.dict() for category in regulation_data]
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size_kb = round(os.path.getsize(file_path) / 1024, 2)
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ (v2): {file_path} ({file_size_kb}KB, {processing_time:.2f}ì´ˆ)")
        
        return SaveResponse(
            success=True,
            message="JSON íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
            file_path=file_path,
            categories_count=len(output_data),
            total_faqs=sum(len(cat['faqs']) for cat in output_data),
            saved_at=datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
            file_size_kb=file_size_kb
        )
        
    except Exception as e:
        logger.error(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨ (v2): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/store_json_to_vector", 
         response_model=VectorStoreResponse,
         summary="JSON íŒŒì¼ì„ ë²¡í„° DBì— ì €ì¥ (RAG ì„œë²„ í˜¸í™˜)", 
         description="ì €ì¥ëœ JSON íŒŒì¼ì„ ì½ì–´ì™€ì„œ RAG ì„œë²„ì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.")
async def store_json_to_vector(request: VectorStoreRequest):
    """ì €ì¥ëœ JSON íŒŒì¼ì„ ë²¡í„° DBì— ì €ì¥ (RAG ì„œë²„ í˜¸í™˜)"""
    try:
        # ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if not initialize_vector_db():
            raise HTTPException(status_code=503, detail="ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        
        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        filename = request.filename
        if not filename.endswith('.json'):
            filename += '.json'
        
        file_path = os.path.join("./extracted_regulations", filename)
        
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
        
        # JSON íŒŒì¼ ì½ê¸°
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                regulation_data = json.load(f)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
        
        # ëŒ€ë¶„ë¥˜ëª… ê²°ì •
        main_category_name = request.main_category_name or os.path.splitext(filename)[0]
        
        # ë²¡í„° DBì— ì €ì¥ (RAG ì„œë²„ì™€ ë™ì¼í•œ ë°©ì‹)
        store_result = vector_db_system.store_regulation_data(regulation_data, main_category_name)
        
        logger.info(f"âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ (RAG í˜¸í™˜): {filename} â†’ {main_category_name}")
        
        return VectorStoreResponse(
            success=True,
            message=f"JSON íŒŒì¼ '{filename}'ì´ RAG ì„œë²„ í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
            main_category=main_category_name,
            total_chunks=store_result["total_chunks"],
            chunk_statistics=store_result["chunk_statistics"],
            vector_db_stats={
                "total_documents": store_result["vector_db_total"],
                "batch_count": store_result["batch_count"],
                "rag_compatibility": "ì™„ì „ í˜¸í™˜"
            },
            stored_at=datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/store_extracted_json_to_vector", 
         response_model=VectorStoreResponse,
         summary="ì¶”ì¶œëœ JSONì„ ì§ì ‘ ë²¡í„° DBì— ì €ì¥ (RAG ì„œë²„ í˜¸í™˜)", 
         description="ì¶”ì¶œëœ JSON ë°ì´í„°ë¥¼ íŒŒì¼ ì €ì¥ ì—†ì´ RAG ì„œë²„ì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ì§ì ‘ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.")
async def store_extracted_json_to_vector(request: VectorStoreDirectRequest):
    """ì¶”ì¶œëœ JSON ë°ì´í„°ë¥¼ ì§ì ‘ ë²¡í„° DBì— ì €ì¥ (RAG ì„œë²„ í˜¸í™˜)"""
    try:
        # ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if not initialize_vector_db():
            raise HTTPException(status_code=503, detail="ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        
        # JSON ë°ì´í„° ë³€í™˜
        regulation_data = [category.dict() for category in request.regulation_data]
        
        # ë²¡í„° DBì— ì €ì¥ (RAG ì„œë²„ì™€ ë™ì¼í•œ ë°©ì‹)
        store_result = vector_db_system.store_regulation_data(regulation_data, request.main_category_name)
        
        logger.info(f"âœ… ë²¡í„° DB ì§ì ‘ ì €ì¥ ì™„ë£Œ (RAG í˜¸í™˜): {request.main_category_name}")
        
        return VectorStoreResponse(
            success=True,
            message=f"JSON ë°ì´í„°ê°€ RAG ì„œë²„ í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ëŒ€ë¶„ë¥˜: {request.main_category_name})",
            main_category=request.main_category_name,
            total_chunks=store_result["total_chunks"],
            chunk_statistics=store_result["chunk_statistics"],
            vector_db_stats={
                "total_documents": store_result["vector_db_total"],
                "batch_count": store_result["batch_count"],
                "rag_compatibility": "ì™„ì „ í˜¸í™˜",
                "chunking_strategy": "qa_full + question_focused + answer_focused + sentence_level"
            },
            stored_at=datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB ì§ì ‘ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë²¡í„° DB ì§ì ‘ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/vector_search", 
         response_model=VectorSearchResponse,
         summary="ë²¡í„° DB ê²€ìƒ‰ (RAG ì„œë²„ í˜¸í™˜)", 
         description="RAG ì„œë²„ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
async def vector_search(request: VectorSearchRequest):
    """ë²¡í„° DBì—ì„œ ê²€ìƒ‰ (RAG ì„œë²„ í˜¸í™˜)"""
    try:
        # ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œë„
        if vector_db_system is None:
            logger.info("ğŸ” ë²¡í„° DB ì‹œìŠ¤í…œ ìë™ ì´ˆê¸°í™” ì‹œë„...")
            if not initialize_vector_db():
                raise HTTPException(
                    status_code=503, 
                    detail={
                        "error": "ë²¡í„° DB ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨",
                        "message": "ë²¡í„° DB ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                        "suggestions": [
                            "ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
                            "ChromaDB ì €ì¥ ê³µê°„ì„ í™•ì¸í•´ì£¼ì„¸ìš”",
                            "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”"
                        ]
                    }
                )
        
        # ë²¡í„° DBê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        stats = vector_db_system.get_stats()
        if stats['total_documents'] == 0:
            return VectorSearchResponse(
                query=request.query,
                results=[],
                count=0,
                main_category_filter=request.main_category_filter,
                search_type="ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰ (RAG í˜¸í™˜, ë°ì´í„° ì—†ìŒ)"
            )
        
        # ê²€ìƒ‰ ìˆ˜í–‰ (RAG ì„œë²„ì™€ ë™ì¼í•œ ë°©ì‹)
        results = vector_db_system.search(
            query=request.query,
            top_k=request.top_k,
            main_category_filter=request.main_category_filter
        )
        
        # ê²°ê³¼ ë³€í™˜
        search_results = [VectorSearchResult(**result) for result in results]
        
        return VectorSearchResponse(
            query=request.query,
            results=search_results,
            count=len(results),
            main_category_filter=request.main_category_filter,
            search_type="ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰ (RAG ì„œë²„ í˜¸í™˜)"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/vector_stats", 
         response_model=VectorStatsResponse,
         summary="ë²¡í„° DB í†µê³„ (RAG ì„œë²„ í˜¸í™˜)", 
         description="RAG ì„œë²„ì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ ìƒíƒœì™€ í†µê³„ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. (ì €ì¥ ìœ„ì¹˜: ./chroma_db)")
async def get_vector_stats():
    """ë²¡í„° DB í†µê³„ ì •ë³´ (RAG ì„œë²„ í˜¸í™˜)"""
    try:
        if vector_db_system is None:
            # ì´ˆê¸°í™” ì‹œë„
            if not initialize_vector_db():
                raise HTTPException(status_code=503, detail="ë²¡í„° DB ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        stats = vector_db_system.get_stats()
        
        return VectorStatsResponse(
            total_documents=stats['total_documents'],
            collection_name=stats['collection_name'],
            persist_directory=stats['persist_directory'],
            chroma_db_path=stats.get('chroma_db_path', './chroma_db'),
            chroma_db_size_mb=stats.get('chroma_db_size_mb', 0.0),
            model_name=stats.get('model_name', 'nlpai-lab/KURE-v1'),
            main_categories=stats['main_categories'],
            total_main_categories=stats['total_main_categories'],
            chunk_statistics=stats['chunk_statistics'],
            vector_db_ready=stats['vector_db_ready']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë²¡í„° DB í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/saved_files", 
         response_model=SavedFilesResponse,
         summary="ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ")
async def get_saved_files():
    """ì €ì¥ëœ ë‚´ê·œ JSON íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
    try:
        save_directory = "./extracted_regulations"
        if not os.path.exists(save_directory):
            return SavedFilesResponse(files=[], total_count=0, total_size_kb=0.0)
        
        files = []
        for filename in os.listdir(save_directory):
            if filename.endswith('.json'):
                file_path = os.path.join(save_directory, filename)
                stat = os.stat(file_path)
                
                files.append(FileInfo(
                    filename=filename,
                    file_path=file_path,
                    size_kb=round(stat.st_size / 1024, 2),
                    created_at=datetime.fromtimestamp(stat.st_ctime, pytz.timezone('Asia/Seoul')).isoformat(),
                    modified_at=datetime.fromtimestamp(stat.st_mtime, pytz.timezone('Asia/Seoul')).isoformat()
                ))
        
        # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        files.sort(key=lambda x: x.modified_at, reverse=True)
        
        return SavedFilesResponse(
            files=files,
            total_count=len(files),
            total_size_kb=round(sum(f.size_kb for f in files), 2)
        )
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/download_file/{filename}", summary="ì €ì¥ëœ íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
async def download_file(filename: str):
    """ì €ì¥ëœ JSON íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        # íŒŒì¼ëª… ë³´ì•ˆ ê²€ì¦
        safe_filename = os.path.basename(filename)
        if not safe_filename.endswith('.json'):
            safe_filename += '.json'
        
        file_path = os.path.join("./extracted_regulations", safe_filename)
        
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ì½ê¸°
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return JSONResponse(
            content=json.loads(content),
            headers={
                "Content-Disposition": f"attachment; filename={safe_filename}",
                "Content-Type": "application/json; charset=utf-8"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

@app.delete("/api/delete_file/{filename}", summary="ì €ì¥ëœ íŒŒì¼ ì‚­ì œ")
async def delete_file(filename: str):
    """ì €ì¥ëœ JSON íŒŒì¼ ì‚­ì œ"""
    try:
        # íŒŒì¼ëª… ë³´ì•ˆ ê²€ì¦
        safe_filename = os.path.basename(filename)
        if not safe_filename.endswith('.json'):
            safe_filename += '.json'
        
        file_path = os.path.join("./extracted_regulations", safe_filename)
        
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ì‚­ì œ
        os.remove(file_path)
        
        logger.info(f"ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {file_path}")
        
        return {
            "success": True,
            "message": f"íŒŒì¼ '{safe_filename}'ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_file": safe_filename,
            "deleted_at": datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/export_vector_to_json/{main_category}", 
         summary="ë²¡í„° DB ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (RAG ì„œë²„ í˜¸í™˜)", 
         description="ë²¡í„° DBì— ì €ì¥ëœ íŠ¹ì • ëŒ€ë¶„ë¥˜ ë°ì´í„°ë¥¼ RAG ì„œë²„ê°€ ì½ì„ ìˆ˜ ìˆëŠ” JSON íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.")
async def export_vector_to_json(main_category: str):
    """ë²¡í„° DBì˜ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° (RAG ì„œë²„ í˜¸í™˜)"""
    try:
        if vector_db_system is None:
            if not initialize_vector_db():
                raise HTTPException(status_code=503, detail="ë²¡í„° DB ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë²¡í„° DBì—ì„œ íŠ¹ì • ëŒ€ë¶„ë¥˜ì˜ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_results = vector_db_system.collection.get(
            where={"main_category": main_category},
            include=['documents', 'metadatas']
        )
        
        if not all_results or not all_results.get('metadatas'):
            raise HTTPException(status_code=404, detail=f"ëŒ€ë¶„ë¥˜ '{main_category}'ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë©”íƒ€ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™” (RAG ì„œë²„ í˜•ì‹ì— ë§ì¶¤)
        categories_dict = {}
        
        for metadata in all_results['metadatas']:
            # qa_full ì²­í¬ë§Œ ì‚¬ìš© (ì¤‘ë³µ ë°©ì§€)
            if metadata.get('chunk_type') != 'qa_full':
                continue
                
            sub_category = metadata['sub_category']
            question = metadata['question']
            answer = metadata['answer']
            
            if sub_category not in categories_dict:
                categories_dict[sub_category] = {
                    "category": sub_category,
                    "faqs": []
                }
            
            categories_dict[sub_category]["faqs"].append({
                "question": question,
                "answer": answer
            })
        
        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (RAG ì„œë²„ê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹)
        exported_data = list(categories_dict.values())
        
        # data_json ë””ë ‰í† ë¦¬ì— ì €ì¥ (RAG ì„œë²„ í˜¸í™˜)
        data_json_dir = "./data_json"
        os.makedirs(data_json_dir, exist_ok=True)
        
        output_filename = f"{main_category}.json"
        output_path = os.path.join(data_json_dir, output_filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(exported_data, f, ensure_ascii=False, indent=2)
        
        file_size_kb = round(os.path.getsize(output_path) / 1024, 2)
        
        logger.info(f"âœ… ë²¡í„° DB ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ (RAG í˜¸í™˜): {output_path}")
        
        return {
            "success": True,
            "message": f"'{main_category}' ë°ì´í„°ë¥¼ RAG ì„œë²„ í˜¸í™˜ JSON íŒŒì¼ë¡œ ë‚´ë³´ëƒˆìŠµë‹ˆë‹¤.",
            "main_category": main_category,
            "output_file": output_filename,
            "output_path": output_path,
            "categories_count": len(exported_data),
            "total_faqs": sum(len(cat["faqs"]) for cat in exported_data),
            "file_size_kb": file_size_kb,
            "exported_at": datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
            "rag_compatibility": "ì™„ì „ í˜¸í™˜",
            "usage_note": "í¬íŠ¸ 5000ì˜ RAG ì„œë²„ì—ì„œ /api/rebuild_indexë¥¼ í˜¸ì¶œí•˜ì—¬ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")

@app.get("/api/export_all_vector_to_json", 
         summary="ë²¡í„° DBì˜ ëª¨ë“  ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (RAG ì„œë²„ í˜¸í™˜)", 
         description="ë²¡í„° DBì— ì €ì¥ëœ ëª¨ë“  ëŒ€ë¶„ë¥˜ ë°ì´í„°ë¥¼ RAG ì„œë²„ê°€ ì½ì„ ìˆ˜ ìˆëŠ” JSON íŒŒì¼ë“¤ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.")
async def export_all_vector_to_json():
    """ë²¡í„° DBì˜ ëª¨ë“  ë°ì´í„°ë¥¼ JSON íŒŒì¼ë“¤ë¡œ ë‚´ë³´ë‚´ê¸° (RAG ì„œë²„ í˜¸í™˜)"""
    try:
        if vector_db_system is None:
            if not initialize_vector_db():
                raise HTTPException(status_code=503, detail="ë²¡í„° DB ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë²¡í„° DB í†µê³„ì—ì„œ ëª¨ë“  ëŒ€ë¶„ë¥˜ ê°€ì ¸ì˜¤ê¸°
        stats = vector_db_system.get_stats()
        main_categories = list(stats['main_categories'].keys())
        
        if not main_categories:
            raise HTTPException(status_code=404, detail="ë²¡í„° DBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # data_json ë””ë ‰í† ë¦¬ ì¤€ë¹„ (RAG ì„œë²„ í˜¸í™˜)
        data_json_dir = "./data_json"
        os.makedirs(data_json_dir, exist_ok=True)
        
        exported_files = []
        total_categories = 0
        total_faqs = 0
        
        for main_category in main_categories:
            try:
                # ê° ëŒ€ë¶„ë¥˜ë³„ë¡œ ë°ì´í„° ì¶”ì¶œ
                all_results = vector_db_system.collection.get(
                    where={"main_category": main_category},
                    include=['documents', 'metadatas']
                )
                
                if not all_results or not all_results.get('metadatas'):
                    continue
                
                # ë©”íƒ€ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™” (RAG ì„œë²„ í˜•ì‹ì— ë§ì¶¤)
                categories_dict = {}
                
                for metadata in all_results['metadatas']:
                    # qa_full ì²­í¬ë§Œ ì‚¬ìš© (ì¤‘ë³µ ë°©ì§€)
                    if metadata.get('chunk_type') != 'qa_full':
                        continue
                        
                    sub_category = metadata['sub_category']
                    question = metadata['question']
                    answer = metadata['answer']
                    
                    if sub_category not in categories_dict:
                        categories_dict[sub_category] = {
                            "category": sub_category,
                            "faqs": []
                        }
                    
                    categories_dict[sub_category]["faqs"].append({
                        "question": question,
                        "answer": answer
                    })
                
                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (RAG ì„œë²„ê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹)
                exported_data = list(categories_dict.values())
                
                # íŒŒì¼ ì €ì¥
                output_filename = f"{main_category}.json"
                output_path = os.path.join(data_json_dir, output_filename)
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(exported_data, f, ensure_ascii=False, indent=2)
                
                file_size_kb = round(os.path.getsize(output_path) / 1024, 2)
                faqs_count = sum(len(cat["faqs"]) for cat in exported_data)
                
                exported_files.append({
                    "main_category": main_category,
                    "filename": output_filename,
                    "categories_count": len(exported_data),
                    "faqs_count": faqs_count,
                    "file_size_kb": file_size_kb
                })
                
                total_categories += len(exported_data)
                total_faqs += faqs_count
                
                logger.info(f"âœ… ë‚´ë³´ë‚´ê¸° ì™„ë£Œ (RAG í˜¸í™˜): {main_category} ({len(exported_data)}ê°œ ì¹´í…Œê³ ë¦¬, {faqs_count}ê°œ FAQ)")
                
            except Exception as e:
                logger.error(f"âŒ {main_category} ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
                continue
        
        if not exported_files:
            raise HTTPException(status_code=500, detail="ë°ì´í„° ë‚´ë³´ë‚´ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        return {
            "success": True,
            "message": f"ë²¡í„° DBì˜ ëª¨ë“  ë°ì´í„°ë¥¼ RAG ì„œë²„ í˜¸í™˜ JSON íŒŒì¼ë¡œ ë‚´ë³´ëƒˆìŠµë‹ˆë‹¤.",
            "exported_files": exported_files,
            "summary": {
                "total_main_categories": len(exported_files),
                "total_categories": total_categories,
                "total_faqs": total_faqs,
                "output_directory": data_json_dir
            },
            "exported_at": datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
            "rag_compatibility": "ì™„ì „ í˜¸í™˜",
            "next_steps": [
                "í¬íŠ¸ 5000ì˜ RAG ì„œë²„ì—ì„œ curl -X POST 'http://0.0.0.0:5000/api/rebuild_index'ë¥¼ í˜¸ì¶œí•˜ì—¬ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”.",
                "RAG ì„œë²„ê°€ ìë™ìœ¼ë¡œ ./data_json ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ì¸ì‹í•˜ê³  ë¡œë“œí•©ë‹ˆë‹¤.",
                "ë¡œë“œ ì™„ë£Œ í›„ í¬íŠ¸ 5000ì—ì„œ ê³ ê¸‰ ê²€ìƒ‰ ë° ëŒ€í™” ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì „ì²´ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")

@app.get("/api/health", summary="ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ (RAG ì„œë²„ í˜¸í™˜)")
async def health_check():
    """ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ (RAG ì„œë²„ í˜¸í™˜)"""
    try:
        # LLM API ì—°ê²° í…ŒìŠ¤íŠ¸
        test_response = requests.post(
            llm_client.api_url,
            json={
                "model": "qwen3-30b-a3b-mlx",
                "messages": [{"role": "user", "content": "í…ŒìŠ¤íŠ¸"}],
                "temperature": 0.1,
                "max_tokens": 10,
                "stream": False
            },
            timeout=10
        )
        
        llm_available = test_response.status_code == 200
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ì²´í¬
        save_directory = "./extracted_regulations"
        directory_exists = os.path.exists(save_directory)
        if not directory_exists:
            os.makedirs(save_directory, exist_ok=True)
        
        # ë²¡í„° DB ìƒíƒœ í™•ì¸ (RAG ì„œë²„ í˜¸í™˜)
        vector_db_available = False
        vector_db_stats = {}
        
        try:
            if vector_db_system is None:
                initialize_vector_db()
            if vector_db_system is not None:
                vector_db_stats = vector_db_system.get_stats()
                vector_db_available = vector_db_stats.get('vector_db_ready', False)
                logger.info(f"ğŸ“ ChromaDB ìƒíƒœ í™•ì¸ ì™„ë£Œ (RAG í˜¸í™˜): {vector_db_stats.get('chroma_db_path', './chroma_db')}")
        except Exception as e:
            logger.warning(f"ë²¡í„° DB ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        overall_status = "healthy" if (llm_available and vector_db_available) else "partial"
        
        return {
            "status": overall_status,
            "llm_api_available": llm_available,
            "llm_api_url": llm_client.api_url,
            "vector_db_available": vector_db_available,
            "vector_db_path": "./chroma_db",
            "vector_db_stats": vector_db_stats,
            "save_directory_available": True,
            "save_directory_path": save_directory,
            "rag_compatibility": "ì™„ì „ í˜¸í™˜",
            "features": [
                "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ",
                "AI ê¸°ë°˜ êµ¬ì¡° ë¶„ì„", 
                "ì§ˆë¬¸-ë‹µë³€ ìë™ ìƒì„±",
                "ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜",
                "JSON êµ¬ì¡°í™” ì¶œë ¥",
                "íŒŒì¼ ì €ì¥ ë° ê´€ë¦¬",
                "ë²¡í„° DB ì €ì¥ (RAG í˜¸í™˜)",
                "ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰"
            ],
            "extraction_capabilities": [
                "10ê°€ì§€ ì§ˆë¬¸ íŒ¨í„´ ì§€ì›",
                "ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜",
                "ì›ë¬¸ ê¸°ë°˜ ì •í™•í•œ ë‹µë³€",
                "ì²­í¬ ë‹¨ìœ„ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬",
                "ì¤‘ë³µ ì œê±° ë° ë³‘í•©",
                "Form Data & JSON Body ì§€ì›",
                "ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬"
            ],
            "vector_db_capabilities": [
                "ChromaDB ê¸°ë°˜ ë²¡í„° ì €ì¥ (ê²½ë¡œ: ./chroma_db)",
                "SentenceTransformer ì„ë² ë”©",
                "RAG ì„œë²„ í˜¸í™˜ ì²­í‚¹ ì „ëµ (qa_full, question_focused, answer_focused, sentence_level)",
                "ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§",
                "ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰",
                "ì‹¤ì‹œê°„ í†µê³„ ì¡°íšŒ",
                "RAG ì„œë²„ ì™„ì „ í˜¸í™˜"
            ],
            "api_endpoints": {
                "extraction": "/api/extract_regulation_json",
                "extraction_v2": "/api/extract_regulation_json_v2",
                "save_json": "/api/save_extracted_json_v2",
                "store_file_to_vector": "/api/store_json_to_vector",
                "store_direct_to_vector": "/api/store_extracted_json_to_vector",
                "vector_search": "/api/vector_search",
                "vector_stats": "/api/vector_stats",
                "export_vector_data": "/api/export_vector_to_json/{main_category}",
                "export_all_vector_data": "/api/export_all_vector_to_json",
                "file_list": "/api/saved_files",
                "file_download": "/api/download_file/{filename}",
                "file_delete": "/api/delete_file/{filename}",
                "health": "/api/health"
            },
            "timestamp": datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
        return {
            "status": "unhealthy",
            "llm_api_available": False,
            "vector_db_available": False,
            "error": str(e),
            "timestamp": datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
        }

# ì‹œì‘ ì´ë²¤íŠ¸
@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    logger.info("ğŸš€ ì‹œìŠ¤í…œ ì‹œì‘ ì¤‘... (RAG ì„œë²„ í˜¸í™˜)")
    # ë²¡í„° DBëŠ” í•„ìš”í•  ë•Œ ì´ˆê¸°í™”

if __name__ == "__main__":
    import uvicorn
    
    print("=" * 80)
    print("ğŸ”¥ íšŒì‚¬ ë‚´ê·œ PDF JSON ì¶”ì¶œ + ë²¡í„° DB í†µí•© API ì„œë²„ v3.1 (RAG ì„œë²„ í˜¸í™˜)")
    print("=" * 80)
    print("ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:")
    print("   ğŸ“„ PDF ì—…ë¡œë“œ ë° ìë™ íŒŒì‹±")
    print("   ğŸ§  AI ê¸°ë°˜ ë‚´ìš© êµ¬ì¡° ë¶„ì„")  
    print("   ğŸ“Š í‘œì¤€ JSON í˜•íƒœë¡œ ë³€í™˜")
    print("   ğŸ¯ ì§ˆë¬¸-ë‹µë³€ ìë™ ìƒì„±")
    print("   ğŸ“ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜")
    print("   ğŸ’¾ íŒŒì¼ ì €ì¥ ë° ê´€ë¦¬")
    print("   ğŸ” ë²¡í„° DB ì €ì¥ ë° ê²€ìƒ‰ (RAG ì„œë²„ ì™„ì „ í˜¸í™˜)")
    print("=" * 80)
    print("ğŸ”§ API ì—”ë“œí¬ì¸íŠ¸:")
    print("   - POST /api/extract_regulation_json       : ğŸ“„ PDF â†’ JSON ë³€í™˜")
    print("   - POST /api/extract_regulation_json_v2    : ğŸ“„ PDF â†’ JSON (ë²¡í„° DBìš©)")
    print("   - POST /api/save_extracted_json_v2        : ğŸ’¾ JSON ì €ì¥")
    print("   - POST /api/store_json_to_vector          : ğŸ” íŒŒì¼ â†’ ë²¡í„° DB (RAG í˜¸í™˜)")
    print("   - POST /api/store_extracted_json_to_vector: ğŸ” JSON â†’ ë²¡í„° DB (RAG í˜¸í™˜)")
    print("   - POST /api/vector_search                 : ğŸ” ë²¡í„° ê²€ìƒ‰ (RAG í˜¸í™˜)")
    print("   - GET  /api/vector_stats                  : ğŸ“Š ë²¡í„° DB í†µê³„ (RAG í˜¸í™˜)")
    print("   - GET  /api/export_vector_to_json/{category}: ğŸ“¤ ë²¡í„° DB â†’ JSON (RAG í˜¸í™˜)")
    print("   - GET  /api/export_all_vector_to_json     : ğŸ“¤ ì „ì²´ ë²¡í„° DB â†’ JSON (RAG í˜¸í™˜)")
    print("   - GET  /api/saved_files                   : ğŸ“ íŒŒì¼ ëª©ë¡ ì¡°íšŒ")
    print("   - GET  /api/download_file/{filename}      : â¬‡ï¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
    print("   - DELETE /api/delete_file/{filename}      : ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œ")
    print("   - GET  /api/health                        : â¤ï¸ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ (RAG í˜¸í™˜)")
    print("   - GET  /docs                              : ğŸ“– API ë¬¸ì„œ")
    print("=" * 80)
    print("ğŸ” ë²¡í„° DB ê¸°ëŠ¥ (RAG ì„œë²„ ì™„ì „ í˜¸í™˜):")
    print("   â€¢ ChromaDB ê¸°ë°˜ ê³ ì„±ëŠ¥ ë²¡í„° ì €ì¥ (ì €ì¥ ìœ„ì¹˜: ./chroma_db)")
    print("   â€¢ SentenceTransformer í•œêµ­ì–´ ì„ë² ë”© (nlpai-lab/KURE-v1)")
    print("   â€¢ RAG ì„œë²„ì™€ ë™ì¼í•œ ì²­í‚¹ ì „ëµ:")
    print("     - qa_full: ì§ˆë¬¸+ë‹µë³€ ì „ì²´")
    print("     - question_focused: ì§ˆë¬¸ ì¤‘ì‹¬")
    print("     - answer_focused: ë‹µë³€ ì¤‘ì‹¬")
    print("     - sentence_level: ë¬¸ì¥ ë‹¨ìœ„")
    print("   â€¢ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§")
    print("   â€¢ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰")
    print("   â€¢ ì‹¤ì‹œê°„ í†µê³„ ì¡°íšŒ")
    print("   â€¢ RAG ì„œë²„ í¬íŠ¸ 5000ê³¼ ì™„ì „ í˜¸í™˜")
    print("=" * 80)
    print("ğŸ§  AI ë¶„ì„ íŠ¹ì§•:")
    print("   â€¢ 10ê°€ì§€ ì§ˆë¬¸ íŒ¨í„´ìœ¼ë¡œ í¬ê´„ì  ë¶„ì„")
    print("   â€¢ ì›ë¬¸ ê¸°ë°˜ ì •í™•í•œ ë‹µë³€ ìƒì„±")
    print("   â€¢ ì˜ë¯¸ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜")
    print("   â€¢ ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬")
    print("   â€¢ ì¤‘ë³µ ì œê±° ë° ìŠ¤ë§ˆíŠ¸ ë³‘í•©")
    print("   â€¢ Form Data & JSON Body ë™ì‹œ ì§€ì›")
    print("   â€¢ ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…")
    print("=" * 80)
    print("ğŸ“ ì›Œí¬í”Œë¡œìš° (RAG ì„œë²„ í˜¸í™˜):")
    print("   1. PDF ì—…ë¡œë“œ â†’ JSON ì¶”ì¶œ")
    print("   2. JSON íŒŒì¼ ì €ì¥ (ì„ íƒì‚¬í•­)")
    print("   3. ë²¡í„° DBì— ì €ì¥ (RAG ì„œë²„ì™€ ë™ì¼í•œ í˜•ì‹)")
    print("   4. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰")
    print("   5. RAG ì„œë²„ë¡œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
    print("   â˜… ì‹ ê·œ: PDF â†’ JSON (ë²¡í„°ìš©) â†’ ë²¡í„° DB (RAG í˜¸í™˜) (ì›ìŠ¤í†±)")
    print("   â˜… í˜¸í™˜: ë²¡í„° DB â†’ JSON â†’ RAG ì„œë²„ í¬íŠ¸ 5000 (ì™„ì „ í˜¸í™˜)")
    print("=" * 80)
    print("ğŸ”§ RAG ì„œë²„ ì—°ë™ ë°©ë²•:")
    print("   1. ì´ ì„œë²„(í¬íŠ¸ 5001)ì—ì„œ PDF â†’ JSON â†’ ë²¡í„° DB ì €ì¥")
    print("   2. /api/export_all_vector_to_json í˜¸ì¶œí•˜ì—¬ ./data_jsonì— JSON íŒŒì¼ ìƒì„±")
    print("   3. RAG ì„œë²„(í¬íŠ¸ 5000)ì—ì„œ curl -X POST 'http://localhost:5000/api/rebuild_index' í˜¸ì¶œ")
    print("   4. RAG ì„œë²„ì—ì„œ ê³ ê¸‰ ê²€ìƒ‰ ë° ëŒ€í™” ê¸°ëŠ¥ ì‚¬ìš©")
    print("=" * 80)
    print("ğŸ¯ ì²­í‚¹ ì „ëµ í˜¸í™˜ì„±:")
    print("   â€¢ RAG ì„œë²„ì™€ 100% ë™ì¼í•œ ì²­í‚¹ ë°©ì‹ ì‚¬ìš©")
    print("   â€¢ ë©”íƒ€ë°ì´í„° êµ¬ì¡° ì™„ì „ ì¼ì¹˜")
    print("   â€¢ ì„ë² ë”© ëª¨ë¸ ë™ì¼ (nlpai-lab/KURE-v1)")
    print("   â€¢ ChromaDB ì»¬ë ‰ì…˜ êµ¬ì¡° ë™ì¼")
    print("=" * 80)
    print("ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:")
    print("   1. http://localhost:5001/docs ì ‘ì†")
    print("   2. PDF ì—…ë¡œë“œ â†’ JSON ì¶”ì¶œ")
    print("   3. JSONì„ ë²¡í„° DBì— ì €ì¥ (RAG í˜¸í™˜)")
    print("   4. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("   5. RAG ì„œë²„ë¡œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
    print("   â˜… ì›ìŠ¤í†±: /api/extract_regulation_json_v2 â†’ /api/store_extracted_json_to_vector")
    print("   â˜… RAG ì—°ë™: /api/export_all_vector_to_json â†’ RAG ì„œë²„ ì¬ë¡œë“œ")
    print("=" * 80)
    print("ğŸ” í˜¸í™˜ì„± ë³´ì¥:")
    print("   â€¢ ë™ì¼í•œ ëª¨ë¸: nlpai-lab/KURE-v1")
    print("   â€¢ ë™ì¼í•œ ì²­í‚¹: qa_full, question_focused, answer_focused, sentence_level")
    print("   â€¢ ë™ì¼í•œ ë©”íƒ€ë°ì´í„°: main_category, sub_category, question, answer, source_file, chunk_type")
    print("   â€¢ ë™ì¼í•œ ì»¬ë ‰ì…˜: company_regulations")
    print("   â€¢ ë™ì¼í•œ ì €ì¥ì†Œ: ./chroma_db")
    print("=" * 80)
    print("âš ï¸ ì¤‘ìš” ì‚¬í•­:")
    print("   â€¢ ì´ ì„œë²„ëŠ” RAG ì„œë²„(í¬íŠ¸ 5000)ì™€ ì™„ì „ í˜¸í™˜ë©ë‹ˆë‹¤")
    print("   â€¢ ë²¡í„° DB ë°ì´í„°ëŠ” ì–‘ë°©í–¥ í˜¸í™˜ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    print("   â€¢ JSON ë‚´ë³´ë‚´ê¸°ë¡œ RAG ì„œë²„ì— ë°ì´í„° ì „ì†¡ ê°€ëŠ¥")
    print("   â€¢ ë™ì¼í•œ ChromaDB ê²½ë¡œ(./chroma_db) ì‚¬ìš©")
    print("=" * 80)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    logger.info("ğŸ” RAG ì„œë²„ í˜¸í™˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    
    logger.info("âœ… RAG ì„œë²„ í˜¸í™˜ FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")
    
    # FastAPI ì•± ì‹¤í–‰
    uvicorn.run(app, host="0.0.0.0", port=5001, log_level="info")